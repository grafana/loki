---
title: Helm Chart Values
menuTitle: Helm chart values
description: Reference for Helm Chart values.
aliases:
  - ../../../installation/helm/reference/
weight: 500
keywords: []
---

<!-- Autogenerated. Modify ../production/helm/loki/reference.md.gotmpl -->

# Helm Chart Values
<!-- vale Grafana.Quotes = NO -->
<!-- The reference title is required. Use a noun-based title. -->
<!-- vale Grafana.Quotes = YES -->

This is the generated reference for the Loki Helm Chart values.

> **Note:** This reference is for the Loki Helm chart version 3.0 or greater.
> If you are using the `grafana/loki-stack` Helm chart from the community repo,
> please refer to the `values.yaml` of the respective Github repository
> [grafana/helm-charts](https://github.com/grafana/helm-charts/tree/main/charts/loki-stack).

<!-- Override default values table from helm-docs. See https://github.com/norwoodj/helm-docs/tree/master#advanced-table-rendering -->

{{< responsive-table >}}
<table>
	<thead>
		<th>Key</th>
		<th>Type</th>
		<th>Description</th>
		<th>Default</th>
	</thead>
	<tbody>
		<tr>
			<td>adminApi</td>
			<td>object</td>
			<td>Configuration for the `admin-api` target</td>
			<td><pre lang="json">
{
  "affinity": {},
  "annotations": {},
  "containerSecurityContext": {
    "allowPrivilegeEscalation": false,
    "capabilities": {
      "drop": [
        "ALL"
      ]
    },
    "readOnlyRootFilesystem": true
  },
  "env": [],
  "extraArgs": {},
  "extraContainers": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "initContainers": [],
  "labels": {},
  "nodeSelector": {},
  "podSecurityContext": {
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001
  },
  "readinessProbe": {
    "httpGet": {
      "path": "/ready",
      "port": "http-metrics"
    },
    "initialDelaySeconds": 45
  },
  "replicas": 1,
  "resources": {},
  "service": {
    "annotations": {},
    "labels": {}
  },
  "strategy": {
    "type": "RollingUpdate"
  },
  "terminationGracePeriodSeconds": 60,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.affinity</td>
			<td>object</td>
			<td>Affinity for admin-api Pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.annotations</td>
			<td>object</td>
			<td>Additional annotations for the `admin-api` Deployment</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.env</td>
			<td>list</td>
			<td>Configure optional environment variables</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.extraArgs</td>
			<td>object</td>
			<td>Additional CLI arguments for the `admin-api` target</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.extraContainers</td>
			<td>list</td>
			<td>Conifgure optional extraContainers</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.extraVolumeMounts</td>
			<td>list</td>
			<td>Additional volume mounts for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.extraVolumes</td>
			<td>list</td>
			<td>Additional volumes for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.initContainers</td>
			<td>list</td>
			<td>Configure optional initContainers</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.labels</td>
			<td>object</td>
			<td>Additional labels for the `admin-api` Deployment</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.nodeSelector</td>
			<td>object</td>
			<td>Node selector for admin-api Pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.podSecurityContext</td>
			<td>object</td>
			<td>Run container as user `enterprise-logs(uid=10001)` `fsGroup` must not be specified, because these security options are applied on container level not on Pod level.</td>
			<td><pre lang="json">
{
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001
}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.readinessProbe</td>
			<td>object</td>
			<td>Readiness probe</td>
			<td><pre lang="json">
{
  "httpGet": {
    "path": "/ready",
    "port": "http-metrics"
  },
  "initialDelaySeconds": 45
}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.replicas</td>
			<td>int</td>
			<td>Define the amount of instances</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.resources</td>
			<td>object</td>
			<td>Values are defined in small.yaml and large.yaml</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.service</td>
			<td>object</td>
			<td>Additional labels and annotations for the `admin-api` Service</td>
			<td><pre lang="json">
{
  "annotations": {},
  "labels": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.strategy</td>
			<td>object</td>
			<td>Update strategy</td>
			<td><pre lang="json">
{
  "type": "RollingUpdate"
}
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the admin-api to shutdown before it is killed</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>adminApi.tolerations</td>
			<td>list</td>
			<td>Tolerations for admin-api Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend</td>
			<td>object</td>
			<td>Configuration for the backend pod(s)</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "backend"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "annotations": {},
  "autoscaling": {
    "behavior": {},
    "enabled": false,
    "maxReplicas": 6,
    "minReplicas": 3,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "dnsConfig": {},
  "extraArgs": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "nodeSelector": {},
  "persistence": {
    "dataVolumeParameters": {
      "emptyDir": {}
    },
    "enableStatefulSetAutoDeletePVC": true,
    "selector": null,
    "size": "10Gi",
    "storageClass": null,
    "volumeClaimsEnabled": true
  },
  "podAnnotations": {},
  "podLabels": {},
  "podManagementPolicy": "Parallel",
  "priorityClassName": null,
  "replicas": 3,
  "resources": {},
  "selectorLabels": {},
  "service": {
    "annotations": {},
    "labels": {}
  },
  "targetModule": "backend",
  "terminationGracePeriodSeconds": 300,
  "tolerations": [],
  "topologySpreadConstraints": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.affinity</td>
			<td>object</td>
			<td>Affinity for backend pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>backend.annotations</td>
			<td>object</td>
			<td>Annotations for backend StatefulSet</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.behavior</td>
			<td>object</td>
			<td>Behavior policies while scaling.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the backend.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the backend.</td>
			<td><pre lang="json">
6
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the backend.</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilization percentage for the backend.</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>backend.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilization percentage for the backend.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.dnsConfig</td>
			<td>object</td>
			<td>DNS config for backend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the backend</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the backend image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the backend image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the backend image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.nodeSelector</td>
			<td>object</td>
			<td>Node selector for backend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.dataVolumeParameters</td>
			<td>object</td>
			<td>Parameters used for the `data` volume when volumeClaimEnabled if false</td>
			<td><pre lang="json">
{
  "emptyDir": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.selector</td>
			<td>string</td>
			<td>Selector for persistent disk</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.persistence.volumeClaimsEnabled</td>
			<td>bool</td>
			<td>Enable volume claims in pod spec</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>backend.podAnnotations</td>
			<td>object</td>
			<td>Annotations for backend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.podLabels</td>
			<td>object</td>
			<td>Additional labels for each `backend` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.podManagementPolicy</td>
			<td>string</td>
			<td>The default is to deploy all pods in parallel.</td>
			<td><pre lang="json">
"Parallel"
</pre>
</td>
		</tr>
		<tr>
			<td>backend.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for backend pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>backend.replicas</td>
			<td>int</td>
			<td>Number of replicas for the backend</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>backend.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the backend</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.selectorLabels</td>
			<td>object</td>
			<td>Additional selector labels for each `backend` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.service.annotations</td>
			<td>object</td>
			<td>Annotations for backend Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.service.labels</td>
			<td>object</td>
			<td>Additional labels for backend Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>backend.targetModule</td>
			<td>string</td>
			<td>Comma-separated list of Loki modules to load for the read</td>
			<td><pre lang="json">
"backend"
</pre>
</td>
		</tr>
		<tr>
			<td>backend.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the backend to shutdown before it is killed. Especially for the ingester, this must be increased. It must be long enough so backends can be gracefully shutdown flushing/transferring all data and to successfully leave the member ring on shutdown.</td>
			<td><pre lang="json">
300
</pre>
</td>
		</tr>
		<tr>
			<td>backend.tolerations</td>
			<td>list</td>
			<td>Tolerations for backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>backend.topologySpreadConstraints</td>
			<td>list</td>
			<td>Topology Spread Constraints for backend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor</td>
			<td>object</td>
			<td>Configuration for the bloom compactor</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "bloom-compactor"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "livenessProbe": {},
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "claims": [
      {
        "name": "data",
        "size": "10Gi",
        "storageClass": null
      }
    ],
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "size": "10Gi",
    "storageClass": null,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "readinessProbe": {},
  "replicas": 0,
  "resources": {},
  "serviceAccount": {
    "annotations": {},
    "automountServiceAccountToken": true,
    "create": false,
    "imagePullSecrets": [],
    "name": null
  },
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.affinity</td>
			<td>object</td>
			<td>Affinity for bloom compactor pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the bloom compactor</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the bloom compactor image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the bloom compactor image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the bloom compactor image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.livenessProbe</td>
			<td>object</td>
			<td>liveness probe settings for ingester pods. If empty use `loki.livenessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.nodeSelector</td>
			<td>object</td>
			<td>Node selector for bloom compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for bloom compactor PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.claims</td>
			<td>list</td>
			<td>List of the bloom compactor PVCs</td>
			<td><pre lang="list">

</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs for the bloom compactor</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.podAnnotations</td>
			<td>object</td>
			<td>Annotations for bloom compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.podLabels</td>
			<td>object</td>
			<td>Labels for bloom compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for bloom compactor pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.readinessProbe</td>
			<td>object</td>
			<td>readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.replicas</td>
			<td>int</td>
			<td>Number of replicas for the bloom compactor</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the bloom compactor</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.serviceAccount.annotations</td>
			<td>object</td>
			<td>Annotations for the bloom compactor service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.serviceAccount.automountServiceAccountToken</td>
			<td>bool</td>
			<td>Set this toggle to false to opt out of automounting API credentials for the service account</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.serviceAccount.imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for the bloom compactor service account</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.serviceAccount.name</td>
			<td>string</td>
			<td>The name of the ServiceAccount to use for the bloom compactor. If not set and create is true, a name is generated by appending "-bloom-compactor" to the common ServiceAccount.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.serviceLabels</td>
			<td>object</td>
			<td>Labels for bloom compactor service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the bloom compactor to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>bloomCompactor.tolerations</td>
			<td>list</td>
			<td>Tolerations for bloom compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway</td>
			<td>object</td>
			<td>Configuration for the bloom gateway</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "bloom-gateway"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "livenessProbe": {},
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "claims": [
      {
        "name": "data",
        "size": "10Gi",
        "storageClass": null
      }
    ],
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "size": "10Gi",
    "storageClass": null,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "readinessProbe": {},
  "replicas": 0,
  "resources": {},
  "serviceAccount": {
    "annotations": {},
    "automountServiceAccountToken": true,
    "create": false,
    "imagePullSecrets": [],
    "name": null
  },
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.affinity</td>
			<td>object</td>
			<td>Affinity for bloom gateway pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the bloom gateway</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the bloom gateway image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the bloom gateway image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the bloom gateway image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.livenessProbe</td>
			<td>object</td>
			<td>liveness probe settings for ingester pods. If empty use `loki.livenessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.nodeSelector</td>
			<td>object</td>
			<td>Node selector for bloom gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for bloom gateway PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.claims</td>
			<td>list</td>
			<td>List of the bloom gateway PVCs</td>
			<td><pre lang="list">

</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs for the bloom gateway</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.podAnnotations</td>
			<td>object</td>
			<td>Annotations for bloom gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.podLabels</td>
			<td>object</td>
			<td>Labels for bloom gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for bloom gateway pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.readinessProbe</td>
			<td>object</td>
			<td>readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.replicas</td>
			<td>int</td>
			<td>Number of replicas for the bloom gateway</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the bloom gateway</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.serviceAccount.annotations</td>
			<td>object</td>
			<td>Annotations for the bloom gateway service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.serviceAccount.automountServiceAccountToken</td>
			<td>bool</td>
			<td>Set this toggle to false to opt out of automounting API credentials for the service account</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.serviceAccount.imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for the bloom gateway service account</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.serviceAccount.name</td>
			<td>string</td>
			<td>The name of the ServiceAccount to use for the bloom gateway. If not set and create is true, a name is generated by appending "-bloom-gateway" to the common ServiceAccount.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.serviceLabels</td>
			<td>object</td>
			<td>Labels for bloom gateway service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the bloom gateway to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>bloomGateway.tolerations</td>
			<td>list</td>
			<td>Tolerations for bloom gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.affinity</td>
			<td>object</td>
			<td>Affinity for chunks-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.allocatedMemory</td>
			<td>int</td>
			<td>Amount of memory allocated to chunks-cache for object storage (in MB).</td>
			<td><pre lang="json">
8192
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.annotations</td>
			<td>object</td>
			<td>Annotations for the chunks-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.batchSize</td>
			<td>int</td>
			<td>Batchsize for sending and receiving chunks from chunks cache</td>
			<td><pre lang="json">
4
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.connectionLimit</td>
			<td>int</td>
			<td>Maximum number of connections allowed</td>
			<td><pre lang="json">
16384
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.defaultValidity</td>
			<td>string</td>
			<td>Specify how long cached chunks should be stored in the chunks-cache before being expired</td>
			<td><pre lang="json">
"0s"
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.enabled</td>
			<td>bool</td>
			<td>Specifies whether memcached based chunks-cache should be enabled</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.extraArgs</td>
			<td>object</td>
			<td>Additional CLI args for chunks-cache</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.extraContainers</td>
			<td>list</td>
			<td>Additional containers to be added to the chunks-cache pod.</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.extraExtendedOptions</td>
			<td>string</td>
			<td>Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag. Example: extraExtendedOptions: 'tls,no_hashexpand'</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.extraVolumeMounts</td>
			<td>list</td>
			<td>Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers). Example: extraVolumeMounts: - name: extra-volume   mountPath: /etc/extra-volume   readOnly: true</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.extraVolumes</td>
			<td>list</td>
			<td>Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers). Example: extraVolumes: - name: extra-volume   secret:    secretName: extra-volume-secret</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.initContainers</td>
			<td>list</td>
			<td>Extra init containers for chunks-cache pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.maxItemMemory</td>
			<td>int</td>
			<td>Maximum item memory for chunks-cache (in MB).</td>
			<td><pre lang="json">
5
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.nodeSelector</td>
			<td>object</td>
			<td>Node selector for chunks-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.parallelism</td>
			<td>int</td>
			<td>Parallel threads for sending and receiving chunks from chunks cache</td>
			<td><pre lang="json">
5
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.podAnnotations</td>
			<td>object</td>
			<td>Annotations for chunks-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.podDisruptionBudget</td>
			<td>object</td>
			<td>Pod Disruption Budget</td>
			<td><pre lang="json">
{
  "maxUnavailable": 1
}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.podLabels</td>
			<td>object</td>
			<td>Labels for chunks-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.podManagementPolicy</td>
			<td>string</td>
			<td>Management policy for chunks-cache pods</td>
			<td><pre lang="json">
"Parallel"
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.port</td>
			<td>int</td>
			<td>Port of the chunks-cache service</td>
			<td><pre lang="json">
11211
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for chunks-cache pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.replicas</td>
			<td>int</td>
			<td>Total number of chunks-cache replicas</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.resources</td>
			<td>string</td>
			<td>Resource requests and limits for the chunks-cache By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.service</td>
			<td>object</td>
			<td>Service annotations and labels</td>
			<td><pre lang="json">
{
  "annotations": {},
  "labels": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.statefulStrategy</td>
			<td>object</td>
			<td>Stateful chunks-cache strategy</td>
			<td><pre lang="json">
{
  "type": "RollingUpdate"
}
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the chunks-cache to shutdown before it is killed</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.timeout</td>
			<td>string</td>
			<td>Memcached operation timeout</td>
			<td><pre lang="json">
"2000ms"
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.tolerations</td>
			<td>list</td>
			<td>Tolerations for chunks-cache pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.topologySpreadConstraints</td>
			<td>list</td>
			<td>topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints. labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.writebackBuffer</td>
			<td>int</td>
			<td>Max number of objects to use for cache write back</td>
			<td><pre lang="json">
500000
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.writebackParallelism</td>
			<td>int</td>
			<td>Number of parallel threads for cache write back</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>chunksCache.writebackSizeLimit</td>
			<td>string</td>
			<td>Max memory to use for cache write back</td>
			<td><pre lang="json">
"500MB"
</pre>
</td>
		</tr>
		<tr>
			<td>clusterLabelOverride</td>
			<td>string</td>
			<td>Overrides the chart's cluster label</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor</td>
			<td>object</td>
			<td>Configuration for the compactor</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "compactor"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "livenessProbe": {},
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "claims": [
      {
        "name": "data",
        "size": "10Gi",
        "storageClass": null
      }
    ],
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "size": "10Gi",
    "storageClass": null,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "readinessProbe": {},
  "replicas": 0,
  "resources": {},
  "serviceAccount": {
    "annotations": {},
    "automountServiceAccountToken": true,
    "create": false,
    "imagePullSecrets": [],
    "name": null
  },
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.affinity</td>
			<td>object</td>
			<td>Affinity for compactor pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the compactor</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the compactor image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the compactor image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the compactor image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.livenessProbe</td>
			<td>object</td>
			<td>liveness probe settings for ingester pods. If empty use `loki.livenessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.nodeSelector</td>
			<td>object</td>
			<td>Node selector for compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for compactor PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.claims</td>
			<td>list</td>
			<td>List of the compactor PVCs</td>
			<td><pre lang="list">

</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs for the compactor</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.podAnnotations</td>
			<td>object</td>
			<td>Annotations for compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.podLabels</td>
			<td>object</td>
			<td>Labels for compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for compactor pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.readinessProbe</td>
			<td>object</td>
			<td>readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.replicas</td>
			<td>int</td>
			<td>Number of replicas for the compactor</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the compactor</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.serviceAccount.annotations</td>
			<td>object</td>
			<td>Annotations for the compactor service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.serviceAccount.automountServiceAccountToken</td>
			<td>bool</td>
			<td>Set this toggle to false to opt out of automounting API credentials for the service account</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.serviceAccount.imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for the compactor service account</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.serviceAccount.name</td>
			<td>string</td>
			<td>The name of the ServiceAccount to use for the compactor. If not set and create is true, a name is generated by appending "-compactor" to the common ServiceAccount.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.serviceLabels</td>
			<td>object</td>
			<td>Labels for compactor service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the compactor to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>compactor.tolerations</td>
			<td>list</td>
			<td>Tolerations for compactor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>deploymentMode</td>
			<td>string</td>
			<td>Deployment mode lets you specify how to deploy Loki. There are 3 options: - SingleBinary: Loki is deployed as a single binary, useful for small installs typically without HA, up to a few tens of GB/day. - SimpleScalable: Loki is deployed as 3 targets: read, write, and backend. Useful for medium installs easier to manage than distributed, up to a about 1TB/day. - Distributed: Loki is deployed as individual microservices. The most complicated but most capable, useful for large installs, typically over 1TB/day. There are also 2 additional modes used for migrating between deployment modes: - SingleBinary<->SimpleScalable: Migrate from SingleBinary to SimpleScalable (or vice versa) - SimpleScalable<->Distributed: Migrate from SimpleScalable to Distributed (or vice versa) Note: SimpleScalable and Distributed REQUIRE the use of object storage.</td>
			<td><pre lang="json">
"SimpleScalable"
</pre>
</td>
		</tr>
		<tr>
			<td>distributor</td>
			<td>object</td>
			<td>Configuration for the distributor</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "distributor"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "autoscaling": {
    "behavior": {
      "enabled": false,
      "scaleDown": {},
      "scaleUp": {}
    },
    "customMetrics": [],
    "enabled": false,
    "maxReplicas": 3,
    "minReplicas": 1,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "maxSurge": 0,
  "maxUnavailable": null,
  "nodeSelector": {},
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.affinity</td>
			<td>object</td>
			<td>Affinity for distributor pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.appProtocol</td>
			<td>object</td>
			<td>Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.appProtocol.grpc</td>
			<td>string</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.behavior.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling behaviours</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.behavior.scaleDown</td>
			<td>object</td>
			<td>define scale down policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.behavior.scaleUp</td>
			<td>object</td>
			<td>define scale up policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.customMetrics</td>
			<td>list</td>
			<td>Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the distributor</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the distributor</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the distributor</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the distributor</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the distributor</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the distributor</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the distributor image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the distributor image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the distributor image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.maxSurge</td>
			<td>int</td>
			<td>Max Surge for distributor pods</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.maxUnavailable</td>
			<td>string</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.nodeSelector</td>
			<td>object</td>
			<td>Node selector for distributor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.podAnnotations</td>
			<td>object</td>
			<td>Annotations for distributor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.podLabels</td>
			<td>object</td>
			<td>Labels for distributor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for distributor pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.replicas</td>
			<td>int</td>
			<td>Number of replicas for the distributor</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the distributor</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.serviceLabels</td>
			<td>object</td>
			<td>Labels for distributor service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the distributor to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>distributor.tolerations</td>
			<td>list</td>
			<td>Tolerations for distributor pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise</td>
			<td>object</td>
			<td>Configuration for running Enterprise Loki</td>
			<td><pre lang="json">
{
  "adminApi": {
    "enabled": true
  },
  "adminToken": {
    "additionalNamespaces": [],
    "secret": null
  },
  "canarySecret": null,
  "cluster_name": null,
  "config": "{{- if .Values.enterprise.adminApi.enabled }}\n{{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") (eq .Values.loki.storage.type \"azure\") }}\nadmin_client:\n  storage:\n    s3:\n      bucket_name: admin\n{{- end }}\n{{- end }}\nauth:\n  type: {{ .Values.enterprise.adminApi.enabled | ternary \"enterprise\" \"trust\" }}\nauth_enabled: {{ .Values.loki.auth_enabled }}\ncluster_name: {{ include \"loki.clusterName\" . }}\nlicense:\n  path: /etc/loki/license/license.jwt\n",
  "enabled": false,
  "externalConfigName": "",
  "externalLicenseName": null,
  "gelGateway": true,
  "image": {
    "digest": null,
    "pullPolicy": "IfNotPresent",
    "registry": "docker.io",
    "repository": "grafana/enterprise-logs",
    "tag": null
  },
  "license": {
    "contents": "NOTAVALIDLICENSE"
  },
  "provisioner": {
    "additionalTenants": [],
    "annotations": {},
    "enabled": true,
    "env": [],
    "extraVolumeMounts": [],
    "image": {
      "digest": null,
      "pullPolicy": "IfNotPresent",
      "registry": "docker.io",
      "repository": "grafana/enterprise-logs-provisioner",
      "tag": null
    },
    "labels": {},
    "priorityClassName": null,
    "provisionedSecretPrefix": null,
    "securityContext": {
      "fsGroup": 10001,
      "runAsGroup": 10001,
      "runAsNonRoot": true,
      "runAsUser": 10001
    }
  },
  "tokengen": {
    "annotations": {},
    "enabled": true,
    "env": [],
    "extraArgs": [],
    "extraEnvFrom": [],
    "extraVolumeMounts": [],
    "extraVolumes": [],
    "labels": {},
    "priorityClassName": "",
    "securityContext": {
      "fsGroup": 10001,
      "runAsGroup": 10001,
      "runAsNonRoot": true,
      "runAsUser": 10001
    },
    "targetModule": "tokengen",
    "tolerations": []
  },
  "useExternalLicense": false,
  "version": "v3.0.0"
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.adminApi</td>
			<td>object</td>
			<td>If enabled, the correct admin_client storage will be configured. If disabled while running enterprise, make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.</td>
			<td><pre lang="json">
{
  "enabled": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.adminToken.additionalNamespaces</td>
			<td>list</td>
			<td>Additional namespace to also create the token in. Useful if your Grafana instance is in a different namespace</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.adminToken.secret</td>
			<td>string</td>
			<td>Alternative name for admin token secret, needed by tokengen and provisioner jobs</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.canarySecret</td>
			<td>string</td>
			<td>Alternative name of the secret to store token for the canary</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.cluster_name</td>
			<td>string</td>
			<td>Optional name of the GEL cluster, otherwise will use .Release.Name The cluster name must match what is in your GEL license</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.externalConfigName</td>
			<td>string</td>
			<td>Name of the external config secret to use</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.externalLicenseName</td>
			<td>string</td>
			<td>Name of external license secret to use</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.gelGateway</td>
			<td>bool</td>
			<td>Use GEL gateway, if false will use the default nginx gateway</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.image.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.image.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.image.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"grafana/enterprise-logs"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.image.tag</td>
			<td>string</td>
			<td>Docker image tag</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.license</td>
			<td>object</td>
			<td>Grafana Enterprise Logs license In order to use Grafana Enterprise Logs features, you will need to provide the contents of your Grafana Enterprise Logs license, either by providing the contents of the license.jwt, or the name Kubernetes Secret that contains your license.jwt. To set the license contents, use the flag `--set-file 'enterprise.license.contents=./license.jwt'`</td>
			<td><pre lang="json">
{
  "contents": "NOTAVALIDLICENSE"
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner</td>
			<td>object</td>
			<td>Configuration for `provisioner` target</td>
			<td><pre lang="json">
{
  "additionalTenants": [],
  "annotations": {},
  "enabled": true,
  "env": [],
  "extraVolumeMounts": [],
  "image": {
    "digest": null,
    "pullPolicy": "IfNotPresent",
    "registry": "docker.io",
    "repository": "grafana/enterprise-logs-provisioner",
    "tag": null
  },
  "labels": {},
  "priorityClassName": null,
  "provisionedSecretPrefix": null,
  "securityContext": {
    "fsGroup": 10001,
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.additionalTenants</td>
			<td>list</td>
			<td>Additional tenants to be created. Each tenant will get a read and write policy and associated token. Tenant must have a name and a namespace for the secret containting the token to be created in. For example additionalTenants:   - name: loki     secretNamespace: grafana</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.annotations</td>
			<td>object</td>
			<td>Additional annotations for the `provisioner` Job</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.enabled</td>
			<td>bool</td>
			<td>Whether the job should be part of the deployment</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.env</td>
			<td>list</td>
			<td>Additional Kubernetes environment</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the provisioner pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image</td>
			<td>object</td>
			<td>Provisioner image to Utilize</td>
			<td><pre lang="json">
{
  "digest": null,
  "pullPolicy": "IfNotPresent",
  "registry": "docker.io",
  "repository": "grafana/enterprise-logs-provisioner",
  "tag": null
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"grafana/enterprise-logs-provisioner"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.image.tag</td>
			<td>string</td>
			<td>Overrides the image tag whose default is the chart's appVersion</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.labels</td>
			<td>object</td>
			<td>Additional labels for the `provisioner` Job</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for provisioner Job</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.provisionedSecretPrefix</td>
			<td>string</td>
			<td>Name of the secret to store provisioned tokens in</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.provisioner.securityContext</td>
			<td>object</td>
			<td>Run containers as user `enterprise-logs(uid=10001)`</td>
			<td><pre lang="json">
{
  "fsGroup": 10001,
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen</td>
			<td>object</td>
			<td>Configuration for `tokengen` target</td>
			<td><pre lang="json">
{
  "annotations": {},
  "enabled": true,
  "env": [],
  "extraArgs": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "labels": {},
  "priorityClassName": "",
  "securityContext": {
    "fsGroup": 10001,
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001
  },
  "targetModule": "tokengen",
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.annotations</td>
			<td>object</td>
			<td>Additional annotations for the `tokengen` Job</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.enabled</td>
			<td>bool</td>
			<td>Whether the job should be part of the deployment</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.env</td>
			<td>list</td>
			<td>Additional Kubernetes environment</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.extraArgs</td>
			<td>list</td>
			<td>Additional CLI arguments for the `tokengen` target</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the tokengen pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.extraVolumeMounts</td>
			<td>list</td>
			<td>Additional volume mounts for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.extraVolumes</td>
			<td>list</td>
			<td>Additional volumes for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.labels</td>
			<td>object</td>
			<td>Additional labels for the `tokengen` Job</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for tokengen Pods</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.securityContext</td>
			<td>object</td>
			<td>Run containers as user `enterprise-logs(uid=10001)`</td>
			<td><pre lang="json">
{
  "fsGroup": 10001,
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.targetModule</td>
			<td>string</td>
			<td>Comma-separated list of Loki modules to load for tokengen</td>
			<td><pre lang="json">
"tokengen"
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.tokengen.tolerations</td>
			<td>list</td>
			<td>Tolerations for tokengen Job</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterprise.useExternalLicense</td>
			<td>bool</td>
			<td>Set to true when providing an external license</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway</td>
			<td>object</td>
			<td>If running enterprise and using the default enterprise gateway, configs go here.</td>
			<td><pre lang="json">
{
  "affinity": {},
  "annotations": {},
  "containerSecurityContext": {
    "allowPrivilegeEscalation": false,
    "capabilities": {
      "drop": [
        "ALL"
      ]
    },
    "readOnlyRootFilesystem": true
  },
  "env": [],
  "extraArgs": {},
  "extraContainers": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "initContainers": [],
  "labels": {},
  "nodeSelector": {},
  "podSecurityContext": {
    "fsGroup": 10001,
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001
  },
  "readinessProbe": {
    "httpGet": {
      "path": "/ready",
      "port": "http-metrics"
    },
    "initialDelaySeconds": 45
  },
  "replicas": 1,
  "resources": {},
  "service": {
    "annotations": {},
    "labels": {},
    "type": "ClusterIP"
  },
  "strategy": {
    "type": "RollingUpdate"
  },
  "terminationGracePeriodSeconds": 60,
  "tolerations": [],
  "useDefaultProxyURLs": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.affinity</td>
			<td>object</td>
			<td>Affinity for gateway Pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.annotations</td>
			<td>object</td>
			<td>Additional annotations for the `gateway` Pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.env</td>
			<td>list</td>
			<td>Configure optional environment variables</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.extraArgs</td>
			<td>object</td>
			<td>Additional CLI arguments for the `gateway` target</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.extraContainers</td>
			<td>list</td>
			<td>Conifgure optional extraContainers</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.extraVolumeMounts</td>
			<td>list</td>
			<td>Additional volume mounts for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.extraVolumes</td>
			<td>list</td>
			<td>Additional volumes for Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.initContainers</td>
			<td>list</td>
			<td>Configure optional initContainers</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.labels</td>
			<td>object</td>
			<td>Additional labels for the `gateway` Pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.nodeSelector</td>
			<td>object</td>
			<td>Node selector for gateway Pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.podSecurityContext</td>
			<td>object</td>
			<td>Run container as user `enterprise-logs(uid=10001)`</td>
			<td><pre lang="json">
{
  "fsGroup": 10001,
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.readinessProbe</td>
			<td>object</td>
			<td>Readiness probe</td>
			<td><pre lang="json">
{
  "httpGet": {
    "path": "/ready",
    "port": "http-metrics"
  },
  "initialDelaySeconds": 45
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.replicas</td>
			<td>int</td>
			<td>Define the amount of instances</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.resources</td>
			<td>object</td>
			<td>Values are defined in small.yaml and large.yaml</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.service</td>
			<td>object</td>
			<td>Service overriding service type</td>
			<td><pre lang="json">
{
  "annotations": {},
  "labels": {},
  "type": "ClusterIP"
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.strategy</td>
			<td>object</td>
			<td>update strategy</td>
			<td><pre lang="json">
{
  "type": "RollingUpdate"
}
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the gateway to shutdown before it is killed</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.tolerations</td>
			<td>list</td>
			<td>Tolerations for gateway Pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>enterpriseGateway.useDefaultProxyURLs</td>
			<td>bool</td>
			<td>If you want to use your own proxy URLs, set this to false.</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>extraObjects</td>
			<td>list</td>
			<td></td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>fullnameOverride</td>
			<td>string</td>
			<td>Overrides the chart's computed fullname</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.affinity</td>
			<td>object</td>
			<td>Affinity for gateway pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.annotations</td>
			<td>object</td>
			<td>Annotations for gateway deployment</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.behavior</td>
			<td>object</td>
			<td>Behavior policies while scaling.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the gateway</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the gateway</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the gateway</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the gateway</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the gateway</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.basicAuth.enabled</td>
			<td>bool</td>
			<td>Enables basic authentication for the gateway</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.basicAuth.existingSecret</td>
			<td>string</td>
			<td>Existing basic auth secret to use. Must contain '.htpasswd'</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.basicAuth.htpasswd</td>
			<td>string</td>
			<td>Uses the specified users from the `loki.tenants` list to create the htpasswd file if `loki.tenants` is not set, the `gateway.basicAuth.username` and `gateway.basicAuth.password` are used The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes high CPU load.</td>
			<td><pre lang="json">
"{{ if .Values.loki.tenants }}\n\n\n  {{- range $t := .Values.loki.tenants }}\n{{ htpasswd (required \"All tenants must have a 'name' set\" $t.name) (required \"All tenants must have a 'password' set\" $t.password) }}\n\n\n  {{- end }}\n{{ else }} {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }} {{ end }}"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.basicAuth.password</td>
			<td>string</td>
			<td>The basic auth password for the gateway</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.basicAuth.username</td>
			<td>string</td>
			<td>The basic auth username for the gateway</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.containerSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for gateway containers</td>
			<td><pre lang="json">
{
  "allowPrivilegeEscalation": false,
  "capabilities": {
    "drop": [
      "ALL"
    ]
  },
  "readOnlyRootFilesystem": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.deploymentStrategy.type</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"RollingUpdate"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.dnsConfig</td>
			<td>object</td>
			<td>DNS config for gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.enabled</td>
			<td>bool</td>
			<td>Specifies whether the gateway should be enabled</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the gateway</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.image.digest</td>
			<td>string</td>
			<td>Overrides the gateway image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.image.pullPolicy</td>
			<td>string</td>
			<td>The gateway image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the gateway image</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.image.repository</td>
			<td>string</td>
			<td>The gateway image repository</td>
			<td><pre lang="json">
"nginxinc/nginx-unprivileged"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.image.tag</td>
			<td>string</td>
			<td>The gateway image tag</td>
			<td><pre lang="json">
"1.24-alpine"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.annotations</td>
			<td>object</td>
			<td>Annotations for the gateway ingress</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.enabled</td>
			<td>bool</td>
			<td>Specifies whether an ingress for the gateway should be created</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.hosts</td>
			<td>list</td>
			<td>Hosts configuration for the gateway ingress, passed through the `tpl` function to allow templating</td>
			<td><pre lang="json">
[
  {
    "host": "gateway.loki.example.com",
    "paths": [
      {
        "path": "/"
      }
    ]
  }
]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.ingressClassName</td>
			<td>string</td>
			<td>Ingress Class Name. MAY be required for Kubernetes versions >= 1.18</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.labels</td>
			<td>object</td>
			<td>Labels for the gateway ingress</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.ingress.tls</td>
			<td>list</td>
			<td>TLS configuration for the gateway ingress. Hosts passed through the `tpl` function to allow templating</td>
			<td><pre lang="json">
[
  {
    "hosts": [
      "gateway.loki.example.com"
    ],
    "secretName": "loki-gateway-tls"
  }
]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.lifecycle</td>
			<td>object</td>
			<td>Lifecycle for the gateway container</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.customBackendUrl</td>
			<td>string</td>
			<td>Override Backend URL</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.customReadUrl</td>
			<td>string</td>
			<td>Override Read URL</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.customWriteUrl</td>
			<td>string</td>
			<td>Override Write URL</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.enableIPv6</td>
			<td>bool</td>
			<td>Enable listener for IPv6, disable on IPv4-only systems</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.file</td>
			<td>string</td>
			<td>Config file contents for Nginx. Passed through the `tpl` function to allow templating</td>
			<td><pre lang="">
See values.yaml
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.httpSnippet</td>
			<td>string</td>
			<td>Allows appending custom configuration to the http block, passed through the `tpl` function to allow templating</td>
			<td><pre lang="json">
"{{ if .Values.loki.tenants }}proxy_set_header X-Scope-OrgID $remote_user;{{ end }}"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.logFormat</td>
			<td>string</td>
			<td>NGINX log format</td>
			<td><pre lang="json">
"main '$remote_addr - $remote_user [$time_local]  $status '\n        '\"$request\" $body_bytes_sent \"$http_referer\" '\n        '\"$http_user_agent\" \"$http_x_forwarded_for\"';"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.resolver</td>
			<td>string</td>
			<td>Allows overriding the DNS resolver address nginx will use.</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.schema</td>
			<td>string</td>
			<td>Which schema to be used when building URLs. Can be 'http' or 'https'.</td>
			<td><pre lang="json">
"http"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.serverSnippet</td>
			<td>string</td>
			<td>Allows appending custom configuration to the server block</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nginxConfig.ssl</td>
			<td>bool</td>
			<td>Whether ssl should be appended to the listen directive of the server block or not.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.nodeSelector</td>
			<td>object</td>
			<td>Node selector for gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.podAnnotations</td>
			<td>object</td>
			<td>Annotations for gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.podLabels</td>
			<td>object</td>
			<td>Additional labels for gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.podSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for gateway containers</td>
			<td><pre lang="json">
{
  "fsGroup": 101,
  "runAsGroup": 101,
  "runAsNonRoot": true,
  "runAsUser": 101
}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for gateway pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.readinessProbe.httpGet.path</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"/"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.readinessProbe.httpGet.port</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"http-metrics"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.readinessProbe.initialDelaySeconds</td>
			<td>int</td>
			<td></td>
			<td><pre lang="json">
15
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.readinessProbe.timeoutSeconds</td>
			<td>int</td>
			<td></td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.replicas</td>
			<td>int</td>
			<td>Number of replicas for the gateway</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the gateway</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.annotations</td>
			<td>object</td>
			<td>Annotations for the gateway service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.clusterIP</td>
			<td>string</td>
			<td>ClusterIP of the gateway service</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.labels</td>
			<td>object</td>
			<td>Labels for gateway service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.loadBalancerIP</td>
			<td>string</td>
			<td>Load balancer IPO address if service type is LoadBalancer</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.nodePort</td>
			<td>int</td>
			<td>Node port if service type is NodePort</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.port</td>
			<td>int</td>
			<td>Port of the gateway service</td>
			<td><pre lang="json">
80
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.service.type</td>
			<td>string</td>
			<td>Type of the gateway service</td>
			<td><pre lang="json">
"ClusterIP"
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the gateway to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.tolerations</td>
			<td>list</td>
			<td>Tolerations for gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.topologySpreadConstraints</td>
			<td>list</td>
			<td>Topology Spread Constraints for gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>gateway.verboseLogging</td>
			<td>bool</td>
			<td>Enable logging of 2xx and 3xx HTTP requests</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>global.clusterDomain</td>
			<td>string</td>
			<td>configures cluster domain ("cluster.local" by default)</td>
			<td><pre lang="json">
"cluster.local"
</pre>
</td>
		</tr>
		<tr>
			<td>global.dnsNamespace</td>
			<td>string</td>
			<td>configures DNS service namespace</td>
			<td><pre lang="json">
"kube-system"
</pre>
</td>
		</tr>
		<tr>
			<td>global.dnsService</td>
			<td>string</td>
			<td>configures DNS service name</td>
			<td><pre lang="json">
"kube-dns"
</pre>
</td>
		</tr>
		<tr>
			<td>global.image.registry</td>
			<td>string</td>
			<td>Overrides the Docker registry globally for all images</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>global.priorityClassName</td>
			<td>string</td>
			<td>Overrides the priorityClassName for all pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for Docker images</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway</td>
			<td>object</td>
			<td>Configuration for the index-gateway</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "index-gateway"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "joinMemberlist": true,
  "maxUnavailable": null,
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "inMemory": false,
    "size": "10Gi",
    "storageClass": null,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 300,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.affinity</td>
			<td>object</td>
			<td>Affinity for index-gateway pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the index-gateway</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the index-gateway image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the index-gateway image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the index-gateway image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.joinMemberlist</td>
			<td>bool</td>
			<td>Whether the index gateway should join the memberlist hashring</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.maxUnavailable</td>
			<td>string</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.nodeSelector</td>
			<td>object</td>
			<td>Node selector for index-gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for index gateway PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs which is required when using boltdb-shipper</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.inMemory</td>
			<td>bool</td>
			<td>Use emptyDir with ramdisk for storage. **Please note that all data in indexGateway will be lost on pod restart**</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.size</td>
			<td>string</td>
			<td>Size of persistent or memory disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.podAnnotations</td>
			<td>object</td>
			<td>Annotations for index-gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.podLabels</td>
			<td>object</td>
			<td>Labels for index-gateway pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for index-gateway pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.replicas</td>
			<td>int</td>
			<td>Number of replicas for the index-gateway</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the index-gateway</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.serviceLabels</td>
			<td>object</td>
			<td>Labels for index-gateway service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the index-gateway to shutdown before it is killed.</td>
			<td><pre lang="json">
300
</pre>
</td>
		</tr>
		<tr>
			<td>indexGateway.tolerations</td>
			<td>list</td>
			<td>Tolerations for index-gateway pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester</td>
			<td>object</td>
			<td>Configuration for the ingester</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "ingester"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "autoscaling": {
    "behavior": {
      "enabled": false,
      "scaleDown": {},
      "scaleUp": {}
    },
    "customMetrics": [],
    "enabled": false,
    "maxReplicas": 3,
    "minReplicas": 1,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "lifecycle": {},
  "livenessProbe": {},
  "maxUnavailable": 1,
  "nodeSelector": {},
  "persistence": {
    "claims": [
      {
        "name": "data",
        "size": "10Gi",
        "storageClass": null
      }
    ],
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "inMemory": false,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "readinessProbe": {},
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 300,
  "tolerations": [],
  "topologySpreadConstraints": [
    {
      "labelSelector": {
        "matchLabels": {
          "app.kubernetes.io/component": "ingester"
        }
      },
      "maxSkew": 1,
      "topologyKey": "kubernetes.io/hostname",
      "whenUnsatisfiable": "ScheduleAnyway"
    }
  ],
  "zoneAwareReplication": {
    "enabled": true,
    "maxUnavailablePct": 33,
    "migration": {
      "enabled": false,
      "excludeDefaultZone": false,
      "readPath": false,
      "writePath": false
    },
    "zoneA": {
      "annotations": {},
      "extraAffinity": {},
      "nodeSelector": null,
      "podAnnotations": {}
    },
    "zoneB": {
      "annotations": {},
      "extraAffinity": {},
      "nodeSelector": null,
      "podAnnotations": {}
    },
    "zoneC": {
      "annotations": {},
      "extraAffinity": {},
      "nodeSelector": null,
      "podAnnotations": {}
    }
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.affinity</td>
			<td>object</td>
			<td>Affinity for ingester pods. Ignored if zoneAwareReplication is enabled.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.appProtocol</td>
			<td>object</td>
			<td>Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.appProtocol.grpc</td>
			<td>string</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.behavior.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling behaviours</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.behavior.scaleDown</td>
			<td>object</td>
			<td>define scale down policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.behavior.scaleUp</td>
			<td>object</td>
			<td>define scale up policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.customMetrics</td>
			<td>list</td>
			<td>Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the ingester</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the ingester</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the ingester</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the ingester</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the ingester</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the ingester</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the ingester image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the ingester image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the ingester image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.lifecycle</td>
			<td>object</td>
			<td>Lifecycle for the ingester container</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.livenessProbe</td>
			<td>object</td>
			<td>liveness probe settings for ingester pods. If empty use `loki.livenessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.maxUnavailable</td>
			<td>int</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.nodeSelector</td>
			<td>object</td>
			<td>Node selector for ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.persistence.claims</td>
			<td>list</td>
			<td>List of the ingester PVCs</td>
			<td><pre lang="list">

</pre>
</td>
		</tr>
		<tr>
			<td>ingester.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs which is required when using boltdb-shipper</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.persistence.inMemory</td>
			<td>bool</td>
			<td>Use emptyDir with ramdisk for storage. **Please note that all data in ingester will be lost on pod restart**</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.podAnnotations</td>
			<td>object</td>
			<td>Annotations for ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.podLabels</td>
			<td>object</td>
			<td>Labels for ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.readinessProbe</td>
			<td>object</td>
			<td>readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.replicas</td>
			<td>int</td>
			<td>Number of replicas for the ingester, when zoneAwareReplication.enabled is true, the total number of replicas will match this value with each zone having 1/3rd of the total replicas.</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the ingester</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.serviceLabels</td>
			<td>object</td>
			<td>Labels for ingestor service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor, this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring all data and to successfully leave the member ring on shutdown.</td>
			<td><pre lang="json">
300
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.tolerations</td>
			<td>list</td>
			<td>Tolerations for ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.topologySpreadConstraints</td>
			<td>list</td>
			<td>topologySpread for ingester pods.</td>
			<td><pre lang="">
Defaults to allow skew no more than 1 node
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication</td>
			<td>object</td>
			<td>Enabling zone awareness on ingesters will create 3 statefulests where all writes will send a replica to each zone. This is primarily intended to accelerate rollout operations by allowing for multiple ingesters within a single zone to be shutdown and restart simultaneously (the remaining 2 zones will be guaranteed to have at least one copy of the data). Note: This can be used to run Loki over multiple cloud provider availability zones however this is not currently recommended as Loki is not optimized for this and cross zone network traffic costs can become extremely high extremely quickly. Even with zone awareness enabled, it is recommended to run Loki in a single availability zone.</td>
			<td><pre lang="json">
{
  "enabled": true,
  "maxUnavailablePct": 33,
  "migration": {
    "enabled": false,
    "excludeDefaultZone": false,
    "readPath": false,
    "writePath": false
  },
  "zoneA": {
    "annotations": {},
    "extraAffinity": {},
    "nodeSelector": null,
    "podAnnotations": {}
  },
  "zoneB": {
    "annotations": {},
    "extraAffinity": {},
    "nodeSelector": null,
    "podAnnotations": {}
  },
  "zoneC": {
    "annotations": {},
    "extraAffinity": {},
    "nodeSelector": null,
    "podAnnotations": {}
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.enabled</td>
			<td>bool</td>
			<td>Enable zone awareness.</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.maxUnavailablePct</td>
			<td>int</td>
			<td>The percent of replicas in each zone that will be restarted at once. In a value of 0-100</td>
			<td><pre lang="json">
33
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.migration</td>
			<td>object</td>
			<td>The migration block allows migrating non zone aware ingesters to zone aware ingesters.</td>
			<td><pre lang="json">
{
  "enabled": false,
  "excludeDefaultZone": false,
  "readPath": false,
  "writePath": false
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneA</td>
			<td>object</td>
			<td>zoneA configuration</td>
			<td><pre lang="json">
{
  "annotations": {},
  "extraAffinity": {},
  "nodeSelector": null,
  "podAnnotations": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneA.annotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone A statefulset</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneA.extraAffinity</td>
			<td>object</td>
			<td>optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneA.nodeSelector</td>
			<td>string</td>
			<td>optionally define a node selector for this zone</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneA.podAnnotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone A pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneB.annotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone B statefulset</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneB.extraAffinity</td>
			<td>object</td>
			<td>optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneB.nodeSelector</td>
			<td>string</td>
			<td>optionally define a node selector for this zone</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneB.podAnnotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone B pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneC.annotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone C statefulset</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneC.extraAffinity</td>
			<td>object</td>
			<td>optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneC.nodeSelector</td>
			<td>string</td>
			<td>optionally define a node selector for this zone</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ingester.zoneAwareReplication.zoneC.podAnnotations</td>
			<td>object</td>
			<td>Specific annotations to add to zone C pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ingress</td>
			<td>object</td>
			<td>Ingress configuration Use either this ingress or the gateway, but not both at once. If you enable this, make sure to disable the gateway. You'll need to supply authn configuration for your ingress controller.</td>
			<td><pre lang="json">
{
  "annotations": {},
  "enabled": false,
  "hosts": [
    "loki.example.com"
  ],
  "ingressClassName": "",
  "labels": {},
  "paths": {
    "read": [
      "/api/prom/tail",
      "/loki/api/v1/tail",
      "/loki/api",
      "/api/prom/rules",
      "/loki/api/v1/rules",
      "/prometheus/api/v1/rules",
      "/prometheus/api/v1/alerts"
    ],
    "singleBinary": [
      "/api/prom/push",
      "/loki/api/v1/push",
      "/api/prom/tail",
      "/loki/api/v1/tail",
      "/loki/api",
      "/api/prom/rules",
      "/loki/api/v1/rules",
      "/prometheus/api/v1/rules",
      "/prometheus/api/v1/alerts"
    ],
    "write": [
      "/api/prom/push",
      "/loki/api/v1/push"
    ]
  },
  "tls": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>ingress.hosts</td>
			<td>list</td>
			<td>Hosts configuration for the ingress, passed through the `tpl` function to allow templating</td>
			<td><pre lang="json">
[
  "loki.example.com"
]
</pre>
</td>
		</tr>
		<tr>
			<td>ingress.tls</td>
			<td>list</td>
			<td>TLS configuration for the ingress. Hosts passed through the `tpl` function to allow templating</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage</td>
			<td>object</td>
			<td>kubetclImage is used in the enterprise provisioner and tokengen jobs</td>
			<td><pre lang="json">
{
  "digest": null,
  "pullPolicy": "IfNotPresent",
  "registry": "docker.io",
  "repository": "bitnami/kubectl",
  "tag": null
}
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"bitnami/kubectl"
</pre>
</td>
		</tr>
		<tr>
			<td>kubectlImage.tag</td>
			<td>string</td>
			<td>Overrides the image tag whose default is the chart's appVersion</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>loki</td>
			<td>object</td>
			<td>Configuration for running Loki</td>
			<td><pre lang="json">
{
  "analytics": {},
  "annotations": {},
  "auth_enabled": true,
  "commonConfig": {
    "compactor_address": "{{ include \"loki.compactorAddress\" . }}",
    "path_prefix": "/var/loki",
    "replication_factor": 3
  },
  "compactor": {},
  "config": "{{- if .Values.enterprise.enabled}}\n{{- tpl .Values.enterprise.config . }}\n{{- else }}\nauth_enabled: {{ .Values.loki.auth_enabled }}\n{{- end }}\n\n{{- with .Values.loki.server }}\nserver:\n  {{- toYaml . | nindent 2}}\n{{- end}}\n\npattern_ingester:\n  enabled: {{ .Values.loki.pattern_ingester.enabled }}\n\nmemberlist:\n{{- if .Values.loki.memberlistConfig }}\n  {{- toYaml .Values.loki.memberlistConfig | nindent 2 }}\n{{- else }}\n{{- if .Values.loki.extraMemberlistConfig}}\n{{- toYaml .Values.loki.extraMemberlistConfig | nindent 2}}\n{{- end }}\n  join_members:\n    - {{ include \"loki.memberlist\" . }}\n    {{- with .Values.migrate.fromDistributed }}\n    {{- if .enabled }}\n    - {{ .memberlistService }}\n    {{- end }}\n    {{- end }}\n{{- end }}\n\n{{- with .Values.loki.ingester }}\ningester:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- if .Values.loki.commonConfig}}\ncommon:\n{{- toYaml .Values.loki.commonConfig | nindent 2}}\n  storage:\n  {{- include \"loki.commonStorageConfig\" . | nindent 4}}\n{{- end}}\n\n{{- with .Values.loki.limits_config }}\nlimits_config:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\nruntime_config:\n  file: /etc/loki/runtime-config/runtime-config.yaml\n\n{{- with .Values.chunksCache }}\n{{- if .enabled }}\nchunk_store_config:\n  chunk_cache_config:\n    default_validity: {{ .defaultValidity }}\n    background:\n      writeback_goroutines: {{ .writebackParallelism }}\n      writeback_buffer: {{ .writebackBuffer }}\n      writeback_size_limit: {{ .writebackSizeLimit }}\n    memcached:\n      batch_size: {{ .batchSize }}\n      parallelism: {{ .parallelism }}\n    memcached_client:\n      addresses: dnssrvnoa+_memcached-client._tcp.{{ template \"loki.fullname\" $ }}-chunks-cache.{{ $.Release.Namespace }}.svc\n      consistent_hash: true\n      timeout: {{ .timeout }}\n      max_idle_conns: 72\n{{- end }}\n{{- end }}\n\n{{- if .Values.loki.schemaConfig }}\nschema_config:\n{{- toYaml .Values.loki.schemaConfig | nindent 2}}\n{{- end }}\n\n{{- if .Values.loki.useTestSchema }}\nschema_config:\n{{- toYaml .Values.loki.testSchemaConfig | nindent 2}}\n{{- end }}\n\n{{ include \"loki.rulerConfig\" . }}\n\n{{- if or .Values.tableManager.retention_deletes_enabled .Values.tableManager.retention_period }}\ntable_manager:\n  retention_deletes_enabled: {{ .Values.tableManager.retention_deletes_enabled }}\n  retention_period: {{ .Values.tableManager.retention_period }}\n{{- end }}\n\nquery_range:\n  align_queries_with_step: true\n  {{- with .Values.loki.query_range }}\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n  {{- end }}\n  {{- if .Values.resultsCache.enabled }}\n  {{- with .Values.resultsCache }}\n  cache_results: true\n  results_cache:\n    cache:\n      default_validity: {{ .defaultValidity }}\n      background:\n        writeback_goroutines: {{ .writebackParallelism }}\n        writeback_buffer: {{ .writebackBuffer }}\n        writeback_size_limit: {{ .writebackSizeLimit }}\n      memcached_client:\n        consistent_hash: true\n        addresses: dnssrvnoa+_memcached-client._tcp.{{ template \"loki.fullname\" $ }}-results-cache.{{ $.Release.Namespace }}.svc\n        timeout: {{ .timeout }}\n        update_interval: 1m\n  {{- end }}\n  {{- end }}\n\n{{- with .Values.loki.storage_config }}\nstorage_config:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.query_scheduler }}\nquery_scheduler:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.compactor }}\ncompactor:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.analytics }}\nanalytics:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.querier }}\nquerier:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.index_gateway }}\nindex_gateway:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.frontend }}\nfrontend:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.frontend_worker }}\nfrontend_worker:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\n{{- with .Values.loki.distributor }}\ndistributor:\n  {{- tpl (. | toYaml) $ | nindent 4 }}\n{{- end }}\n\ntracing:\n  enabled: {{ .Values.loki.tracing.enabled }}\n",
  "configObjectName": "{{ include \"loki.name\" . }}",
  "configStorageType": "ConfigMap",
  "containerSecurityContext": {
    "allowPrivilegeEscalation": false,
    "capabilities": {
      "drop": [
        "ALL"
      ]
    },
    "readOnlyRootFilesystem": true
  },
  "distributor": {},
  "enableServiceLinks": true,
  "extraMemberlistConfig": {},
  "frontend": {
    "scheduler_address": "{{ include \"loki.querySchedulerAddress\" . }}",
    "tail_proxy_url": "{{ include \"loki.querierAddress\" . }}"
  },
  "frontend_worker": {
    "scheduler_address": "{{ include \"loki.querySchedulerAddress\" . }}"
  },
  "generatedConfigObjectName": "{{ include \"loki.name\" . }}",
  "image": {
    "digest": null,
    "pullPolicy": "IfNotPresent",
    "registry": "docker.io",
    "repository": "grafana/loki",
    "tag": null
  },
  "index_gateway": {
    "mode": "simple"
  },
  "ingester": {},
  "limits_config": {
    "max_cache_freshness_per_query": "10m",
    "query_timeout": "300s",
    "reject_old_samples": true,
    "reject_old_samples_max_age": "168h",
    "split_queries_by_interval": "15m",
    "volume_enabled": true
  },
  "memberlistConfig": {},
  "memcached": {
    "chunk_cache": {
      "batch_size": 256,
      "enabled": false,
      "host": "",
      "parallelism": 10,
      "service": "memcached-client"
    },
    "results_cache": {
      "default_validity": "12h",
      "enabled": false,
      "host": "",
      "service": "memcached-client",
      "timeout": "500ms"
    }
  },
  "pattern_ingester": {
    "enabled": false
  },
  "podAnnotations": {},
  "podLabels": {},
  "podSecurityContext": {
    "fsGroup": 10001,
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001
  },
  "querier": {},
  "query_range": {},
  "query_scheduler": {},
  "readinessProbe": {
    "httpGet": {
      "path": "/ready",
      "port": "http-metrics"
    },
    "initialDelaySeconds": 30,
    "timeoutSeconds": 1
  },
  "revisionHistoryLimit": 10,
  "rulerConfig": {},
  "runtimeConfig": {},
  "schemaConfig": {},
  "server": {
    "grpc_listen_port": 9095,
    "http_listen_port": 3100,
    "http_server_read_timeout": "600s",
    "http_server_write_timeout": "600s"
  },
  "serviceAnnotations": {},
  "serviceLabels": {},
  "storage": {
    "azure": {
      "accountKey": null,
      "accountName": null,
      "connectionString": null,
      "endpointSuffix": null,
      "requestTimeout": null,
      "useFederatedToken": false,
      "useManagedIdentity": false,
      "userAssignedId": null
    },
    "filesystem": {
      "chunks_directory": "/var/loki/chunks",
      "rules_directory": "/var/loki/rules"
    },
    "gcs": {
      "chunkBufferSize": 0,
      "enableHttp2": true,
      "requestTimeout": "0s"
    },
    "s3": {
      "accessKeyId": null,
      "backoff_config": {},
      "endpoint": null,
      "http_config": {},
      "insecure": false,
      "region": null,
      "s3": null,
      "s3ForcePathStyle": false,
      "secretAccessKey": null,
      "signatureVersion": null
    },
    "swift": {
      "auth_url": null,
      "auth_version": null,
      "connect_timeout": null,
      "container_name": null,
      "domain_id": null,
      "domain_name": null,
      "internal": null,
      "max_retries": null,
      "password": null,
      "project_domain_id": null,
      "project_domain_name": null,
      "project_id": null,
      "project_name": null,
      "region_name": null,
      "request_timeout": null,
      "user_domain_id": null,
      "user_domain_name": null,
      "user_id": null,
      "username": null
    },
    "type": "s3"
  },
  "storage_config": {
    "boltdb_shipper": {
      "index_gateway_client": {
        "server_address": "{{ include \"loki.indexGatewayAddress\" . }}"
      }
    },
    "hedging": {
      "at": "250ms",
      "max_per_second": 20,
      "up_to": 3
    },
    "tsdb_shipper": {
      "index_gateway_client": {
        "server_address": "{{ include \"loki.indexGatewayAddress\" . }}"
      }
    }
  },
  "structuredConfig": {},
  "tenants": [],
  "testSchemaConfig": {
    "configs": [
      {
        "from": "2024-04-01",
        "index": {
          "period": "24h",
          "prefix": "index_"
        },
        "object_store": "{{ include \"loki.testSchemaObjectStore\" . }}",
        "schema": "v13",
        "store": "tsdb"
      }
    ]
  },
  "tracing": {
    "enabled": false
  },
  "useTestSchema": false
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.analytics</td>
			<td>object</td>
			<td>Optional analytics configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.annotations</td>
			<td>object</td>
			<td>Common annotations for all deployments/StatefulSets</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.commonConfig</td>
			<td>object</td>
			<td>Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration</td>
			<td><pre lang="json">
{
  "compactor_address": "{{ include \"loki.compactorAddress\" . }}",
  "path_prefix": "/var/loki",
  "replication_factor": 3
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.compactor</td>
			<td>object</td>
			<td>Optional compactor configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.config</td>
			<td>string</td>
			<td>Config file contents for Loki</td>
			<td><pre lang="">
See values.yaml
</pre>
</td>
		</tr>
		<tr>
			<td>loki.configObjectName</td>
			<td>string</td>
			<td>The name of the object which Loki will mount as a volume containing the config. If the configStorageType is Secret, this will be the name of the Secret, if it is ConfigMap, this will be the name of the ConfigMap. The value will be passed through tpl.</td>
			<td><pre lang="json">
"{{ include \"loki.name\" . }}"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.configStorageType</td>
			<td>string</td>
			<td>Defines what kind of object stores the configuration, a ConfigMap or a Secret. In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/loki/latest/configuration/#use-environment-variables-in-the-configuration). Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).</td>
			<td><pre lang="json">
"ConfigMap"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.containerSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for Loki containers</td>
			<td><pre lang="json">
{
  "allowPrivilegeEscalation": false,
  "capabilities": {
    "drop": [
      "ALL"
    ]
  },
  "readOnlyRootFilesystem": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.distributor</td>
			<td>object</td>
			<td>Optional distributor configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.enableServiceLinks</td>
			<td>bool</td>
			<td>Should enableServiceLinks be enabled. Default to enable</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>loki.extraMemberlistConfig</td>
			<td>object</td>
			<td>Extra memberlist configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.generatedConfigObjectName</td>
			<td>string</td>
			<td>The name of the Secret or ConfigMap that will be created by this chart. If empty, no configmap or secret will be created. The value will be passed through tpl.</td>
			<td><pre lang="json">
"{{ include \"loki.name\" . }}"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.image.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>loki.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.image.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.image.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"grafana/loki"
</pre>
</td>
		</tr>
		<tr>
			<td>loki.image.tag</td>
			<td>string</td>
			<td>Overrides the image tag whose default is the chart's appVersion TODO: needed for 3rd target backend functionality revert to null or latest once this behavior is relased</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>loki.index_gateway</td>
			<td>object</td>
			<td>Optional index gateway configuration</td>
			<td><pre lang="json">
{
  "mode": "simple"
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.ingester</td>
			<td>object</td>
			<td>Optional ingester configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.limits_config</td>
			<td>object</td>
			<td>Limits config</td>
			<td><pre lang="json">
{
  "max_cache_freshness_per_query": "10m",
  "query_timeout": "300s",
  "reject_old_samples": true,
  "reject_old_samples_max_age": "168h",
  "split_queries_by_interval": "15m",
  "volume_enabled": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.memberlistConfig</td>
			<td>object</td>
			<td>memberlist configuration (overrides embedded default)</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.memcached</td>
			<td>object</td>
			<td>Configure memcached as an external cache for chunk and results cache. Disabled by default must enable and specify a host for each cache you would like to use.</td>
			<td><pre lang="json">
{
  "chunk_cache": {
    "batch_size": 256,
    "enabled": false,
    "host": "",
    "parallelism": 10,
    "service": "memcached-client"
  },
  "results_cache": {
    "default_validity": "12h",
    "enabled": false,
    "host": "",
    "service": "memcached-client",
    "timeout": "500ms"
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.pattern_ingester</td>
			<td>object</td>
			<td>Optional pattern ingester configuration</td>
			<td><pre lang="json">
{
  "enabled": false
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.podAnnotations</td>
			<td>object</td>
			<td>Common annotations for all pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.podLabels</td>
			<td>object</td>
			<td>Common labels for all pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.podSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for Loki pods</td>
			<td><pre lang="json">
{
  "fsGroup": 10001,
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.querier</td>
			<td>object</td>
			<td>Optional querier configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.query_range</td>
			<td>object</td>
			<td>Optional querier configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.query_scheduler</td>
			<td>object</td>
			<td>Additional query scheduler config</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.revisionHistoryLimit</td>
			<td>int</td>
			<td>The number of old ReplicaSets to retain to allow rollback</td>
			<td><pre lang="json">
10
</pre>
</td>
		</tr>
		<tr>
			<td>loki.rulerConfig</td>
			<td>object</td>
			<td>Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.runtimeConfig</td>
			<td>object</td>
			<td>Provides a reloadable runtime configuration file for some specific configuration</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.schemaConfig</td>
			<td>object</td>
			<td>Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.server</td>
			<td>object</td>
			<td>Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.</td>
			<td><pre lang="json">
{
  "grpc_listen_port": 9095,
  "http_listen_port": 3100,
  "http_server_read_timeout": "600s",
  "http_server_write_timeout": "600s"
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.serviceAnnotations</td>
			<td>object</td>
			<td>Common annotations for all services</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.serviceLabels</td>
			<td>object</td>
			<td>Common labels for all services</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.storage</td>
			<td>object</td>
			<td>Storage config. Providing this will automatically populate all necessary storage configs in the templated config.</td>
			<td><pre lang="json">
{
  "azure": {
    "accountKey": null,
    "accountName": null,
    "connectionString": null,
    "endpointSuffix": null,
    "requestTimeout": null,
    "useFederatedToken": false,
    "useManagedIdentity": false,
    "userAssignedId": null
  },
  "filesystem": {
    "chunks_directory": "/var/loki/chunks",
    "rules_directory": "/var/loki/rules"
  },
  "gcs": {
    "chunkBufferSize": 0,
    "enableHttp2": true,
    "requestTimeout": "0s"
  },
  "s3": {
    "accessKeyId": null,
    "backoff_config": {},
    "endpoint": null,
    "http_config": {},
    "insecure": false,
    "region": null,
    "s3": null,
    "s3ForcePathStyle": false,
    "secretAccessKey": null,
    "signatureVersion": null
  },
  "swift": {
    "auth_url": null,
    "auth_version": null,
    "connect_timeout": null,
    "container_name": null,
    "domain_id": null,
    "domain_name": null,
    "internal": null,
    "max_retries": null,
    "password": null,
    "project_domain_id": null,
    "project_domain_name": null,
    "project_id": null,
    "project_name": null,
    "region_name": null,
    "request_timeout": null,
    "user_domain_id": null,
    "user_domain_name": null,
    "user_id": null,
    "username": null
  },
  "type": "s3"
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.storage.s3.backoff_config</td>
			<td>object</td>
			<td>Check https://grafana.com/docs/loki/latest/configure/#s3_storage_config for more info on how to provide a backoff_config</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.storage_config</td>
			<td>object</td>
			<td>Additional storage config</td>
			<td><pre lang="json">
{
  "boltdb_shipper": {
    "index_gateway_client": {
      "server_address": "{{ include \"loki.indexGatewayAddress\" . }}"
    }
  },
  "hedging": {
    "at": "250ms",
    "max_per_second": 20,
    "up_to": 3
  },
  "tsdb_shipper": {
    "index_gateway_client": {
      "server_address": "{{ include \"loki.indexGatewayAddress\" . }}"
    }
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.structuredConfig</td>
			<td>object</td>
			<td>Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.tenants</td>
			<td>list</td>
			<td>Tenants list to be created on nginx htpasswd file, with name and password keys</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>loki.tracing</td>
			<td>object</td>
			<td>Enable tracing</td>
			<td><pre lang="json">
{
  "enabled": false
}
</pre>
</td>
		</tr>
		<tr>
			<td>loki.useTestSchema</td>
			<td>bool</td>
			<td>a real Loki install requires a proper schemaConfig defined above this, however for testing or playing around you can enable useTestSchema</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.annotations</td>
			<td>object</td>
			<td>Additional annotations for the `loki-canary` Daemonset</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.dnsConfig</td>
			<td>object</td>
			<td>DNS config for canary pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.enabled</td>
			<td>bool</td>
			<td></td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.extraArgs</td>
			<td>list</td>
			<td>Additional CLI arguments for the `loki-canary' command</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the canary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the canary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the canary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the canary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image</td>
			<td>object</td>
			<td>Image to use for loki canary</td>
			<td><pre lang="json">
{
  "digest": null,
  "pullPolicy": "IfNotPresent",
  "registry": "docker.io",
  "repository": "grafana/loki-canary",
  "tag": null
}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"grafana/loki-canary"
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.image.tag</td>
			<td>string</td>
			<td>Overrides the image tag whose default is the chart's appVersion</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.labelname</td>
			<td>string</td>
			<td>The name of the label to look for at loki when doing the checks.</td>
			<td><pre lang="json">
"pod"
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.nodeSelector</td>
			<td>object</td>
			<td>Node selector for canary pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.podLabels</td>
			<td>object</td>
			<td>Additional labels for each `loki-canary` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for loki-canary pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.push</td>
			<td>bool</td>
			<td></td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the canary</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.service.annotations</td>
			<td>object</td>
			<td>Annotations for loki-canary Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.service.labels</td>
			<td>object</td>
			<td>Additional labels for loki-canary Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.tolerations</td>
			<td>list</td>
			<td>Tolerations for canary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>lokiCanary.updateStrategy</td>
			<td>object</td>
			<td>Update strategy for the `loki-canary` Daemonset pods</td>
			<td><pre lang="json">
{
  "rollingUpdate": {
    "maxUnavailable": 1
  },
  "type": "RollingUpdate"
}
</pre>
</td>
		</tr>
		<tr>
			<td>memberlist.service.publishNotReadyAddresses</td>
			<td>bool</td>
			<td></td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.containerSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for memcached containers</td>
			<td><pre lang="json">
{
  "allowPrivilegeEscalation": false,
  "capabilities": {
    "drop": [
      "ALL"
    ]
  },
  "readOnlyRootFilesystem": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.image.pullPolicy</td>
			<td>string</td>
			<td>Memcached Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.image.repository</td>
			<td>string</td>
			<td>Memcached Docker image repository</td>
			<td><pre lang="json">
"memcached"
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.image.tag</td>
			<td>string</td>
			<td>Memcached Docker image tag</td>
			<td><pre lang="json">
"1.6.23-alpine"
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.podSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext override for memcached pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>memcached.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for memcached pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.containerSecurityContext</td>
			<td>object</td>
			<td>The SecurityContext for memcached exporter containers</td>
			<td><pre lang="json">
{
  "allowPrivilegeEscalation": false,
  "capabilities": {
    "drop": [
      "ALL"
    ]
  },
  "readOnlyRootFilesystem": true
}
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.enabled</td>
			<td>bool</td>
			<td>Whether memcached metrics should be exported</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.extraArgs</td>
			<td>object</td>
			<td>Extra args to add to the exporter container. Example: extraArgs:   memcached.tls.enable: true   memcached.tls.cert-file: /certs/cert.crt   memcached.tls.key-file: /certs/cert.key   memcached.tls.ca-file: /certs/ca.crt   memcached.tls.insecure-skip-verify: false   memcached.tls.server-name: memcached</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.image.pullPolicy</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.image.repository</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"prom/memcached-exporter"
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.image.tag</td>
			<td>string</td>
			<td></td>
			<td><pre lang="json">
"v0.14.2"
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.resources.limits</td>
			<td>object</td>
			<td></td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>memcachedExporter.resources.requests</td>
			<td>object</td>
			<td></td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>migrate</td>
			<td>object</td>
			<td>Options that may be necessary when performing a migration from another helm chart</td>
			<td><pre lang="json">
{
  "fromDistributed": {
    "enabled": false,
    "memberlistService": ""
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>migrate.fromDistributed</td>
			<td>object</td>
			<td>When migrating from a distributed chart like loki-distributed or enterprise-logs</td>
			<td><pre lang="json">
{
  "enabled": false,
  "memberlistService": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>migrate.fromDistributed.enabled</td>
			<td>bool</td>
			<td>Set to true if migrating from a distributed helm chart</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>migrate.fromDistributed.memberlistService</td>
			<td>string</td>
			<td>If migrating from a distributed service, provide the distributed deployment's memberlist service DNS so the new deployment can join its ring.</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>minio</td>
			<td>object</td>
			<td>Configuration for the minio subchart</td>
			<td><pre lang="json">
{
  "address": null,
  "buckets": [
    {
      "name": "chunks",
      "policy": "none",
      "purge": false
    },
    {
      "name": "ruler",
      "policy": "none",
      "purge": false
    },
    {
      "name": "admin",
      "policy": "none",
      "purge": false
    }
  ],
  "drivesPerNode": 2,
  "enabled": false,
  "persistence": {
    "size": "5Gi"
  },
  "replicas": 1,
  "resources": {
    "requests": {
      "cpu": "100m",
      "memory": "128Mi"
    }
  },
  "rootPassword": "supersecret",
  "rootUser": "enterprise-logs"
}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring</td>
			<td>object</td>
			<td>DEPRECATED Monitoring section determines which monitoring features to enable, this section is being replaced by https://github.com/grafana/meta-monitoring-chart</td>
			<td><pre lang="json">
{
  "dashboards": {
    "annotations": {},
    "enabled": false,
    "labels": {
      "grafana_dashboard": "1"
    },
    "namespace": null
  },
  "rules": {
    "additionalGroups": [],
    "additionalRuleLabels": {},
    "alerting": true,
    "annotations": {},
    "disabled": {},
    "enabled": false,
    "labels": {},
    "namespace": null
  },
  "selfMonitoring": {
    "enabled": false,
    "grafanaAgent": {
      "annotations": {},
      "enableConfigReadAPI": false,
      "installOperator": false,
      "labels": {},
      "priorityClassName": null,
      "resources": {},
      "tolerations": []
    },
    "logsInstance": {
      "annotations": {},
      "clients": null,
      "labels": {}
    },
    "podLogs": {
      "additionalPipelineStages": [],
      "annotations": {},
      "apiVersion": "monitoring.grafana.com/v1alpha1",
      "labels": {},
      "relabelings": []
    },
    "tenant": {
      "name": "self-monitoring",
      "password": null,
      "secretNamespace": "{{ .Release.Namespace }}"
    }
  },
  "serviceMonitor": {
    "annotations": {},
    "enabled": false,
    "interval": "15s",
    "labels": {},
    "metricRelabelings": [],
    "metricsInstance": {
      "annotations": {},
      "enabled": true,
      "labels": {},
      "remoteWrite": null
    },
    "namespaceSelector": {},
    "relabelings": [],
    "scheme": "http",
    "scrapeTimeout": null,
    "tlsConfig": null
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.dashboards.annotations</td>
			<td>object</td>
			<td>Additional annotations for the dashboards ConfigMap</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.dashboards.enabled</td>
			<td>bool</td>
			<td>If enabled, create configmap with dashboards for monitoring Loki</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.dashboards.labels</td>
			<td>object</td>
			<td>Labels for the dashboards ConfigMap</td>
			<td><pre lang="json">
{
  "grafana_dashboard": "1"
}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.dashboards.namespace</td>
			<td>string</td>
			<td>Alternative namespace to create dashboards ConfigMap in</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.additionalGroups</td>
			<td>list</td>
			<td>Additional groups to add to the rules file</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.additionalRuleLabels</td>
			<td>object</td>
			<td>Additional labels for PrometheusRule alerts</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.alerting</td>
			<td>bool</td>
			<td>Include alerting rules</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.annotations</td>
			<td>object</td>
			<td>Additional annotations for the rules PrometheusRule resource</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.disabled</td>
			<td>object</td>
			<td>If you disable all the alerts and keep .monitoring.rules.alerting set to true, the chart will fail to render.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.enabled</td>
			<td>bool</td>
			<td>If enabled, create PrometheusRule resource with Loki recording rules</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.labels</td>
			<td>object</td>
			<td>Additional labels for the rules PrometheusRule resource</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.rules.namespace</td>
			<td>string</td>
			<td>Alternative namespace to create PrometheusRule resources in</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.annotations</td>
			<td>object</td>
			<td>Grafana Agent annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.enableConfigReadAPI</td>
			<td>bool</td>
			<td>Enable the config read api on port 8080 of the agent</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.installOperator</td>
			<td>bool</td>
			<td>Controls whether to install the Grafana Agent Operator and its CRDs. Note that helm will not install CRDs if this flag is enabled during an upgrade. In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.labels</td>
			<td>object</td>
			<td>Additional Grafana Agent labels</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for GrafanaAgent pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the grafanaAgent pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.grafanaAgent.tolerations</td>
			<td>list</td>
			<td>Tolerations for GrafanaAgent pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.logsInstance.annotations</td>
			<td>object</td>
			<td>LogsInstance annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.logsInstance.clients</td>
			<td>string</td>
			<td>Additional clients for remote write</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.logsInstance.labels</td>
			<td>object</td>
			<td>Additional LogsInstance labels</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.podLogs.additionalPipelineStages</td>
			<td>list</td>
			<td>Additional pipeline stages to process logs after scraping https://grafana.com/docs/agent/latest/operator/api/#pipelinestagespec-a-namemonitoringgrafanacomv1alpha1pipelinestagespeca</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.podLogs.annotations</td>
			<td>object</td>
			<td>PodLogs annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.podLogs.apiVersion</td>
			<td>string</td>
			<td>PodLogs version</td>
			<td><pre lang="json">
"monitoring.grafana.com/v1alpha1"
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.podLogs.labels</td>
			<td>object</td>
			<td>Additional PodLogs labels</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.podLogs.relabelings</td>
			<td>list</td>
			<td>PodLogs relabel configs to apply to samples before scraping https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.tenant</td>
			<td>object</td>
			<td>Tenant to use for self monitoring</td>
			<td><pre lang="json">
{
  "name": "self-monitoring",
  "password": null,
  "secretNamespace": "{{ .Release.Namespace }}"
}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.tenant.name</td>
			<td>string</td>
			<td>Name of the tenant</td>
			<td><pre lang="json">
"self-monitoring"
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.tenant.password</td>
			<td>string</td>
			<td>Password of the gateway for Basic auth</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.selfMonitoring.tenant.secretNamespace</td>
			<td>string</td>
			<td>Namespace to create additional tenant token secret in. Useful if your Grafana instance is in a separate namespace. Token will still be created in the canary namespace.</td>
			<td><pre lang="json">
"{{ .Release.Namespace }}"
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.annotations</td>
			<td>object</td>
			<td>ServiceMonitor annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.enabled</td>
			<td>bool</td>
			<td>If enabled, ServiceMonitor resources for Prometheus Operator are created</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.interval</td>
			<td>string</td>
			<td>ServiceMonitor scrape interval Default is 15s because included recording rules use a 1m rate, and scrape interval needs to be at least 1/4 rate interval.</td>
			<td><pre lang="json">
"15s"
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.labels</td>
			<td>object</td>
			<td>Additional ServiceMonitor labels</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricRelabelings</td>
			<td>list</td>
			<td>ServiceMonitor metric relabel configs to apply to samples before ingestion https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricsInstance</td>
			<td>object</td>
			<td>If defined, will create a MetricsInstance for the Grafana Agent Operator.</td>
			<td><pre lang="json">
{
  "annotations": {},
  "enabled": true,
  "labels": {},
  "remoteWrite": null
}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricsInstance.annotations</td>
			<td>object</td>
			<td>MetricsInstance annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricsInstance.enabled</td>
			<td>bool</td>
			<td>If enabled, MetricsInstance resources for Grafana Agent Operator are created</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricsInstance.labels</td>
			<td>object</td>
			<td>Additional MetricsInstance labels</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.metricsInstance.remoteWrite</td>
			<td>string</td>
			<td>If defined a MetricsInstance will be created to remote write metrics.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.namespaceSelector</td>
			<td>object</td>
			<td>Namespace selector for ServiceMonitor resources</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.relabelings</td>
			<td>list</td>
			<td>ServiceMonitor relabel configs to apply to samples before scraping https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.scheme</td>
			<td>string</td>
			<td>ServiceMonitor will use http by default, but you can pick https as well</td>
			<td><pre lang="json">
"http"
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.scrapeTimeout</td>
			<td>string</td>
			<td>ServiceMonitor scrape timeout in Go duration format (e.g. 15s)</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>monitoring.serviceMonitor.tlsConfig</td>
			<td>string</td>
			<td>ServiceMonitor will use these tlsConfig settings to make the health check requests</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>nameOverride</td>
			<td>string</td>
			<td>Overrides the chart's name</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.alertmanager.namespaceSelector</td>
			<td>object</td>
			<td>Specifies the namespace the alertmanager is running in</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.alertmanager.podSelector</td>
			<td>object</td>
			<td>Specifies the alertmanager Pods. As this is cross-namespace communication, you also need the namespaceSelector.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.alertmanager.port</td>
			<td>int</td>
			<td>Specify the alertmanager port used for alerting</td>
			<td><pre lang="json">
9093
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.discovery.namespaceSelector</td>
			<td>object</td>
			<td>Specifies the namespace the discovery Pods are running in</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.discovery.podSelector</td>
			<td>object</td>
			<td>Specifies the Pods labels used for discovery. As this is cross-namespace communication, you also need the namespaceSelector.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.discovery.port</td>
			<td>int</td>
			<td>Specify the port used for discovery</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.egressKubeApiserver.enabled</td>
			<td>bool</td>
			<td>Enable additional cilium egress rules to kube-apiserver for backend.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.egressWorld.enabled</td>
			<td>bool</td>
			<td>Enable additional cilium egress rules to external world for write, read and backend.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.enabled</td>
			<td>bool</td>
			<td>Specifies whether Network Policies should be created</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.externalStorage.cidrs</td>
			<td>list</td>
			<td>Specifies specific network CIDRs you want to limit access to</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.externalStorage.ports</td>
			<td>list</td>
			<td>Specify the port used for external storage, e.g. AWS S3</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.flavor</td>
			<td>string</td>
			<td>Specifies whether the policies created will be standard Network Policies (flavor: kubernetes) or Cilium Network Policies (flavor: cilium)</td>
			<td><pre lang="json">
"kubernetes"
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.ingress.namespaceSelector</td>
			<td>object</td>
			<td>Specifies the namespaces which are allowed to access the http port</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.ingress.podSelector</td>
			<td>object</td>
			<td>Specifies the Pods which are allowed to access the http port. As this is cross-namespace communication, you also need the namespaceSelector.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.metrics.cidrs</td>
			<td>list</td>
			<td>Specifies specific network CIDRs which are allowed to access the metrics port. In case you use namespaceSelector, you also have to specify your kubelet networks here. The metrics ports are also used for probes.</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.metrics.namespaceSelector</td>
			<td>object</td>
			<td>Specifies the namespaces which are allowed to access the metrics port</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>networkPolicy.metrics.podSelector</td>
			<td>object</td>
			<td>Specifies the Pods which are allowed to access the metrics port. As this is cross-namespace communication, you also need the namespaceSelector.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester</td>
			<td>object</td>
			<td>Configuration for the pattern ingester</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "pattern-ingester"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "livenessProbe": {},
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "claims": [
      {
        "name": "data",
        "size": "10Gi",
        "storageClass": null
      }
    ],
    "enableStatefulSetAutoDeletePVC": false,
    "enabled": false,
    "size": "10Gi",
    "storageClass": null,
    "whenDeleted": "Retain",
    "whenScaled": "Retain"
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "readinessProbe": {},
  "replicas": 0,
  "resources": {},
  "serviceAccount": {
    "annotations": {},
    "automountServiceAccountToken": true,
    "create": false,
    "imagePullSecrets": [],
    "name": null
  },
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.affinity</td>
			<td>object</td>
			<td>Affinity for pattern ingester pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the pattern ingester</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the pattern ingester image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the pattern ingester image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the pattern ingester image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.livenessProbe</td>
			<td>object</td>
			<td>liveness probe settings for ingester pods. If empty use `loki.livenessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.nodeSelector</td>
			<td>object</td>
			<td>Node selector for pattern ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for pattern ingester PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.claims</td>
			<td>list</td>
			<td>List of the pattern ingester PVCs</td>
			<td><pre lang="list">

</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs for the pattern ingester</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.podAnnotations</td>
			<td>object</td>
			<td>Annotations for pattern ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.podLabels</td>
			<td>object</td>
			<td>Labels for pattern ingester pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for pattern ingester pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.readinessProbe</td>
			<td>object</td>
			<td>readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.replicas</td>
			<td>int</td>
			<td>Number of replicas for the pattern ingester</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the pattern ingester</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.serviceAccount.annotations</td>
			<td>object</td>
			<td>Annotations for the pattern ingester service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.serviceAccount.automountServiceAccountToken</td>
			<td>bool</td>
			<td>Set this toggle to false to opt out of automounting API credentials for the service account</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.serviceAccount.imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for the pattern ingester service account</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.serviceAccount.name</td>
			<td>string</td>
			<td>The name of the ServiceAccount to use for the pattern ingester. If not set and create is true, a name is generated by appending "-pattern-ingester" to the common ServiceAccount.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.serviceLabels</td>
			<td>object</td>
			<td>Labels for pattern ingester service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the pattern ingester to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>patternIngester.tolerations</td>
			<td>list</td>
			<td>Tolerations for pattern ingester pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier</td>
			<td>object</td>
			<td>Configuration for the querier</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "querier"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "autoscaling": {
    "behavior": {
      "enabled": false,
      "scaleDown": {},
      "scaleUp": {}
    },
    "customMetrics": [],
    "enabled": false,
    "maxReplicas": 3,
    "minReplicas": 1,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "command": null,
  "dnsConfig": {},
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "maxSurge": 0,
  "maxUnavailable": null,
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "enabled": false,
    "size": "10Gi",
    "storageClass": null
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": [],
  "topologySpreadConstraints": [
    {
      "labelSelector": {
        "matchLabels": {
          "app.kubernetes.io/component": "querier"
        }
      },
      "maxSkew": 1,
      "topologyKey": "kubernetes.io/hostname",
      "whenUnsatisfiable": "ScheduleAnyway"
    }
  ]
}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.affinity</td>
			<td>object</td>
			<td>Affinity for querier pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>querier.appProtocol</td>
			<td>object</td>
			<td>Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.appProtocol.grpc</td>
			<td>string</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.behavior.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling behaviours</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.behavior.scaleDown</td>
			<td>object</td>
			<td>define scale down policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.behavior.scaleUp</td>
			<td>object</td>
			<td>define scale up policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.customMetrics</td>
			<td>list</td>
			<td>Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the querier, this is only used if `indexGateway.enabled: true`</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the querier</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the querier</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the querier</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>querier.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the querier</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.dnsConfig</td>
			<td>object</td>
			<td>DNSConfig for querier pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the querier</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the querier image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the querier image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the querier image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.maxSurge</td>
			<td>int</td>
			<td>Max Surge for querier pods</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>querier.maxUnavailable</td>
			<td>string</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.nodeSelector</td>
			<td>object</td>
			<td>Node selector for querier pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for querier PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs for the querier cache</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>querier.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>querier.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.podAnnotations</td>
			<td>object</td>
			<td>Annotations for querier pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.podLabels</td>
			<td>object</td>
			<td>Labels for querier pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for querier pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>querier.replicas</td>
			<td>int</td>
			<td>Number of replicas for the querier</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>querier.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the querier</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.serviceLabels</td>
			<td>object</td>
			<td>Labels for querier service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>querier.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the querier to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>querier.tolerations</td>
			<td>list</td>
			<td>Tolerations for querier pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>querier.topologySpreadConstraints</td>
			<td>list</td>
			<td>topologySpread for querier pods.</td>
			<td><pre lang="">
Defaults to allow skew no more then 1 node
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend</td>
			<td>object</td>
			<td>Configuration for the query-frontend</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "query-frontend"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "autoscaling": {
    "behavior": {
      "enabled": false,
      "scaleDown": {},
      "scaleUp": {}
    },
    "customMetrics": [],
    "enabled": false,
    "maxReplicas": 3,
    "minReplicas": 1,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "command": null,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "maxUnavailable": null,
  "nodeSelector": {},
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.affinity</td>
			<td>object</td>
			<td>Affinity for query-frontend pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.appProtocol</td>
			<td>object</td>
			<td>Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.appProtocol.grpc</td>
			<td>string</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.behavior.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling behaviours</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.behavior.scaleDown</td>
			<td>object</td>
			<td>define scale down policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.behavior.scaleUp</td>
			<td>object</td>
			<td>define scale up policies, must conform to HPAScalingRules</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.customMetrics</td>
			<td>list</td>
			<td>Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the query-frontend</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the query-frontend</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the query-frontend</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the query-frontend</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the query-frontend</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the query-frontend</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the query-frontend image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the query-frontend image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the query-frontend image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.maxUnavailable</td>
			<td>string</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.nodeSelector</td>
			<td>object</td>
			<td>Node selector for query-frontend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.podAnnotations</td>
			<td>object</td>
			<td>Annotations for query-frontend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.podLabels</td>
			<td>object</td>
			<td>Labels for query-frontend pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for query-frontend pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.replicas</td>
			<td>int</td>
			<td>Number of replicas for the query-frontend</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the query-frontend</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.serviceLabels</td>
			<td>object</td>
			<td>Labels for query-frontend service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the query-frontend to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>queryFrontend.tolerations</td>
			<td>list</td>
			<td>Tolerations for query-frontend pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler</td>
			<td>object</td>
			<td>Configuration for the query-scheduler</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "query-scheduler"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "maxUnavailable": 1,
  "nodeSelector": {},
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.affinity</td>
			<td>object</td>
			<td>Affinity for query-scheduler pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the query-scheduler</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the query-scheduler image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the query-scheduler image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the query-scheduler image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.maxUnavailable</td>
			<td>int</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.nodeSelector</td>
			<td>object</td>
			<td>Node selector for query-scheduler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.podAnnotations</td>
			<td>object</td>
			<td>Annotations for query-scheduler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.podLabels</td>
			<td>object</td>
			<td>Labels for query-scheduler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for query-scheduler pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.replicas</td>
			<td>int</td>
			<td>Number of replicas for the query-scheduler. It should be lower than `-querier.max-concurrent` to avoid generating back-pressure in queriers; it's also recommended that this value evenly divides the latter</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the query-scheduler</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.serviceLabels</td>
			<td>object</td>
			<td>Labels for query-scheduler service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the query-scheduler to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>queryScheduler.tolerations</td>
			<td>list</td>
			<td>Tolerations for query-scheduler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>rbac.namespaced</td>
			<td>bool</td>
			<td>Whether to install RBAC in the namespace only or cluster-wide. Useful if you want to watch ConfigMap globally.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>rbac.pspAnnotations</td>
			<td>object</td>
			<td>Specify PSP annotations Ref: https://kubernetes.io/docs/reference/access-authn-authz/psp-to-pod-security-standards/#podsecuritypolicy-annotations</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>rbac.pspEnabled</td>
			<td>bool</td>
			<td>If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>rbac.sccEnabled</td>
			<td>bool</td>
			<td>For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>read</td>
			<td>object</td>
			<td>Configuration for the read pod(s)</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "read"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "annotations": {},
  "autoscaling": {
    "behavior": {},
    "enabled": false,
    "maxReplicas": 6,
    "minReplicas": 2,
    "targetCPUUtilizationPercentage": 60,
    "targetMemoryUtilizationPercentage": null
  },
  "dnsConfig": {},
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "legacyReadTarget": false,
  "lifecycle": {},
  "nodeSelector": {},
  "persistence": {
    "enableStatefulSetAutoDeletePVC": true,
    "selector": null,
    "size": "10Gi",
    "storageClass": null
  },
  "podAnnotations": {},
  "podLabels": {},
  "podManagementPolicy": "Parallel",
  "priorityClassName": null,
  "replicas": 3,
  "resources": {},
  "selectorLabels": {},
  "service": {
    "annotations": {},
    "labels": {}
  },
  "targetModule": "read",
  "terminationGracePeriodSeconds": 30,
  "tolerations": [],
  "topologySpreadConstraints": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>read.affinity</td>
			<td>object</td>
			<td>Affinity for read pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>read.annotations</td>
			<td>object</td>
			<td>Annotations for read deployment</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.behavior</td>
			<td>object</td>
			<td>Behavior policies while scaling.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the read</td>
			<td><pre lang="json">
6
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the read</td>
			<td><pre lang="json">
2
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the read</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>read.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the read</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.dnsConfig</td>
			<td>object</td>
			<td>DNS config for read pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the read</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the read image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the read image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the read image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.legacyReadTarget</td>
			<td>bool</td>
			<td>Whether or not to use the 2 target type simple scalable mode (read, write) or the 3 target type (read, write, backend). Legacy refers to the 2 target type, so true will run two targets, false will run 3 targets.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>read.lifecycle</td>
			<td>object</td>
			<td>Lifecycle for the read container</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.nodeSelector</td>
			<td>object</td>
			<td>Node selector for read pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>read.persistence.selector</td>
			<td>string</td>
			<td>Selector for persistent disk</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>read.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.podAnnotations</td>
			<td>object</td>
			<td>Annotations for read pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.podLabels</td>
			<td>object</td>
			<td>Additional labels for each `read` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.podManagementPolicy</td>
			<td>string</td>
			<td>The default is to deploy all pods in parallel.</td>
			<td><pre lang="json">
"Parallel"
</pre>
</td>
		</tr>
		<tr>
			<td>read.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for read pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>read.replicas</td>
			<td>int</td>
			<td>Number of replicas for the read</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>read.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the read</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.selectorLabels</td>
			<td>object</td>
			<td>Additional selector labels for each `read` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.service.annotations</td>
			<td>object</td>
			<td>Annotations for read Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.service.labels</td>
			<td>object</td>
			<td>Additional labels for read Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>read.targetModule</td>
			<td>string</td>
			<td>Comma-separated list of Loki modules to load for the read</td>
			<td><pre lang="json">
"read"
</pre>
</td>
		</tr>
		<tr>
			<td>read.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the read to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>read.tolerations</td>
			<td>list</td>
			<td>Tolerations for read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>read.topologySpreadConstraints</td>
			<td>list</td>
			<td>Topology Spread Constraints for read pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.affinity</td>
			<td>object</td>
			<td>Affinity for results-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.allocatedMemory</td>
			<td>int</td>
			<td>Amount of memory allocated to results-cache for object storage (in MB).</td>
			<td><pre lang="json">
1024
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.annotations</td>
			<td>object</td>
			<td>Annotations for the results-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.connectionLimit</td>
			<td>int</td>
			<td>Maximum number of connections allowed</td>
			<td><pre lang="json">
16384
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.defaultValidity</td>
			<td>string</td>
			<td>Specify how long cached results should be stored in the results-cache before being expired</td>
			<td><pre lang="json">
"12h"
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.enabled</td>
			<td>bool</td>
			<td>Specifies whether memcached based results-cache should be enabled</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.extraArgs</td>
			<td>object</td>
			<td>Additional CLI args for results-cache</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.extraContainers</td>
			<td>list</td>
			<td>Additional containers to be added to the results-cache pod.</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.extraExtendedOptions</td>
			<td>string</td>
			<td>Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag. Example: extraExtendedOptions: 'tls,modern,track_sizes'</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.extraVolumeMounts</td>
			<td>list</td>
			<td>Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers). Example: extraVolumeMounts: - name: extra-volume   mountPath: /etc/extra-volume   readOnly: true</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.extraVolumes</td>
			<td>list</td>
			<td>Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers). Example: extraVolumes: - name: extra-volume   secret:    secretName: extra-volume-secret</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.initContainers</td>
			<td>list</td>
			<td>Extra init containers for results-cache pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.maxItemMemory</td>
			<td>int</td>
			<td>Maximum item results-cache for memcached (in MB).</td>
			<td><pre lang="json">
5
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.nodeSelector</td>
			<td>object</td>
			<td>Node selector for results-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.podAnnotations</td>
			<td>object</td>
			<td>Annotations for results-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.podDisruptionBudget</td>
			<td>object</td>
			<td>Pod Disruption Budget</td>
			<td><pre lang="json">
{
  "maxUnavailable": 1
}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.podLabels</td>
			<td>object</td>
			<td>Labels for results-cache pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.podManagementPolicy</td>
			<td>string</td>
			<td>Management policy for results-cache pods</td>
			<td><pre lang="json">
"Parallel"
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.port</td>
			<td>int</td>
			<td>Port of the results-cache service</td>
			<td><pre lang="json">
11211
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for results-cache pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.replicas</td>
			<td>int</td>
			<td>Total number of results-cache replicas</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.resources</td>
			<td>string</td>
			<td>Resource requests and limits for the results-cache By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.service</td>
			<td>object</td>
			<td>Service annotations and labels</td>
			<td><pre lang="json">
{
  "annotations": {},
  "labels": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.statefulStrategy</td>
			<td>object</td>
			<td>Stateful results-cache strategy</td>
			<td><pre lang="json">
{
  "type": "RollingUpdate"
}
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the results-cache to shutdown before it is killed</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.timeout</td>
			<td>string</td>
			<td>Memcached operation timeout</td>
			<td><pre lang="json">
"500ms"
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.tolerations</td>
			<td>list</td>
			<td>Tolerations for results-cache pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.topologySpreadConstraints</td>
			<td>list</td>
			<td>topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints. labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.writebackBuffer</td>
			<td>int</td>
			<td>Max number of objects to use for cache write back</td>
			<td><pre lang="json">
500000
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.writebackParallelism</td>
			<td>int</td>
			<td>Number of parallel threads for cache write back</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>resultsCache.writebackSizeLimit</td>
			<td>string</td>
			<td>Max memory to use for cache write back</td>
			<td><pre lang="json">
"500MB"
</pre>
</td>
		</tr>
		<tr>
			<td>rollout_operator</td>
			<td>object</td>
			<td>Setting for the Grafana Rollout Operator https://github.com/grafana/helm-charts/tree/main/charts/rollout-operator</td>
			<td><pre lang="json">
{
  "enabled": false,
  "podSecurityContext": {
    "fsGroup": 10001,
    "runAsGroup": 10001,
    "runAsNonRoot": true,
    "runAsUser": 10001,
    "seccompProfile": {
      "type": "RuntimeDefault"
    }
  },
  "securityContext": {
    "allowPrivilegeEscalation": false,
    "capabilities": {
      "drop": [
        "ALL"
      ]
    },
    "readOnlyRootFilesystem": true
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>rollout_operator.podSecurityContext</td>
			<td>object</td>
			<td>podSecurityContext is the pod security context for the rollout operator. When installing on OpenShift, override podSecurityContext settings with  rollout_operator:   podSecurityContext:     fsGroup: null     runAsGroup: null     runAsUser: null</td>
			<td><pre lang="json">
{
  "fsGroup": 10001,
  "runAsGroup": 10001,
  "runAsNonRoot": true,
  "runAsUser": 10001,
  "seccompProfile": {
    "type": "RuntimeDefault"
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler</td>
			<td>object</td>
			<td>Configuration for the ruler</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "ruler"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "appProtocol": {
    "grpc": ""
  },
  "command": null,
  "directories": {},
  "dnsConfig": {},
  "enabled": true,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "hostAliases": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "initContainers": [],
  "maxUnavailable": null,
  "nodeSelector": {},
  "persistence": {
    "annotations": {},
    "enabled": false,
    "size": "10Gi",
    "storageClass": null
  },
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "replicas": 0,
  "resources": {},
  "serviceLabels": {},
  "terminationGracePeriodSeconds": 300,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.affinity</td>
			<td>object</td>
			<td>Affinity for ruler pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.appProtocol</td>
			<td>object</td>
			<td>Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"</td>
			<td><pre lang="json">
{
  "grpc": ""
}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.directories</td>
			<td>object</td>
			<td>Directories containing rules files</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.dnsConfig</td>
			<td>object</td>
			<td>DNSConfig for ruler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.enabled</td>
			<td>bool</td>
			<td>The ruler component is optional and can be disabled if desired.</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the ruler</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.hostAliases</td>
			<td>list</td>
			<td>hostAliases to add</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the ruler image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the ruler image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the ruler image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.maxUnavailable</td>
			<td>string</td>
			<td>Pod Disruption Budget maxUnavailable</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.nodeSelector</td>
			<td>object</td>
			<td>Node selector for ruler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.persistence.annotations</td>
			<td>object</td>
			<td>Annotations for ruler PVCs</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.persistence.enabled</td>
			<td>bool</td>
			<td>Enable creating PVCs which is required when using recording rules</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.podAnnotations</td>
			<td>object</td>
			<td>Annotations for ruler pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.podLabels</td>
			<td>object</td>
			<td>Labels for compactor pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for ruler pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.replicas</td>
			<td>int</td>
			<td>Number of replicas for the ruler</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the ruler</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.serviceLabels</td>
			<td>object</td>
			<td>Labels for ruler service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the ruler to shutdown before it is killed</td>
			<td><pre lang="json">
300
</pre>
</td>
		</tr>
		<tr>
			<td>ruler.tolerations</td>
			<td>list</td>
			<td>Tolerations for ruler pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.annotations</td>
			<td>object</td>
			<td>Annotations for the service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.automountServiceAccountToken</td>
			<td>bool</td>
			<td>Set this toggle to false to opt out of automounting API credentials for the service account</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.create</td>
			<td>bool</td>
			<td>Specifies whether a ServiceAccount should be created</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.imagePullSecrets</td>
			<td>list</td>
			<td>Image pull secrets for the service account</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.labels</td>
			<td>object</td>
			<td>Labels for the service account</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>serviceAccount.name</td>
			<td>string</td>
			<td>The name of the ServiceAccount to use. If not set and create is true, a name is generated using the fullname template</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.enableUniqueFilenames</td>
			<td>bool</td>
			<td>Ensure that rule files aren't conflicting and being overwritten by prefixing their name with the namespace they are defined in.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.image.repository</td>
			<td>string</td>
			<td>The Docker registry and image for the k8s sidecar</td>
			<td><pre lang="json">
"kiwigrid/k8s-sidecar"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.image.sha</td>
			<td>string</td>
			<td>Docker image sha. If empty, no sha will be used</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.image.tag</td>
			<td>string</td>
			<td>Docker image tag</td>
			<td><pre lang="json">
"1.24.3"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.livenessProbe</td>
			<td>object</td>
			<td>Liveness probe definition. Probe is disabled on the sidecar by default.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.readinessProbe</td>
			<td>object</td>
			<td>Readiness probe definition. Probe is disabled on the sidecar by default.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the sidecar</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.enabled</td>
			<td>bool</td>
			<td>Whether or not to create a sidecar to ingest rule from specific ConfigMaps and/or Secrets.</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.folder</td>
			<td>string</td>
			<td>Folder into which the rules will be placed.</td>
			<td><pre lang="json">
"/rules"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.label</td>
			<td>string</td>
			<td>Label that the configmaps/secrets with rules will be marked with.</td>
			<td><pre lang="json">
"loki_rule"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.labelValue</td>
			<td>string</td>
			<td>Label value that the configmaps/secrets with rules will be set to.</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.logLevel</td>
			<td>string</td>
			<td>Log level of the sidecar container.</td>
			<td><pre lang="json">
"INFO"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.resource</td>
			<td>string</td>
			<td>Search in configmap, secret, or both.</td>
			<td><pre lang="json">
"both"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.script</td>
			<td>string</td>
			<td>Absolute path to the shell script to execute after a configmap or secret has been reloaded.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.searchNamespace</td>
			<td>string</td>
			<td>Comma separated list of namespaces. If specified, the sidecar will search for config-maps/secrets inside these namespaces. Otherwise the namespace in which the sidecar is running will be used. It's also possible to specify 'ALL' to search in all namespaces.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.watchClientTimeout</td>
			<td>int</td>
			<td>WatchClientTimeout: is a client-side timeout, configuring your local socket. If you have a network outage dropping all packets with no RST/FIN, this is how long your client waits before realizing & dropping the connection. Defaults to 66sec.</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.watchMethod</td>
			<td>string</td>
			<td>Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH request, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.</td>
			<td><pre lang="json">
"WATCH"
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.rules.watchServerTimeout</td>
			<td>int</td>
			<td>WatchServerTimeout: request to the server, asking it to cleanly close the connection after that. defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S.</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.securityContext</td>
			<td>object</td>
			<td>The SecurityContext for the sidecar.</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>sidecar.skipTlsVerify</td>
			<td>bool</td>
			<td>Set to true to skip tls verification for kube api calls.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.affinity</td>
			<td>object</td>
			<td>Affinity for single binary pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.annotations</td>
			<td>object</td>
			<td>Annotations for single binary StatefulSet</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the single binary</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the single binary</td>
			<td><pre lang="json">
1
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the single binary</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilisation percentage for the single binary</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.dnsConfig</td>
			<td>object</td>
			<td>DNS config for single binary pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraArgs</td>
			<td>list</td>
			<td>Labels for single binary service</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraContainers</td>
			<td>list</td>
			<td>Extra containers to add to the single binary loki pod</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the single binary image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the single binary image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the single binary image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.nodeSelector</td>
			<td>object</td>
			<td>Node selector for single binary pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.persistence.enabled</td>
			<td>bool</td>
			<td>Enable persistent disk</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.persistence.selector</td>
			<td>string</td>
			<td>Selector for persistent disk</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.podAnnotations</td>
			<td>object</td>
			<td>Annotations for single binary pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.podLabels</td>
			<td>object</td>
			<td>Additional labels for each `single binary` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for single binary pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.replicas</td>
			<td>int</td>
			<td>Number of replicas for the single binary</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the single binary</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.selectorLabels</td>
			<td>object</td>
			<td>Additional selector labels for each `single binary` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.service.annotations</td>
			<td>object</td>
			<td>Annotations for single binary Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.service.labels</td>
			<td>object</td>
			<td>Additional labels for single binary Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.targetModule</td>
			<td>string</td>
			<td>Comma-separated list of Loki modules to load for the single binary</td>
			<td><pre lang="json">
"all"
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the single binary to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>singleBinary.tolerations</td>
			<td>list</td>
			<td>Tolerations for single binary pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager</td>
			<td>object</td>
			<td>DEPRECATED Configuration for the table-manager. The table-manager is only necessary when using a deprecated index type such as Cassandra, Bigtable, or DynamoDB, it has not been necessary since loki introduced self- contained index types like 'boltdb-shipper' and 'tsdb'. This will be removed in a future helm chart.</td>
			<td><pre lang="json">
{
  "affinity": {
    "podAntiAffinity": {
      "requiredDuringSchedulingIgnoredDuringExecution": [
        {
          "labelSelector": {
            "matchLabels": {
              "app.kubernetes.io/component": "table-manager"
            }
          },
          "topologyKey": "kubernetes.io/hostname"
        }
      ]
    }
  },
  "annotations": {},
  "command": null,
  "dnsConfig": {},
  "enabled": false,
  "extraArgs": [],
  "extraContainers": [],
  "extraEnv": [],
  "extraEnvFrom": [],
  "extraVolumeMounts": [],
  "extraVolumes": [],
  "image": {
    "registry": null,
    "repository": null,
    "tag": null
  },
  "nodeSelector": {},
  "podAnnotations": {},
  "podLabels": {},
  "priorityClassName": null,
  "resources": {},
  "retention_deletes_enabled": false,
  "retention_period": 0,
  "service": {
    "annotations": {},
    "labels": {}
  },
  "terminationGracePeriodSeconds": 30,
  "tolerations": []
}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.affinity</td>
			<td>object</td>
			<td>Affinity for table-manager pods.</td>
			<td><pre lang="">
Hard node and anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.annotations</td>
			<td>object</td>
			<td>Annotations for table-manager deployment</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.command</td>
			<td>string</td>
			<td>Command to execute instead of defined in Docker image</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.dnsConfig</td>
			<td>object</td>
			<td>DNS config table-manager pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.enabled</td>
			<td>bool</td>
			<td>Specifies whether the table-manager should be enabled</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the table-manager</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the table-manager image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the table-manager image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the table-manager image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.nodeSelector</td>
			<td>object</td>
			<td>Node selector for table-manager pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.podAnnotations</td>
			<td>object</td>
			<td>Annotations for table-manager pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.podLabels</td>
			<td>object</td>
			<td>Labels for table-manager pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for table-manager pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the table-manager</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.retention_deletes_enabled</td>
			<td>bool</td>
			<td>Enable deletes by retention</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.retention_period</td>
			<td>int</td>
			<td>Set retention period</td>
			<td><pre lang="json">
0
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.service.annotations</td>
			<td>object</td>
			<td>Annotations for table-manager Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.service.labels</td>
			<td>object</td>
			<td>Additional labels for table-manager Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the table-manager to shutdown before it is killed</td>
			<td><pre lang="json">
30
</pre>
</td>
		</tr>
		<tr>
			<td>tableManager.tolerations</td>
			<td>list</td>
			<td>Tolerations for table-manager pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>test</td>
			<td>object</td>
			<td>Section for configuring optional Helm test</td>
			<td><pre lang="json">
{
  "annotations": {},
  "canaryServiceAddress": "http://loki-canary:3500/metrics",
  "enabled": true,
  "image": {
    "digest": null,
    "pullPolicy": "IfNotPresent",
    "registry": "docker.io",
    "repository": "grafana/loki-helm-test",
    "tag": "ewelch-distributed-helm-chart-17db5ee"
  },
  "labels": {},
  "prometheusAddress": "",
  "timeout": "1m"
}
</pre>
</td>
		</tr>
		<tr>
			<td>test.annotations</td>
			<td>object</td>
			<td>Additional annotations for test pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>test.canaryServiceAddress</td>
			<td>string</td>
			<td>Used to directly query the metrics endpoint of the canary for testing, this approach avoids needing prometheus for testing. This in a newer approach to using prometheusAddress such that tests do not have a dependency on prometheus</td>
			<td><pre lang="json">
"http://loki-canary:3500/metrics"
</pre>
</td>
		</tr>
		<tr>
			<td>test.image</td>
			<td>object</td>
			<td>Image to use for loki canary</td>
			<td><pre lang="json">
{
  "digest": null,
  "pullPolicy": "IfNotPresent",
  "registry": "docker.io",
  "repository": "grafana/loki-helm-test",
  "tag": "ewelch-distributed-helm-chart-17db5ee"
}
</pre>
</td>
		</tr>
		<tr>
			<td>test.image.digest</td>
			<td>string</td>
			<td>Overrides the image tag with an image digest</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>test.image.pullPolicy</td>
			<td>string</td>
			<td>Docker image pull policy</td>
			<td><pre lang="json">
"IfNotPresent"
</pre>
</td>
		</tr>
		<tr>
			<td>test.image.registry</td>
			<td>string</td>
			<td>The Docker registry</td>
			<td><pre lang="json">
"docker.io"
</pre>
</td>
		</tr>
		<tr>
			<td>test.image.repository</td>
			<td>string</td>
			<td>Docker image repository</td>
			<td><pre lang="json">
"grafana/loki-helm-test"
</pre>
</td>
		</tr>
		<tr>
			<td>test.image.tag</td>
			<td>string</td>
			<td>Overrides the image tag whose default is the chart's appVersion</td>
			<td><pre lang="json">
"ewelch-distributed-helm-chart-17db5ee"
</pre>
</td>
		</tr>
		<tr>
			<td>test.labels</td>
			<td>object</td>
			<td>Additional labels for the test pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>test.prometheusAddress</td>
			<td>string</td>
			<td>Address of the prometheus server to query for the test. This overrides any value set for canaryServiceAddress. This is kept for backward compatibility and may be removed in future releases. Previous value was 'http://prometheus:9090'</td>
			<td><pre lang="json">
""
</pre>
</td>
		</tr>
		<tr>
			<td>test.timeout</td>
			<td>string</td>
			<td>Number of times to retry the test before failing</td>
			<td><pre lang="json">
"1m"
</pre>
</td>
		</tr>
		<tr>
			<td>write.affinity</td>
			<td>object</td>
			<td>Affinity for write pods.</td>
			<td><pre lang="">
Hard node anti-affinity
</pre>
</td>
		</tr>
		<tr>
			<td>write.annotations</td>
			<td>object</td>
			<td>Annotations for write StatefulSet</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.behavior</td>
			<td>object</td>
			<td>Behavior policies while scaling.</td>
			<td><pre lang="json">
{
  "scaleDown": {
    "policies": [
      {
        "periodSeconds": 1800,
        "type": "Pods",
        "value": 1
      }
    ],
    "stabilizationWindowSeconds": 3600
  },
  "scaleUp": {
    "policies": [
      {
        "periodSeconds": 900,
        "type": "Pods",
        "value": 1
      }
    ]
  }
}
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.behavior.scaleUp</td>
			<td>object</td>
			<td>see https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown for scaledown details</td>
			<td><pre lang="json">
{
  "policies": [
    {
      "periodSeconds": 900,
      "type": "Pods",
      "value": 1
    }
  ]
}
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.enabled</td>
			<td>bool</td>
			<td>Enable autoscaling for the write.</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.maxReplicas</td>
			<td>int</td>
			<td>Maximum autoscaling replicas for the write.</td>
			<td><pre lang="json">
6
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.minReplicas</td>
			<td>int</td>
			<td>Minimum autoscaling replicas for the write.</td>
			<td><pre lang="json">
2
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.targetCPUUtilizationPercentage</td>
			<td>int</td>
			<td>Target CPU utilisation percentage for the write.</td>
			<td><pre lang="json">
60
</pre>
</td>
		</tr>
		<tr>
			<td>write.autoscaling.targetMemoryUtilizationPercentage</td>
			<td>string</td>
			<td>Target memory utilization percentage for the write.</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.dnsConfig</td>
			<td>object</td>
			<td>DNS config for write pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraArgs</td>
			<td>list</td>
			<td>Additional CLI args for the write</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraContainers</td>
			<td>list</td>
			<td>Containers to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraEnv</td>
			<td>list</td>
			<td>Environment variables to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraEnvFrom</td>
			<td>list</td>
			<td>Environment variables from secrets or configmaps to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraVolumeClaimTemplates</td>
			<td>list</td>
			<td>volumeClaimTemplates to add to StatefulSet</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraVolumeMounts</td>
			<td>list</td>
			<td>Volume mounts to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.extraVolumes</td>
			<td>list</td>
			<td>Volumes to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.image.registry</td>
			<td>string</td>
			<td>The Docker registry for the write image. Overrides `loki.image.registry`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.image.repository</td>
			<td>string</td>
			<td>Docker image repository for the write image. Overrides `loki.image.repository`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.image.tag</td>
			<td>string</td>
			<td>Docker image tag for the write image. Overrides `loki.image.tag`</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.initContainers</td>
			<td>list</td>
			<td>Init containers to add to the write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.lifecycle</td>
			<td>object</td>
			<td>Lifecycle for the write container</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.nodeSelector</td>
			<td>object</td>
			<td>Node selector for write pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.dataVolumeParameters</td>
			<td>object</td>
			<td>Parameters used for the `data` volume when volumeClaimEnabled if false</td>
			<td><pre lang="json">
{
  "emptyDir": {}
}
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.enableStatefulSetAutoDeletePVC</td>
			<td>bool</td>
			<td>Enable StatefulSetAutoDeletePVC feature</td>
			<td><pre lang="json">
false
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.selector</td>
			<td>string</td>
			<td>Selector for persistent disk</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.size</td>
			<td>string</td>
			<td>Size of persistent disk</td>
			<td><pre lang="json">
"10Gi"
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.storageClass</td>
			<td>string</td>
			<td>Storage class to be used. If defined, storageClassName: <storageClass>. If set to "-", storageClassName: "", which disables dynamic provisioning. If empty or set to null, no storageClassName spec is set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.persistence.volumeClaimsEnabled</td>
			<td>bool</td>
			<td>Enable volume claims in pod spec</td>
			<td><pre lang="json">
true
</pre>
</td>
		</tr>
		<tr>
			<td>write.podAnnotations</td>
			<td>object</td>
			<td>Annotations for write pods</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.podLabels</td>
			<td>object</td>
			<td>Additional labels for each `write` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.podManagementPolicy</td>
			<td>string</td>
			<td>The default is to deploy all pods in parallel.</td>
			<td><pre lang="json">
"Parallel"
</pre>
</td>
		</tr>
		<tr>
			<td>write.priorityClassName</td>
			<td>string</td>
			<td>The name of the PriorityClass for write pods</td>
			<td><pre lang="json">
null
</pre>
</td>
		</tr>
		<tr>
			<td>write.replicas</td>
			<td>int</td>
			<td>Number of replicas for the write</td>
			<td><pre lang="json">
3
</pre>
</td>
		</tr>
		<tr>
			<td>write.resources</td>
			<td>object</td>
			<td>Resource requests and limits for the write</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.selectorLabels</td>
			<td>object</td>
			<td>Additional selector labels for each `write` pod</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.service.annotations</td>
			<td>object</td>
			<td>Annotations for write Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.service.labels</td>
			<td>object</td>
			<td>Additional labels for write Service</td>
			<td><pre lang="json">
{}
</pre>
</td>
		</tr>
		<tr>
			<td>write.targetModule</td>
			<td>string</td>
			<td>Comma-separated list of Loki modules to load for the write</td>
			<td><pre lang="json">
"write"
</pre>
</td>
		</tr>
		<tr>
			<td>write.terminationGracePeriodSeconds</td>
			<td>int</td>
			<td>Grace period to allow the write to shutdown before it is killed. Especially for the ingester, this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring all data and to successfully leave the member ring on shutdown.</td>
			<td><pre lang="json">
300
</pre>
</td>
		</tr>
		<tr>
			<td>write.tolerations</td>
			<td>list</td>
			<td>Tolerations for write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
		<tr>
			<td>write.topologySpreadConstraints</td>
			<td>list</td>
			<td>Topology Spread Constraints for write pods</td>
			<td><pre lang="json">
[]
</pre>
</td>
		</tr>
	</tbody>
</table>
{{< /responsive-table >}}

