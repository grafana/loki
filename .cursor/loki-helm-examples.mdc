# Loki Helm Chart - Examples & Best Practices

## Overview

This document provides production-ready configuration examples based on the official example files in `ci/`, `scenarios/`, `docs/examples/`, and the main values files.

---

## Deployment Mode Examples

### Example 1: SingleBinary for Development/Testing

**File**: `single-binary-values.yaml`  
**Use Case**: Local development, CI/CD testing, small deployments (<100GB/day)  
**Storage**: MinIO (S3-compatible)

```yaml
---
deploymentMode: SingleBinary

loki:
  # Low replication for testing
  commonConfig:
    replication_factor: 1
  
  # Production-like schema
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  
  # Optimized for single binary
  ingester:
    chunk_encoding: snappy
  
  querier:
    max_concurrent: 2  # Lower than distributed (default 4)
  
  tracing:
    enabled: true

singleBinary:
  replicas: 1
  
  # Size appropriately
  resources:
    limits:
      cpu: 3
      memory: 4Gi
    requests:
      cpu: 2
      memory: 2Gi
  
  # GOMEMLIMIT for Go 1.19+
  extraEnv:
    - name: GOMEMLIMIT
      value: 3750MiB  # ~90% of memory limit
  
  # Persistence for data
  persistence:
    enabled: true
    size: 10Gi

# Reduce cache for smaller deployments
chunksCache:
  writebackSizeLimit: 10MB  # Default 500MB

# Enable MinIO for local storage
minio:
  enabled: true
  persistence:
    size: 10Gi

# Zero out other deployment modes
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0
ingester:
  replicas: 0
querier:
  replicas: 0
queryFrontend:
  replicas: 0
queryScheduler:
  replicas: 0
distributor:
  replicas: 0
compactor:
  replicas: 0
indexGateway:
  replicas: 0
bloomCompactor:
  replicas: 0
bloomGateway:
  replicas: 0
```

**Testing Configuration** (`ci/default-single-binary-values.yaml`):
```yaml
loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true  # Simplified schema for testing
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
      admin: admin

deploymentMode: SingleBinary
singleBinary:
  replicas: 1

# Disable other modes
read:
  replicas: 0
write:
  replicas: 0
backend:
  replicas: 0
```

---

### Example 2: SimpleScalable for Production

**File**: `simple-scalable-values.yaml`  
**Use Case**: Medium deployments (100GB-1TB/day), easier than distributed  
**Storage**: Object storage required

```yaml
---
deploymentMode: SimpleScalable

loki:
  # Production schema
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  
  ingester:
    chunk_encoding: snappy
  
  querier:
    max_concurrent: 4
  
  tracing:
    enabled: true

# Write path (ingester + distributor)
write:
  replicas: 3
  persistence:
    volumeClaimsEnabled: true
    size: 10Gi
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi

# Read path (querier + query-frontend)
read:
  replicas: 3
  legacyReadTarget: false  # Use 3-target mode
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi

# Backend (compactor + ruler + scheduler + index-gateway)
backend:
  replicas: 3
  persistence:
    volumeClaimsEnabled: true
    size: 10Gi
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

# Enable caching
chunksCache:
  enabled: true
  allocatedMemory: 4096
  replicas: 3

resultsCache:
  enabled: true
  allocatedMemory: 1024
  replicas: 1

# Object storage
minio:
  enabled: true  # Replace with real object storage for production

# Zero out other modes
singleBinary:
  replicas: 0
ingester:
  replicas: 0
querier:
  replicas: 0
queryFrontend:
  replicas: 0
queryScheduler:
  replicas: 0
distributor:
  replicas: 0
compactor:
  replicas: 0
indexGateway:
  replicas: 0
bloomGateway:
  replicas: 0
bloomCompactor:
  replicas: 0
```

**CI Configuration** (`ci/default-values.yaml`):
```yaml
loki:
  commonConfig:
    replication_factor: 1  # Lower for CI
  useTestSchema: true
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
      admin: admin

read:
  replicas: 1
write:
  replicas: 1
backend:
  replicas: 1
```

---

### Example 3: Distributed for Large Scale

**File**: `distributed-values.yaml`  
**Use Case**: Large deployments (>1TB/day), production at scale  
**Storage**: Object storage required

```yaml
---
deploymentMode: Distributed

loki:
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  
  ingester:
    chunk_encoding: snappy
  
  querier:
    max_concurrent: 4
  
  tracing:
    enabled: true

# Write path
distributor:
  replicas: 3
  maxUnavailable: 2
  resources:
    requests:
      cpu: 1
      memory: 1Gi
    limits:
      cpu: 2
      memory: 2Gi

ingester:
  replicas: 3  # Or 9 for zone-aware (3 per zone)
  zoneAwareReplication:
    enabled: false  # Enable for production
  persistence:
    enabled: true
    claims:
      - name: data
        size: 50Gi
  resources:
    requests:
      cpu: 2
      memory: 8Gi
    limits:
      cpu: 4
      memory: 16Gi

# Read path
querier:
  replicas: 3
  maxUnavailable: 2
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi

queryFrontend:
  replicas: 2
  maxUnavailable: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

queryScheduler:
  replicas: 2
  resources:
    requests:
      cpu: 500m
      memory: 512Mi

# Backend components
compactor:
  replicas: 1
  persistence:
    enabled: true
    claims:
      - name: data
        size: 10Gi
  resources:
    requests:
      cpu: 2
      memory: 4Gi

indexGateway:
  replicas: 2
  maxUnavailable: 1
  persistence:
    enabled: true
    size: 50Gi
  resources:
    requests:
      cpu: 2
      memory: 4Gi

ruler:
  enabled: true
  replicas: 1
  persistence:
    enabled: true
    size: 10Gi

# Experimental (disabled)
bloomPlanner:
  replicas: 0
bloomBuilder:
  replicas: 0
bloomGateway:
  replicas: 0

# Object storage
minio:
  enabled: true  # Replace with real object storage

# Caching
chunksCache:
  enabled: true
  allocatedMemory: 8192
  replicas: 3

resultsCache:
  enabled: true
  allocatedMemory: 2048
  replicas: 2

# Zero out SimpleScalable components
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0
singleBinary:
  replicas: 0
```

---

## Production Scenarios

### Scenario 1: AWS with IRSA

**File**: `scenarios/simple-scalable-aws-kube-irsa-values.yaml`  
**Use Case**: SimpleScalable on AWS EKS with IAM Roles for Service Accounts

```yaml
loki:
  storage:
    type: s3
    bucketNames:
      chunks: aws-s3-chunks-bucket
      ruler: aws-s3-ruler-bucket
      admin: aws-s3-admin-bucket
    s3:
      region: eu-central-1
      # No credentials - using IRSA
  
  schemaConfig:
    configs:
      - from: "2023-09-19"
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: tsdb_index_
          period: 1d

# Service account with IRSA annotation
serviceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::222222222:role/loki-role

# Enterprise configuration
enterprise:
  enabled: true
  license:
    contents: "ACTUAL_LICENSE_JWT"
  adminToken:
    secret: loki-admin-token
  provisioner:
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::222222222:role/loki-role

# Configure persistence with EBS
write:
  persistence:
    storageClass: gp2  # or gp3

read:
  persistence:
    storageClass: gp2

backend:
  persistence:
    storageClass: gp2
```

**IAM Policy Example**:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::aws-s3-chunks-bucket",
        "arn:aws:s3:::aws-s3-chunks-bucket/*",
        "arn:aws:s3:::aws-s3-ruler-bucket",
        "arn:aws:s3:::aws-s3-ruler-bucket/*",
        "arn:aws:s3:::aws-s3-admin-bucket",
        "arn:aws:s3:::aws-s3-admin-bucket/*"
      ]
    }
  ]
}
```

---

### Scenario 2: GCS with Workload Identity

**File**: `docs/examples/oss/overrides-oss-gcs.yaml`  
**Use Case**: SimpleScalable on GKE with Workload Identity

```yaml
loki:
  auth_enabled: false
  
  commonConfig:
    path_prefix: /var/loki
    replication_factor: 3
  
  storage:
    type: gcs
    bucketNames:
      chunks: my-gcs-bucket
      ruler: my-gcs-bucket
      admin: my-gcs-bucket

# No minio
minio:
  enabled: false

# Mount GCS service account (alternative to Workload Identity)
write:
  extraEnv:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "/etc/loki_secrets/gcp_service_account.json"
  extraVolumeMounts:
    - name: loki-secrets
      mountPath: "/etc/loki_secrets"
  extraVolumes:
    - name: loki-secrets
      secret:
        secretName: loki-secrets
        items:
          - key: gcp_service_account.json
            path: gcp_service_account.json

read:
  extraEnv:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "/etc/loki_secrets/gcp_service_account.json"
  extraVolumeMounts:
    - name: loki-secrets
      mountPath: "/etc/loki_secrets"
  extraVolumes:
    - name: loki-secrets
      secret:
        secretName: loki-secrets

gateway:
  extraEnv:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "/etc/loki_secrets/gcp_service_account.json"
  extraVolumeMounts:
    - name: loki-secrets
      mountPath: "/etc/loki_secrets"
  extraVolumes:
    - name: loki-secrets
      secret:
        secretName: loki-secrets
```

**Secret Creation**:
```bash
kubectl create secret generic loki-secrets \
  --from-file=gcp_service_account.json=./service-account-key.json \
  -n loki
```

---

### Scenario 3: Enterprise GEL with GCS

**File**: `docs/examples/enterprise/overrides-enterprise-gcs.yaml`  
**Use Case**: Grafana Enterprise Logs with GCS backend

```yaml
enterprise:
  enabled: true
  useExternalLicense: true
  externalLicenseName: gel-secrets
  adminToken:
    secret: loki-admin-token  # Must create manually

loki:
  auth_enabled: true
  
  storage:
    type: gcs
    bucketNames:
      chunks: my-gcs-bucket
      ruler: my-gcs-bucket
      admin: my-gcs-bucket

minio:
  enabled: false

# Mount license and service account
write:
  extraEnv:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "/etc/gel_secrets/gcp_service_account.json"
  extraVolumeMounts:
    - name: gel-secrets
      mountPath: "/etc/gel_secrets"
  extraVolumes:
    - name: gel-secrets
      secret:
        secretName: gel-secrets
        items:
          - key: license.jwt
            path: license.jwt
          - key: gcp_service_account.json
            path: gcp_service_account.json

# Repeat for read, gateway
```

**Secret Structure** (`docs/examples/enterprise/enterprise-secrets.yaml`):
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: gel-secrets
type: Opaque
stringData:
  gcp_service_account.json: |
    {
      "type": "service_account",
      "project_id": "my-project",
      ...
    }
  license.jwt: |
    eyJhbGc...actual_license_content
```

---

### Scenario 4: Thanos Object Store

**File**: `scenarios/simple-thanos-values.yaml`  
**Use Case**: Using Thanos-style object store configuration

```yaml
loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true
  
  storage:
    type: s3
    use_thanos_objstore: true
    
    # Thanos-style config
    object_store:
      type: s3
      s3:
        access_key_id: thanos-minio
        secret_access_key: thanos-minio123
        region: us-east-1
        insecure: true
        endpoint: http://minio.minio.svc.cluster.local:9000
        http:
          tls_config:
            insecure_skip_verify: true
      
      # GCS example
      gcs:
        bucket_name: test-gcs
        service_account: service-account-test.json
      
      # Azure example
      azure:
        account_name: azure-test
        account_key: "1234567890"
    
    # Bucket names still required
    bucketNames:
      chunks: chunks_thanos
      ruler: ruler_thanos
      admin: admin_thanos

enterprise:
  enabled: true
  adminApi:
    enabled: true
  adminToken:
    secret: loki-admin-token

read:
  replicas: 1
write:
  replicas: 1
backend:
  replicas: 1
```

**Benefits**:
- Consistent with Thanos/Cortex/Mimir
- Better structure for complex object store config
- Future default for Loki

---

### Scenario 5: Ingress Instead of Gateway

**File**: `ci/ingress-values.yaml`  
**Use Case**: Use Kubernetes Ingress Controller instead of nginx gateway

```yaml
# Disable gateway
gateway:
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - host: gateway.loki.example.com
        paths:
          - path: /
            pathType: Prefix

loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
      admin: admin
  
  # Optional pod annotations for ingress controller
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3100"

read:
  replicas: 1
write:
  replicas: 1
backend:
  replicas: 1

# Disable canary and test for CI
monitoring:
  lokiCanary:
    enabled: false
test:
  enabled: false
```

**Ingress with Auth**:
```yaml
ingress:
  enabled: true
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: loki-auth
    nginx.ingress.kubernetes.io/auth-secret-type: auth-map
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header X-Scope-OrgID $remote_user;
  hosts:
    - loki.example.com
  tls:
    - hosts:
        - loki.example.com
      secretName: loki-tls
```

---

### Scenario 6: Legacy Monitoring Setup

**File**: `ci/legacy-monitoring-values.yaml`, `scenarios/legacy-monitoring-values.yaml`  
**Use Case**: Self-monitoring with Grafana Agent Operator

```yaml
loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
      admin: admin
  
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3100"

read:
  replicas: 1
write:
  replicas: 1
backend:
  replicas: 1

monitoring:
  enabled: true
  
  selfMonitoring:
    enabled: true
    grafanaAgent:
      installOperator: true
  
  serviceMonitor:
    labels:
      release: "prometheus"  # Match your Prometheus operator label

test:
  prometheusAddress: "http://prometheus-kube-prometheus-prometheus.prometheus.svc.cluster.local.:9090"
```

---

### Scenario 7: Zone-Aware Ingesters (Distributed)

**File**: Custom production configuration  
**Use Case**: High availability with zone distribution

```yaml
deploymentMode: Distributed

# Enable rollout operator for controlled rollouts
rollout_operator:
  enabled: true

ingester:
  # Total 9 replicas = 3 per zone
  replicas: 9
  
  # Enable zone-aware replication
  zoneAwareReplication:
    enabled: true
    maxUnavailablePct: 33  # 1 pod per zone
    
    zoneA:
      nodeSelector:
        topology.kubernetes.io/zone: us-west-2a
      extraAffinity: {}
    
    zoneB:
      nodeSelector:
        topology.kubernetes.io/zone: us-west-2b
    
    zoneC:
      nodeSelector:
        topology.kubernetes.io/zone: us-west-2c
  
  persistence:
    enabled: true
    claims:
      - name: data
        size: 100Gi
        accessModes: [ReadWriteOnce]
        storageClass: fast-ssd
  
  resources:
    requests:
      cpu: 4
      memory: 16Gi
    limits:
      cpu: 8
      memory: 32Gi
  
  # Longer grace period for WAL drain
  terminationGracePeriodSeconds: 600
  
  # Autoscaling per zone
  autoscaling:
    enabled: true
    minReplicas: 1  # Per zone
    maxReplicas: 5   # Per zone
    targetCPUUtilizationPercentage: 70
    behavior:
      scaleDown:
        policies:
          - type: Pods
            value: 1
            periodSeconds: 1800  # 30 min
        stabilizationWindowSeconds: 3600  # 1 hour

# Distributor with zone awareness
distributor:
  replicas: 5
  # Zone awareness enabled automatically via args if ingester zones enabled

# Other components
querier:
  replicas: 6
queryFrontend:
  replicas: 3
queryScheduler:
  replicas: 2
compactor:
  replicas: 1
indexGateway:
  replicas: 3
```

---

## Best Practices

### Practice 1: Resource Sizing

**Ingester/Write** (memory-intensive):
```yaml
write:  # or ingester
  resources:
    requests:
      memory: 8Gi  # Base + (2GB per 1M active streams)
    limits:
      memory: 16Gi  # 2x requests
  extraEnv:
    - name: GOMEMLIMIT
      value: 15Gi  # 90% of memory limit
```

**Querier/Read** (CPU-intensive):
```yaml
read:  # or querier
  resources:
    requests:
      cpu: 4
      memory: 4Gi
    limits:
      cpu: 8  # 2x requests
      memory: 8Gi
```

**Compactor** (burst workload):
```yaml
compactor:
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi
```

### Practice 2: Affinity Configuration

**Prefer Topology Spread** over affinity for better distribution:

```yaml
[component]:
  # Remove or soften affinity
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: [component]
            topologyKey: kubernetes.io/hostname
  
  # Add topology spread
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app.kubernetes.io/component: [component]
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
```

**Benefit**: Better spreading with flexible scheduling

### Practice 3: Persistence Strategy

**Stateful Components** (ingester, write):
```yaml
persistence:
  enabled: true
  enableStatefulSetAutoDeletePVC: false  # Retain data
  whenDeleted: Retain
  whenScaled: Retain
  size: 50Gi  # Size for WAL + chunks
  storageClass: fast-ssd
```

**Ephemeral Components** (backend, read legacy):
```yaml
persistence:
  enabled: true
  enableStatefulSetAutoDeletePVC: true  # Auto-cleanup
  whenDeleted: Delete
  whenScaled: Delete
  size: 10Gi
```

**Querier** (stateless in 3-target mode):
```yaml
# No persistence needed
# Uses emptyDir for temporary data
```

### Practice 4: Autoscaling Configuration

**Stateless Components** (distributor, querier, query-frontend):
```yaml
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  behavior:
    scaleUp:
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
```

**Stateful Components** (ingester, write):
```yaml
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 12
  targetMemoryUtilizationPercentage: 80  # Memory-based
  behavior:
    scaleUp:
      policies:
        - type: Pods
          value: 1
          periodSeconds: 900  # 15 min
    scaleDown:
      stabilizationWindowSeconds: 3600  # 1 hour
      policies:
        - type: Pods
          value: 1
          periodSeconds: 1800  # 30 min
```

**Rationale**: Slow scale-down for WAL drain

### Practice 5: Limits Configuration

**Production Limits**:
```yaml
loki:
  limits_config:
    # Ingestion
    ingestion_rate_mb: 10
    ingestion_burst_size_mb: 20
    max_line_size: 256KB
    reject_old_samples: true
    reject_old_samples_max_age: 168h  # 1 week
    
    # Queries
    max_query_length: 721h  # 30 days
    max_query_lookback: 0  # No limit
    max_query_parallelism: 32
    query_timeout: 300s
    
    # Streams
    max_streams_per_user: 100000
    max_global_streams_per_user: 0
    
    # Cardinality
    max_label_names_per_series: 30
    max_label_name_length: 1024
    max_label_value_length: 2048
    
    # Retention
    retention_period: 744h  # 31 days
    
    # Cache
    max_cache_freshness_per_query: 10m
    split_queries_by_interval: 15m
    
    # Volume API
    volume_enabled: true
```

### Practice 6: Caching Strategy

**High-Traffic Deployment**:
```yaml
# Chunks cache (reduce object storage reads)
chunksCache:
  enabled: true
  allocatedMemory: 16384  # 16GB L1
  replicas: 3
  persistence:
    enabled: true
    storageSize: 50G  # Persistent cache
  l2:
    enabled: true
    allocatedMemory: 32768  # 32GB L2
    replicas: 3
    l2ChunkCacheHandoff: 345600s  # 4 days

# Results cache (reduce query execution)
resultsCache:
  enabled: true
  allocatedMemory: 4096
  replicas: 3
  defaultValidity: 24h  # Cache results for 1 day
```

**Low-Traffic / Cost-Optimized**:
```yaml
chunksCache:
  enabled: true
  allocatedMemory: 512
  replicas: 1
  l2:
    enabled: false

resultsCache:
  enabled: false  # Disable if queries not repeated
```

### Practice 7: Network Policies

**Production Security**:
```yaml
networkPolicy:
  enabled: true
  flavor: kubernetes  # or cilium
  
  # Allow ingress from ingress controller namespace
  ingress:
    namespaceSelector:
      matchLabels:
        name: ingress-nginx
  
  # Allow metrics from monitoring namespace
  metrics:
    namespaceSelector:
      matchLabels:
        name: monitoring
    cidrs: []  # Or specific CIDR ranges
  
  # Alertmanager in monitoring namespace
  alertmanager:
    port: 9093
    namespaceSelector:
      matchLabels:
        name: monitoring
  
  # External storage (S3, GCS, Azure)
  externalStorage:
    ports: [443]  # HTTPS
    cidrs:
      - 52.219.0.0/16  # AWS S3 us-east-1 example
  
  # For index-gateway discovery
  discovery:
    port: 9095
```

---

## Anti-Patterns & Validations

### Anti-Pattern 1: Filesystem with Multiple Replicas

**Invalid** (from `templates/validate.yaml`):
```yaml
deploymentMode: SingleBinary
loki:
  storage:
    type: filesystem
singleBinary:
  replicas: 3  # ❌ FAIL
```

**Error**: "Cannot run more than 1 Single Binary replica without an object storage backend."

**Correct**:
```yaml
singleBinary:
  replicas: 1  # With filesystem
```

**OR**:
```yaml
loki:
  storage:
    type: s3  # Use object storage
singleBinary:
  replicas: 3  # Now OK
```

---

### Anti-Pattern 2: Mixed Deployment Modes

**Invalid**:
```yaml
deploymentMode: SimpleScalable
singleBinary:
  replicas: 1
write:
  replicas: 3  # ❌ FAIL
```

**Error**: "You have more than zero replicas configured for both..."

**Correct** (transitional):
```yaml
deploymentMode: SingleBinary<->SimpleScalable
singleBinary:
  replicas: 1
write:
  replicas: 3
```

---

### Anti-Pattern 3: Missing Schema Config

**Invalid**:
```yaml
loki:
  schemaConfig: {}
  useTestSchema: false  # ❌ FAIL
```

**Error**: "You must provide a schema_config..."

**Correct**:
```yaml
loki:
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
```

**OR** (testing only):
```yaml
loki:
  useTestSchema: true
```

---

### Anti-Pattern 4: Object Storage Without Bucket Names

**Invalid**:
```yaml
loki:
  storage:
    type: s3
    # Missing bucketNames ❌ FAIL
minio:
  enabled: false
```

**Error**: "Please define loki.storage.bucketNames.chunks"

**Correct**:
```yaml
loki:
  storage:
    type: s3
    bucketNames:
      chunks: my-chunks
      ruler: my-ruler
      admin: my-admin  # If enterprise
```

---

### Anti-Pattern 5: Enterprise Provisioner Without Admin Token

**Invalid** (6.36.0+):
```yaml
enterprise:
  enabled: true
  provisioner:
    enabled: true
  adminToken:
    secret: null  # ❌ FAIL
```

**Error**: "enterprise.adminToken.secret must be set when enterprise.provisioner.enabled is true"

**Correct**:
```bash
kubectl create secret generic loki-admin-token --from-literal=token=... -n loki
```
```yaml
enterprise:
  adminToken:
    secret: loki-admin-token
```

---

### Anti-Pattern 6: Compactor with Multiple Replicas

**Discouraged**:
```yaml
compactor:
  replicas: 3  # ⚠️ Not recommended
```

**Loki Behavior**: Only one compactor should run at a time  
**Alert**: `LokiTooManyCompactorsRunning`

**Correct**:
```yaml
compactor:
  replicas: 1
```

**Note**: Loki prevents multiple compactors from running via ring, but wasted resources

---

## Testing Configurations

### CI Test Configuration

**File**: `ci/non-default-values.yaml`  
**Purpose**: Test non-default features

```yaml
deploymentMode: Distributed

loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
  
  # Test multi-tenancy
  tenants:
    - name: "test-user-1"
      password: "test-password-1"
    - name: "test-user-2"
      passwordHash: "$2y$10$7O40CaY1yz7fu9O24k2/u.ct/wELYHRBsn25v/7AyuQ8E8hrLqpva"
  
  # Test DNS config
  dnsConfig:
    options:
      - name: ndots
        value: "3"

# Test zone-aware (but disabled for single-node Kind)
ingester:
  replicas: 0
  zoneAwareReplication:
    enabled: true
  dnsConfig:
    options:
      - name: ndots
        value: "4"

# Test distributed components
distributor:
  replicas: 1
querier:
  replicas: 1
queryFrontend:
  replicas: 1
queryScheduler:
  replicas: 1
compactor:
  replicas: 1
indexGateway:
  replicas: 1
ruler:
  replicas: 1

# Test memcached
memcached:
  enabled: true
  replicas: 1

chunksCache:
  enabled: true
  allocatedMemory: 128
  suffix: "l1"
  persistence:
    labels:
      foo: bar
  l2:
    enabled: true
    replicas: 2
    allocatedMemory: 128
    persistence:
      labels:
        baz: qux

resultsCache:
  allocatedMemory: 128

# Test gateway with auth
gateway:
  enabled: true
  basicAuth:
    enabled: true

# Test canary as Deployment
lokiCanary:
  kind: Deployment
```

---

### Enterprise Test Configuration

**File**: `ci/enterprise.yaml`  
**Purpose**: Test GEL features in CI

```yaml
enterprise:
  enabled: true
  config: |
    auth:
      type: trust
    auth_enabled: false
    cluster_name: {{ .Release.Name }}
    license:
      path: /etc/loki/license/license.jwt

loki:
  commonConfig:
    replication_factor: 1
  image:
    tag: "main-5e53303"  # Specific test image

storage:
  type: local  # Filesystem for CI

read:
  replicas: 1
write:
  replicas: 1
  persistence:
    enabled: true
    size: 100Mi
backend:
  replicas: 1
  persistence:
    enabled: true
    size: 100Mi

monitoring:
  serviceMonitor:
    labels:
      release: "prometheus"

test:
  prometheusAddress: "http://prometheus-kube-prometheus-prometheus.prometheus.svc.cluster.local.:9090"
```

---

## Common Configuration Combinations

### Combination 1: High Availability + Monitoring

```yaml
deploymentMode: SimpleScalable

# HA configuration
read:
  replicas: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10

write:
  replicas: 3
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10

backend:
  replicas: 3

# Gateway for routing
gateway:
  enabled: true
  replicas: 2
  autoscaling:
    enabled: true

# Monitoring
monitoring:
  dashboards:
    enabled: true
    namespace: monitoring
    labels:
      grafana_dashboard: "1"
  
  rules:
    enabled: true
    alerting: true
    namespace: monitoring
  
  serviceMonitor:
    enabled: true
    labels:
      release: prometheus
  
  selfMonitoring:
    enabled: true
    grafanaAgent:
      installOperator: true

lokiCanary:
  enabled: true
  kind: DaemonSet
```

### Combination 2: Multi-Tenant with Auth

```yaml
loki:
  auth_enabled: true
  tenants:
    - name: team-a
      password: secret-a
    - name: team-b
      password: secret-b
    - name: shared
      password: secret-shared
  
  # Per-tenant limits via runtime config
  runtimeConfig:
    overrides:
      team-a:
        ingestion_rate_mb: 50
        max_streams_per_user: 100000
      team-b:
        ingestion_rate_mb: 20
        max_streams_per_user: 50000
      shared:
        ingestion_rate_mb: 100

gateway:
  enabled: true
  basicAuth:
    enabled: true
    # htpasswd auto-generated from tenants
```

### Combination 3: Cost-Optimized with Retention

```yaml
loki:
  limits_config:
    retention_period: 168h  # 7 days (short)
  
  compactor:
    retention_enabled: true
    retention_delete_delay: 1h  # Quick deletion

compactor:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi

# Smaller caches
chunksCache:
  enabled: true
  allocatedMemory: 1024
  replicas: 1

resultsCache:
  enabled: false  # Disable to save resources

# Smaller replicas
read:
  replicas: 2
write:
  replicas: 2
backend:
  replicas: 2

# Small persistence
write:
  persistence:
    size: 10Gi
    enableStatefulSetAutoDeletePVC: true
```

---

## Configuration Validation Checklist

Before deploying, validate:

- [ ] Deployment mode matches component replicas
- [ ] Object storage configured if using SimpleScalable/Distributed
- [ ] Schema config provided (or useTestSchema for testing)
- [ ] Bucket names configured for object storage
- [ ] Retention period ≤ max_query_length
- [ ] Compactor replicas = 1
- [ ] PDB maxUnavailable set when replicas > 1
- [ ] Admin token secret exists (if using provisioner)
- [ ] Resource requests < limits
- [ ] GOMEMLIMIT set to ~90% of memory limits
- [ ] Zone node selectors match cluster zones (if zone-aware)
- [ ] Network policies allow required traffic
- [ ] Service account annotations correct (IRSA, Workload Identity)

**Validation Commands**:
```bash
# Template validation
helm template loki grafana/loki -f values.yaml --debug

# Dry-run validation
helm upgrade --install loki grafana/loki -f values.yaml --dry-run --debug

# Config validation
helm template loki grafana/loki -f values.yaml | kubectl apply --dry-run=client -f -
```

---

## Example Values File Locations

### Official Examples

**Single Binary**:
- `single-binary-values.yaml` - Production single binary
- `ci/default-single-binary-values.yaml` - CI test config
- `scenarios/default-single-binary-values.yaml` - Scenario test

**Simple Scalable**:
- `simple-scalable-values.yaml` - Production scalable
- `ci/default-values.yaml` - CI test config
- `scenarios/default-values.yaml` - Scenario test
- `scenarios/simple-scalable-aws-kube-irsa-values.yaml` - AWS with IRSA

**Distributed**:
- `distributed-values.yaml` - Production distributed
- `scenarios/default-distributed-values.yaml` - Scenario test
- `ci/distributed-disabled.yaml` - All distributed components

**Enterprise**:
- `docs/examples/enterprise/overrides-enterprise-gcs.yaml` - GEL with GCS
- `ci/enterprise.yaml` - Enterprise CI test

**Special Scenarios**:
- `ci/ingress-values.yaml` - Ingress instead of gateway
- `ci/legacy-monitoring-values.yaml` - Self-monitoring setup
- `scenarios/ingress-values.yaml` - Ingress scenario
- `scenarios/legacy-monitoring-values.yaml` - Monitoring scenario
- `scenarios/simple-thanos-values.yaml` - Thanos object store

### Example README Files

- `docs/examples/README.md` - Examples overview
- `docs/examples/oss/README.md` - OSS deployment guide
- `docs/examples/enterprise/README.md` - Enterprise deployment guide
- `scenarios/README.md` - CI scenario testing guide

---

## Quick Start Examples

### Minimal SingleBinary

```yaml
deploymentMode: SingleBinary

loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true

singleBinary:
  replicas: 1

minio:
  enabled: true

# Zero all other components
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0
```

```bash
helm install loki grafana/loki -f minimal-single.yaml
```

### Minimal SimpleScalable

```yaml
deploymentMode: SimpleScalable

loki:
  commonConfig:
    replication_factor: 1
  useTestSchema: true
  storage:
    bucketNames:
      chunks: chunks
      ruler: ruler
      admin: admin

read:
  replicas: 1
write:
  replicas: 1
backend:
  replicas: 1

minio:
  enabled: true

singleBinary:
  replicas: 0
```

```bash
helm install loki grafana/loki -f minimal-scalable.yaml
```

### Production-Ready SimpleScalable

Based on `simple-scalable-values.yaml` with additions:

```yaml
deploymentMode: SimpleScalable

loki:
  auth_enabled: true
  
  commonConfig:
    replication_factor: 3
  
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  
  storage:
    type: s3
    bucketNames:
      chunks: prod-loki-chunks
      ruler: prod-loki-ruler
    s3:
      region: us-east-1
  
  limits_config:
    retention_period: 2160h  # 90 days
    max_query_length: 2160h
    ingestion_rate_mb: 10
    max_streams_per_user: 100000
  
  ingester:
    chunk_encoding: snappy
  
  compactor:
    retention_enabled: true

# Service account with IRSA
serviceAccount:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/loki

# Write path
write:
  replicas: 3
  persistence:
    size: 50Gi
    storageClass: gp3
  resources:
    requests: {cpu: 2, memory: 4Gi}
    limits: {cpu: 4, memory: 8Gi}
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10

# Read path
read:
  replicas: 3
  legacyReadTarget: false
  resources:
    requests: {cpu: 2, memory: 4Gi}
    limits: {cpu: 4, memory: 8Gi}
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10

# Backend
backend:
  replicas: 3
  persistence:
    size: 20Gi
    storageClass: gp3
  resources:
    requests: {cpu: 1, memory: 2Gi}
    limits: {cpu: 2, memory: 4Gi}

# Gateway
gateway:
  enabled: true
  replicas: 2
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - host: loki.example.com
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: loki-tls
        hosts: [loki.example.com]

# Caching
chunksCache:
  enabled: true
  allocatedMemory: 8192
  replicas: 3

resultsCache:
  enabled: true
  allocatedMemory: 2048
  replicas: 2

# Monitoring
monitoring:
  serviceMonitor:
    enabled: true
    labels:
      release: prometheus
  rules:
    enabled: true
    alerting: true
  dashboards:
    enabled: true
    namespace: monitoring

lokiCanary:
  enabled: true

# Disable MinIO (use real S3)
minio:
  enabled: false

# Zero other modes
singleBinary:
  replicas: 0
```

---

## Makefile Usage Examples

**File**: `production/helm/loki/Makefile`

### Install Distributed Mode

```bash
cd production/helm/loki
make install-distributed

# With custom image
make install-distributed IMAGE=grafana/loki:custom-tag

# With extra args
make install-distributed ARGS="--set loki.auth_enabled=true"
```

### Install Single Binary

```bash
make install-single-binary

# With args
make install-single-binary ARGS="--set singleBinary.replicas=3"
```

### Update Existing Installation

```bash
# Auto-detects deployment mode
make update

# With custom image
make update IMAGE=grafana/loki:3.5.3
```

### Uninstall

```bash
make uninstall
# Uninstalls release and deletes namespace
```

### Update Dependencies

```bash
make update-chart
# Runs: helm dependency update .
```

### Lint

```bash
make lint
# Runs yamllint on src/
```

---

## Next Steps

- For template modification patterns, see **[Modification Guide](./loki-helm-modification-guide.mdc)**
- For architecture details, see **[Architecture & Components](./loki-helm-architecture.mdc)**
- For upgrade procedures, see **[Changelog Insights](./loki-helm-changelog.mdc)**
