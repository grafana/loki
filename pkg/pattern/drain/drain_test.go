package drain

import (
	"bufio"
	"fmt"
	"os"
	"testing"

	"github.com/stretchr/testify/require"
	"golang.org/x/exp/slices"

	"github.com/grafana/loki/v3/pkg/logql/log/pattern"
)

func TestDrain_TrainExtractsPatterns(t *testing.T) {
	t.Parallel()

	// Set this so the test will print the patterns found, in string slice format for easy copy-paste
	outputPatternsForTestUpdate := false

	tests := []struct {
		drain     *Drain
		inputFile string
		patterns  []string
	}{
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: `testdata/agent-logfmt.txt`,
			patterns: []string{
				`ts=2024-04-16T15:10:42.<_> level=info msg="finished node evaluation" controller_id=module.http.cloudwatch_pipelines node_id=prometheus.scrape.<_> duration=<_>.<_>`,
				`ts=2024-04-16T15:10:43.192290389Z caller=filetargetmanager.go:361 level=info component=logs logs_config=default msg="Adding target" key="/var/log/pods/*19a1cce8-5f04-46e0-a124-292b0dd9b343/testcoordinator/*.log:{batch_kubernetes_io_controller_uid=\"25ec5edf-f78e-468b-b6f3-3b9685f0cc8f\", batch_kubernetes_io_job_name=\"testcoordinator-job-2665838\", container=\"testcoordinator\", controller_uid=\"25ec5edf-f78e-468b-b6f3-3b9685f0cc8f\", job=\"k6-cloud/testcoordinator\", job_name=\"testcoordinator-job-2665838\", name=\"testcoordinator\", namespace=\"k6-cloud\", pod=\"testcoordinator-job-2665838-9g8ds\"}"`,
				`ts=2024-04-16T15:10:43.551782223Z caller=tailer.go:245 level=info component=logs logs_config=default component=tailer msg="stopped tailing file" path=/var/log/pods/grafana-com_marketplaces-api-f67ff7567-gqrvb_35649bfd-52ff-4281-9294-5f65fd5a89fc/marketplaces-api/0.log`,
				`ts=2024-04-16T15:10:43.<_> caller=filetargetmanager.go:<_> level=info component=logs logs_config=default msg="<_> target" key="/var/log/pods/*<_>/<_>/*.log:{<_>=\"<_>\", <_>=\"<_> <_>\", namespace=\"<_>\", pod=\"<_>\", <_>=\"<_>\"}"`,
				`ts=2024-04-16T15:10:43.<_> caller=tailer.go:<_> level=info component=logs logs_config=default component=tailer msg="<_> <_> <_> <_> <_> <_> <_> <_> <_>`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetarget.go:192 level=info component=logs logs_config=default msg="filetarget: watcher closed, tailer stopped, positions saved" path=/var/log/pods/*<_>/<_>/*.log`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetarget.go:313 level=info component=logs logs_config=default msg="watching new directory" directory=/var/log/pods/<_>/<_>`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetarget.go:313 level=info component=logs logs_config=default msg="watching new directory" directory=/var/log/pods/hosted-grafana_.<_>/<_>`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetarget.go:326 level=info component=logs logs_config=default msg="removing directory from watcher" directory=/var/log/pods/hosted-grafana_.<_>/<_>`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetargetmanager.go:181 level=info component=logs logs_config=default msg="received file watcher event" name=/var/log/pods/<_>/<_>.<_> op=CREATE`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetargetmanager.go:181 level=info component=logs logs_config=default msg="received file watcher event" name=/var/log/pods/<_>/<_>.<_>.<_> op=CREATE`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetargetmanager.go:181 level=info component=logs logs_config=default msg="received file watcher event" name=/var/log/pods/<_>/<_>/<_>.log op=CREATE`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetargetmanager.go:181 level=info component=logs logs_config=default msg="received file watcher event" name=/var/log/pods/hosted-grafana_.<_>/<_>/0.log.<_>.<_> op=CREATE`,
				`ts=2024-04-16T15:10:<_>.<_> caller=filetargetmanager.go:<_> level=info component=logs logs_config=default msg="<_> target" key="/var/log/pods/*<_>/<_>/*.log:{app=\"grafana\", conprof=\"true\", container=\"<_>\", instanceId=\"<_>\", job=\"hosted-grafana/grafana\", name=\"grafana\", namespace=\"hosted-grafana\", org=\"<_>\", plan=\"free\", pod=\"<_>\", pod_template_hash=\"<_>\", resource_version=\"<_>\", slug=\"<_>\", stackId=\"<_>\"}"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Re-opening moved/deleted file /var/log/pods/<_>/<_>/<_>.log ..."`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Re-opening moved/deleted file /var/log/pods/hosted-grafana_.<_>/<_>/0.log ..."`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Seeked /var/log/pods/<_>/<_>/0.log - &{Offset:0 Whence:0}"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Seeked /var/log/pods/hosted-grafana_.<_>/<_>/0.log - &{Offset:0 Whence:0}"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Successfully reopened /var/log/pods/<_>/<_>/<_>.log"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Successfully reopened /var/log/pods/hosted-grafana_.<_>/<_>/0.log"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Waiting for /var/log/pods/<_>/<_>/0.log to appear..."`,
				`ts=2024-04-16T15:10:<_>.<_> caller=log.go:168 component=logs logs_config=default level=info msg="Waiting for /var/log/pods/hosted-grafana_.<_>/<_>/0.log to appear..."`,
				`ts=2024-04-16T15:10:<_>.<_> caller=logfmt.go:139 level=error component=logs logs_config=default component=file_pipeline component=stage type=logfmt msg="failed to decode logfmt" err="bufio.Scanner: token too long"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=logfmt.go:139 level=error component=logs logs_config=default component=file_pipeline component=stage type=logfmt msg="failed to decode logfmt" err="logfmt syntax error at pos <_> on line 1: unexpected '\"'"`,
				`ts=2024-04-16T15:10:<_>.<_> caller=tailer.go:245 level=info component=logs logs_config=default component=tailer msg="stopped tailing file" path=/var/log/pods/hosted-grafana_.<_>/<_>/0.log`,
				`ts=2024-04-16T15:10:<_>.<_> caller=tailer.go:<_> level=info component=logs logs_config=default component=tailer msg="<_> <_> <_> <_> <_> <_> <_> <_> <_>`,
				`ts=2024-04-16T15:10:<_>.<_> caller=tailer.go:<_> level=info component=logs logs_config=default component=tailer msg="<_> <_>: <_>" path=/var/log/pods/<_>/<_>/0.log`,
				`ts=2024-04-16T15:10:<_>.<_> caller=tailer.go:<_> level=info component=logs logs_config=default component=tailer msg="<_> <_>: <_>" path=/var/log/pods/hosted-grafana_.<_>/<_>/0.log`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: `testdata/ingester-logfmt.txt`,
			patterns: []string{
				`ts=2024-04-17T09:52:46.363974185Z caller=http.go:194 level=debug traceID=1b48f5156a61ca69 msg="GET /debug/pprof/delta_mutex (200) 1.161082ms"`,
				`ts=2024-04-17T09:52:46.<_> caller=head.go:216 level=debug tenant=987678 msg="profile is empty after delta computation" metricName=memory`,
				`ts=2024-04-17T09:52:46.<_> caller=http.go:194 level=debug traceID=<_> orgID=<_> msg="POST /ingester.v1.IngesterService/Push (200) <_>.<_>"`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: `testdata/drone-json.txt`,
			patterns: []string{
				`{"duration":<_>,"level":"debug","method":"GET","msg":"request completed","referer":"","remote":"10.136.105.40:52702","request":"/metrics","status":200,"time":"<_>:<_>:<_>","user-agent":"GrafanaAgent/v0.40.3 (flow; linux; helm)"}`,
				`{"id":"<_>","level":"debug","max-pool":4,"min-pool":0,"msg":"check capacity","pending-builds":0,"running-builds":0,"server-buffer":0,"server-capacity":0,"server-count":0,"time":"<_>:<_>:<_>"}`,
				`{"id":"<_>","level":"debug","msg":"calculate server capacity","time":"<_>:<_>:<_>"}`,
				`{"id":"<_>","level":"debug","msg":"calculate unfinished jobs","time":"<_>:<_>:<_>"}`,
				`{"id":"<_>","level":"debug","msg":"check capacity complete","time":"<_>:<_>:<_>"}`,
				`{"id":"<_>","level":"debug","msg":"no capacity changes required","time":"<_>:<_>:<_>"}`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/distributor-logfmt.txt",
			patterns: []string{
				`ts=2024-05-02T12:17:22.851228301Z caller=http.go:194 level=debug traceID=1e1fe5ba1756bc38 orgID=1819 msg="POST /pyroscope/ingest?aggregationType=sum&from=1714652230&name=flamegraph.com%7Bapp_kubernetes_io_instance%3Dflamegraph-com%2Capp_kubernetes_io_name%3Dflamegraph-com%2Ccluster%3Dflamegraph.com%2Cinstance%3D10.0.11.146%3A8001%2Cjob%3Dkubernetes-pods%2Cnamespace%3Dflamegraph-com%2Cpod%3Dflamegraph-com-backend-79c858c7bf-jw2hn%2Cpod_template_hash%3D79c858c7bf%2Cpyroscope_tenant%3Dpyroscope%2Ctier%3Dbackend%7D&sampleRate=0&spyName=scrape&units=samples&until=1714652240 (200) 22.345191ms"`,
				`ts=2024-05-02T12:17:22.<_> caller=http.go:194 level=debug traceID=<_> orgID=75 msg="POST /ingest?aggregationType=&from=1714652227232613927&name=checkoutservice%7B__session_id__%3D294b9729f5a7de95%2Cnamespace%3Dotel-demo%7D&sampleRate=<_>&spyName=gospy&units=&until=1714652242232506798 (200) <_>.<_>"`,
				`ts=2024-05-02T12:17:22.<_> caller=http.go:194 level=debug traceID=<_> orgID=75 msg="POST /ingest?aggregationType=<_>&from=<_>&name=checkoutservice%7B__session_id__%3D294b9729f5a7de95%2Cnamespace%3Dotel-demo%7D&sampleRate=<_>&spyName=gospy&units=<_>&until=<_> (200) <_>.<_>"`,
				`ts=2024-05-02T12:17:<_>.<_> caller=http.go:194 level=debug traceID=<_> orgID=1819 msg="POST /pyroscope/ingest?aggregationType=sum&from=1714652230&name=flamegraph.com.frontend%7Bapp_kubernetes_io_instance%3Dflamegraph-com%2Capp_kubernetes_io_name%3Dflamegraph-com%2Ccluster%3Dflamegraph.com%2Cinstance%3D10.0.9.115%3A9091%2Cjob%3Dkubernetes-pods%2Cnamespace%3Dflamegraph-com%2Cpod%3Dflamegraph-com-frontend-6fb87f8785-pd87k%2Cpod_template_hash%3D6fb87f8785%2Cpyroscope_tenant%3Dpyroscope%2Ctier%3Dfrontend%7D&sampleRate=0&spyName=scrape&units=samples&until=1714652240 (200) <_>.<_>"`,
				`ts=2024-05-02T12:17:<_>.<_> caller=http.go:194 level=debug traceID=<_> orgID=<_> msg="POST /push.v1.PusherService/Push (<_>) <_>.<_>"`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/journald.txt",
			patterns: []string{
				`						ln --force -s /proc/$(pidof hgrun-pause)/root/bin/hgrun /bin/hgrun;`,
				`						while [ "$(pidof plugins-pause)" = "" ]; do sleep 0.5; done;`,
				`	ts=2024-05-07T11:59:32.025687537Z level=error caller=http_client.go:56 app=hgrun hgrun_version=0.1.453-59-gf3f63162a msg="request`,
				`	ts=2024-05-07T11:59:<_>.<_> level=error caller=http_client.go:56 app=hgrun hgrun_version=0.1.<_> msg="request failed" error="Get \"http://127.0.0.1:3000/api/health\": dial tcp 127.0.0.1:3000: connect: connection refused" method=GET url=http://127.0.0.1:3000/api/health`,
				`2024-05-07T11:59:43.484606Z INFO ExtHandler ExtHandler Downloading agent manifest`,
				`2024-05-07T11:59:<_>.<_> INFO TelemetryEventsCollector ExtHandler Collected 2 events for extension: Microsoft.Azure.Extensions.CustomScript`,
				`<_>.scope: Consumed <_>.<_> CPU time.`,
				`<_>.scope: Deactivated successfully.`,
				`AVC apparmor="DENIED" operation="ptrace" profile="cri-containerd.apparmor.d" pid=<_> comm="pidof" requested_mask="read" denied_mask="read" peer="unconfined"`,
				`E0507 11:59:31.928148    4734 pod_workers.go:1300] "Error syncing pod, skipping" err="unmounted volumes=[terraform-drift-detector-data], unattached volumes=[terraform-drift-detector-data], failed to process volumes=[]: context deadline exceeded" pod="terraform-drift-detector/terraform-drift-detector-d68b4c545-jg2vj" podUID="6c607496-ef26-454e-b2f2-4cb75b233fa3"`,
				`E0507 11:59:34.923938    3027 kuberuntime_manager.go:1261] container &Container{Name:mysqld-exporter,Image:prom/mysqld-exporter:v0.13.0,Command:[],Args:[--collect.info_schema.innodb_metrics],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http-metrics,HostPort:0,ContainerPort:9104,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_USER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:testcrossplane-user-exporter,},Key:username,Optional:nil,},},},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:testcrossplane-user-exporter,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_HOST,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:testcrossplane-user-exporter,},Key:endpoint,Optional:nil,},},},EnvVar{Name:MYSQL_PORT,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:testcrossplane-user-exporter,},Key:port,Optional:nil,},},},EnvVar{Name:MYSQL_TLS_MODE,Value:preferred,ValueFrom:nil,},EnvVar{Name:DATA_SOURCE_NAME,Value:$(MYSQL_USER):$(MYSQL_PASSWORD)@tcp($(MYSQL_HOST):$(MYSQL_PORT))/?tls=$(MYSQL_TLS_MODE),ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzx7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod testcrossplane-exporter-c67cfc58f-vbzl4_crossplane-playground(3d49134d-3378-4ec3-824c-5ff4ea2590a5): CreateContainerConfigError: secret "testcrossplane-user-exporter" not found`,
				`E0507 11:59:35.928465    4734 pod_workers.go:1300] "Error syncing pod, skipping" err="unmounted volumes=[custom-grafana-agent], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="loki-dev-010/custom-grafana-agent-856948968f-6jfks" podUID="17b244cc-ecb9-4fbc-beaa-8fa47fafe013"`,
				`E0507 11:59:<_>.<_>    <_> kuberuntime_manager.go:1256] container &Container{Name:grafana,Image:us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>,Command:[/bin/sh],Args:[-c set -e; while [ "$(pidof hgrun-pause)" = "" ]; do sleep 0.5; done;`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with CrashLoopBackOff: \"back-off <_> restarting failed container=<_> pod=<_>(<_>)\"" pod="<_>/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with CreateContainerConfigError: \"secret \\\"<_>\\\" not found\"" pod="<_>/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with ImagePullBackOff: \"Back-off pulling image \\\"us.gcr.io/hosted-grafana/<_>:<_>.<_>.<_>\\\"\"" pod="<_>/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with ImagePullBackOff: \"Back-off pulling image \\\"us.gcr.io/kubernetes-dev/<_>:<_>\\\"\"" pod="<_>/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"grafana\" with ErrImagePull: \"[rpc error: code = NotFound desc = failed to pull and unpack image \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\\\": failed to resolve reference \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\\\": us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>: not found, failed to pull and unpack image \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\\\": failed to resolve reference \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\\\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/<_>.1.<_>: 403 Forbidden]\"" pod="hosted-grafana/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pdc\" with ErrImageNeverPull: \"Container image \\\"us.gcr.io/hosted-grafana/pdc:0.1.415\\\" is not present with pull policy of Never\"" pod="pdc/<_>" podUID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> prober.go:104] "Probe errored" err="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task <_> not found: not found" probeType="Readiness" pod="hosted-grafana/<_>" podUID="<_>" containerName="grafana"`,
				`E0507 11:59:<_>.<_>    <_> prober.go:239] "Unable to write all bytes from execInContainer" err="short write" expectedBytes=<_> actualBytes=10240`,
				`E0507 11:59:<_>.<_>    <_> remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>: not found" image="us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>"`,
				`E0507 11:59:<_>.<_>    <_> remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/<_>.1.<_>: 403 Forbidden" image="us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>"`,
				`E0507 11:59:<_>.<_>    <_> remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found" containerID="<_>"`,
				`E0507 11:59:<_>.<_>    <_> remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task <_> not found: not found" containerID="<_>" cmd=["/bin/hgrun","check"]`,
				`I0507 11:59:31.815514    2791 azure_credentials.go:220] image(us.gcr.io/hosted-grafana/hosted-grafana-pro) is not from ACR, return empty authentication`,
				`I0507 11:59:34.518822    3224 kuberuntime_container.go:745] "Killing container with a grace period" pod="hosted-grafana/hosted-grafana-api-7b6bd9b949-9csb4" podUID="25cb986c-3d6c-4ed0-abf3-ee59ed6175f9" containerName="hgapi" containerID="containerd://c91436db00920ec961b9d5d6b4859d80a912e862e34fb5c45d8a85684fe6a97e" gracePeriod=30`,
				`I0507 11:59:34.834734    3224 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-95j2t\" (UniqueName: \"kubernetes.io/projected/25cb986c-3d6c-4ed0-abf3-ee59ed6175f9-kube-api-access-95j2t\") pod \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\" (UID: \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\") "`,
				`I0507 11:59:34.834794    3224 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"pdc-certs\" (UniqueName: \"kubernetes.io/secret/25cb986c-3d6c-4ed0-abf3-ee59ed6175f9-pdc-certs\") pod \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\" (UID: \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\") "`,
				`I0507 11:59:34.834835    3224 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"gcs-serviceaccount\" (UniqueName: \"kubernetes.io/secret/25cb986c-3d6c-4ed0-abf3-ee59ed6175f9-gcs-serviceaccount\") pod \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\" (UID: \"25cb986c-3d6c-4ed0-abf3-ee59ed6175f9\") "`,
				`I0507 11:59:34.841404    3224 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/25cb986c-3d6c-4ed0-abf3-ee59ed6175f9-kube-api-access-95j2t" (OuterVolumeSpecName: "kube-api-access-95j2t") pod "25cb986c-3d6c-4ed0-abf3-ee59ed6175f9" (UID: "25cb986c-3d6c-4ed0-abf3-ee59ed6175f9"). InnerVolumeSpecName "kube-api-access-95j2t". PluginName "kubernetes.io/projected", VolumeGidValue ""`,
				`I0507 11:59:34.936025    3224 reconciler_common.go:300] "Volume detached for volume \"pdc-certs\" (UniqueName: \"kubernetes.io/secret/25cb986c-3d6c-4ed0-abf3-ee59ed6175f9-pdc-certs\") on node \"ip-10-60-2-58.us-east-2.compute.internal\" DevicePath \"\""`,
				`I0507 11:59:34.<_>    3224 operation_generator.go:888] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/<_>" (OuterVolumeSpecName: "<_>") pod "25cb986c-3d6c-4ed0-abf3-ee59ed6175f9" (UID: "25cb986c-3d6c-4ed0-abf3-ee59ed6175f9"). InnerVolumeSpecName "<_>". PluginName "kubernetes.io/secret", VolumeGidValue ""`,
				`I0507 11:59:34.<_>    3224 reconciler_common.go:300] "Volume detached for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") on node \"ip-10-60-2-58.us-east-2.compute.internal\" DevicePath \"\""`,
				`I0507 11:59:37.<_>    <_> prober.go:107] "Probe failed" probeType="Readiness" pod="<_>/<_>" podUID="<_>" containerName="<_>" probeResult="failure" output="HTTP probe failed with statuscode: <_>"`,
				`I0507 11:59:38.116658    2791 azure_credentials.go:220] image(us.gcr.io/hosted-grafana/hg-plugins) is not from ACR, return empty authentication`,
				`I0507 11:59:39.168633    2776 kubelet.go:2493] "SyncLoop (probe)" probe="readiness" status="" pod="hosted-grafana/dafdeveuwest2-grafana-7845d969b5-f8h5q"`,
				`I0507 11:59:<_>.<_>    2791 azure_credentials.go:220] image(us.gcr.io/hosted-grafana/hgrun) is not from ACR, return empty authentication`,
				`I0507 11:59:<_>.<_>    6247 prober.go:107] "Probe failed" probeType="Readiness" pod="grafana-agent/grafana-agent-helm-4" podUID="c36c5200-1cd6-4093-893c-c022f91af996" containerName="grafana-agent" probeResult="failure" output="Get \"http://10.0.99.125:3090/-/ready\": dial tcp 10.0.99.125:3090: connect: connection refused"`,
				`I0507 11:59:<_>.<_>    <_> generic.go:334] "Generic (PLEG): container finished" podID="<_>" containerID="<_>" exitCode=1`,
				`I0507 11:59:<_>.<_>    <_> kubelet.go:2498] "SyncLoop (probe)" probe="liveness" status="unhealthy" pod="hosted-grafana/<_>"`,
				`I0507 11:59:<_>.<_>    <_> kubelet.go:2498] "SyncLoop (probe)" probe="readiness" status="ready" pod="hosted-grafana/<_>"`,
				`I0507 11:59:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (PLEG): event for pod" pod="<_>/<_>" event={"ID":"<_>","Type":"<_>","Data":"<_>"}`,
				`I0507 11:59:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop DELETE" source="api" pods=["hosted-grafana/<_>"]`,
				`I0507 11:59:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop REMOVE" source="api" pods=["hosted-grafana/<_>"]`,
				`I0507 11:59:<_>.<_>    <_> kubelet_getters.go:187] "Pod status updated" pod="kube-system/<_>" status="Running"`,
				`I0507 11:59:<_>.<_>    <_> kubelet_pods.go:906] "Unable to retrieve pull secret, the image pull may not succeed." pod="<_>/<_>" secret="" err="secret \"<_>\" not found"`,
				`I0507 11:59:<_>.<_>    <_> kubelet_volumes.go:<_>] "Cleaned up orphaned pod volumes dir" podUID="<_>" path="/var/lib/kubelet/pods/<_>/volumes"`,
				`I0507 11:59:<_>.<_>    <_> pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"<_>"} err="failed to get container status \"<_>\": rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found"`,
				`I0507 11:59:<_>.<_>    <_> prober.go:107] "Probe failed" probeType="Readiness" pod="hosted-grafana/<_>" podUID="<_>" containerName="grafana" probeResult="failure" output=<`,
				`I0507 11:59:<_>.<_>    <_> scope.go:117] "RemoveContainer" containerID="<_>"`,
				`I0507 11:59:<_>.<_>  <_> cache.go:40] re-using cached key and certificate`,
				`IPv4: martian source 10.132.<_>.<_> from 10.132.<_>.<_>, on dev eth0`,
				`PRC: Renewing lease on eth0.`,
				`RCV: Reply message on eth0 from fe80::e9:7eff:fedf:3d37.`,
				`Removed slice libcontainer container kubepods-burstable-pod25cb986c_3d6c_4ed0_abf3_ee59ed6175f9.slice.`,
				`Started cri-containerd-95bf586cd79d43120ff44582d4dbd2476de61744411f8515b9b2c527a41fd5d9.scope.`,
				`Started libcontainer container <_>.`,
				`XMT: Renew on eth0, interval 9700ms.`,
				`XMT: Solicit on eth0, interval <_>.`,
				`audit: type=1400 audit(<_>.<_>:<_>): apparmor="DENIED" operation="ptrace" profile="cri-containerd.apparmor.d" pid=<_> comm="pidof" requested_mask="read" denied_mask="read" peer="unconfined"`,
				`kauditd_printk_skb: <_> callbacks suppressed`,
				`ll header: 00000000: 42 01 0a 80 00 <_> 42 01 0a 80 00 01 08 00`,
				`net_ratelimit: 2 callbacks suppressed`,
				`run-containerd-io.containerd.runtime.v2.task-k8s.<_>.mount: Deactivated successfully.`,
				`run-containerd-runc-k8s.io-e5f17d69eee483ec8d43b26d5d628246984ba92f794ee5f3748935f5b6448b9b-runc.6eAyHn.mount: Deactivated successfully.`,
				`time="2024-05-07T11:59:34.519591759Z" level=info msg="StopContainer for \"c91436db00920ec961b9d5d6b4859d80a912e862e34fb5c45d8a85684fe6a97e\" with timeout 30 (s)"`,
				`time="2024-05-07T11:59:34.520032214Z" level=info msg="Stop container \"c91436db00920ec961b9d5d6b4859d80a912e862e34fb5c45d8a85684fe6a97e\" with signal terminated"`,
				`time="2024-05-07T11:59:34.591282703Z" level=info msg="StopContainer for \"c91436db00920ec961b9d5d6b4859d80a912e862e34fb5c45d8a85684fe6a97e\" returns successfully"`,
				`time="2024-05-07T11:59:34.592005066Z" level=info msg="StopPodSandbox for \"c605ad2cdc74c6b5288f2532ad71cce81a28ef6965f97a89ff6609deb825553a\""`,
				`time="2024-05-07T11:59:34.592084495Z" level=info msg="Container to stop \"c91436db00920ec961b9d5d6b4859d80a912e862e34fb5c45d8a85684fe6a97e\" must be in running or unknown state, current state \"CONTAINER_EXITED\""`,
				`time="2024-05-07T11:59:34.706960850Z" level=info msg="TearDown network for sandbox \"c605ad2cdc74c6b5288f2532ad71cce81a28ef6965f97a89ff6609deb825553a\" successfully"`,
				`time="2024-05-07T11:59:34.707025668Z" level=info msg="StopPodSandbox for \"c605ad2cdc74c6b5288f2532ad71cce81a28ef6965f97a89ff6609deb825553a\" returns successfully"`,
				`time="2024-05-07T11:59:38.117772842Z" level=info msg="PullImage \"us.gcr.io/hosted-grafana/hg-plugins:2024-05-07-v545244-f51851984\""`,
				`time="2024-05-07T11:59:38.484586527Z" level=error msg="Failed to delete exec process \"d9e0a1867ce73695ad859f2b0a76fe8f5053db8a5e49142d747e53a445729bd4\" for container \"6ad3e55547f2192f865518e75009243418b177091c1c781236e2ac8f0324b408\"" error="ttrpc: closed: unknown"`,
				`time="2024-05-07T11:59:<_>.<_>" level=error msg="ContainerStatus for \"<_>\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found"`,
				`time="2024-05-07T11:59:<_>.<_>" level=error msg="ExecSync for \"<_>\" failed" error="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task <_> not found: not found"`,
				`time="2024-05-07T11:59:<_>.<_>" level=error msg="PullImage \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\" failed" error="failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/<_>.1.<_>: 403 Forbidden"`,
				`time="2024-05-07T11:59:<_>.<_>" level=error msg="PullImage \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\" failed" error="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\": us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>: not found"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="CreateContainer within sandbox \"<_>\" for &ContainerMetadata{Name:<_>,Attempt:<_>,} returns container id \"<_>\""`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="CreateContainer within sandbox \"<_>\" for container &ContainerMetadata{Name:<_>,Attempt:<_>,}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageCreate event name:\"sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageCreate event name:\"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageCreate event name:\"us.gcr.io/hosted-grafana/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageUpdate event name:\"sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageUpdate event name:\"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="ImageUpdate event name:\"us.gcr.io/hosted-grafana/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="PullImage \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" returns image reference \"sha256:<_>\""`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="PullImage \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\""`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="Pulled image \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" with image id \"sha256:<_>\", repo tag \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\", repo digest \"us.gcr.io/hosted-grafana/<_>@sha256:<_>\", size \"<_>\" in <_>.<_>"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="RemoveContainer for \"<_>\" returns successfully"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="RemoveContainer for \"<_>\""`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="StartContainer for \"<_>\" returns successfully"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="StartContainer for \"<_>\""`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="cleaning up dead shim" namespace=k8s.io`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="shim disconnected" id=<_> namespace=k8s.io`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="stop pulling image us.gcr.io/hosted-grafana/<_>:<_>.1.<_>: active requests=0, bytes read=<_>"`,
				`time="2024-05-07T11:59:<_>.<_>" level=info msg="trying next host - response was http.StatusNotFound" host=us.gcr.io`,
				`time="2024-05-07T11:59:<_>.<_>" level=warning msg="cleaning up after shim disconnected" id=<_> namespace=k8s.io`,
				`var-lib-containerd-tmpmounts-containerd\<_>.mount: Deactivated successfully.`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/kafka.txt",
			patterns: []string{
				`[2024-05-07 10:55:40,626] INFO [LocalLog partition=ingest-6, dir=/bitnami/kafka/data] Deleting segment files LogSegment(baseOffset=180391157, size=16991045, lastModifiedTime=1715075754780, largestRecordTimestamp=Some(1715075754774)),LogSegment(baseOffset=180393429, size=16997692, lastModifiedTime=1715075760206, largestRecordTimestamp=Some(1715075760186)),LogSegment(baseOffset=180395889, size=16998200, lastModifiedTime=1715075765542, largestRecordTimestamp=Some(1715075765526)),LogSegment(baseOffset=180398373, size=16977347, lastModifiedTime=1715075770515, largestRecordTimestamp=Some(1715075770504)) (kafka.log.LocalLog$)`,
				`[2024-05-07 10:55:53,038] INFO [LocalLog partition=mimir-dev-09-aggregations-offsets-1, dir=/bitnami/kafka/data] Deleting segment files LogSegment(baseOffset=447957, size=948, lastModifiedTime=1715059232052, largestRecordTimestamp=Some(1715059232002)),LogSegment(baseOffset=447969, size=948, lastModifiedTime=1715059424352, largestRecordTimestamp=Some(1715059424301)) (kafka.log.LocalLog$)`,
				`[2024-05-07 10:55:53,<_>] INFO [LocalLog partition=mimir-dev-09-aggregations-offsets-0, dir=/bitnami/kafka/data] Deleting segment files LogSegment(baseOffset=<_>, size=948, lastModifiedTime=<_>, largestRecordTimestamp=Some(<_>)) (kafka.log.LocalLog$)`,
				`[2024-05-07 10:55:<_>,<_>] INFO Deleted log /bitnami/kafka/data/<_>/<_>.log.deleted. (kafka.log.LogSegment)`,
				`[2024-05-07 10:55:<_>,<_>] INFO Deleted offset index /bitnami/kafka/data/<_>/<_>.index.deleted. (kafka.log.LogSegment)`,
				`[2024-05-07 10:55:<_>,<_>] INFO Deleted producer state snapshot /bitnami/kafka/data/<_>/<_>.snapshot.deleted (kafka.log.SnapshotFile)`,
				`[2024-05-07 10:55:<_>,<_>] INFO Deleted time index /bitnami/kafka/data/<_>/<_>.timeindex.deleted. (kafka.log.LogSegment)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [LocalLog partition=<_>, dir=/bitnami/kafka/data] Rolled new log segment at offset <_> in <_> ms. (kafka.log.LocalLog)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [ProducerStateManager partition=<_>] Wrote producer snapshot at offset <_> with 0 producer ids in <_> ms. (kafka.log.ProducerStateManager)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [UnifiedLog partition=<_>, dir=/bitnami/kafka/data] Deleting segment LogSegment(baseOffset=<_>, size=<_>, lastModifiedTime=<_>, largestRecordTimestamp=Some(<_>)) due to retention size <_> breach. Log size after deletion will be <_>. (kafka.log.UnifiedLog)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [UnifiedLog partition=<_>, dir=/bitnami/kafka/data] Deleting segments due to log start offset <_> breach: LogSegment(baseOffset=<_>, size=948, lastModifiedTime=<_>, largestRecordTimestamp=Some(<_>)),LogSegment(baseOffset=<_>, size=948, lastModifiedTime=<_>, largestRecordTimestamp=Some(<_>)) (kafka.log.UnifiedLog)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [UnifiedLog partition=<_>, dir=/bitnami/kafka/data] Deleting segments due to log start offset <_> breach: LogSegment(baseOffset=<_>, size=<_>, lastModifiedTime=<_>, largestRecordTimestamp=Some(<_>)) (kafka.log.UnifiedLog)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [UnifiedLog partition=<_>, dir=/bitnami/kafka/data] Incremented log start offset to <_> due to leader offset increment (kafka.log.UnifiedLog)`,
				`[2024-05-07 10:55:<_>,<_>] INFO [UnifiedLog partition=<_>, dir=/bitnami/kafka/data] Incremented log start offset to <_> due to segment deletion (kafka.log.UnifiedLog)`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/kubernetes.txt",
			patterns: []string{
				`I0507 12:02:27.947830       1 nodeutilization.go:274] "Evicting pods based on priority, if they have same priority, they'll be evicted based on QoS tiers"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:163] "pod does not fit on any other node because of nodeSelector(s), Taint(s), or nodes marked as unschedulable" pod="<_>/<_>"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="<_>/<_>" checks="pod has local storage and descheduler is not configured with evictLocalStoragePods"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="ge-logs/<_>" checks="[pod is a DaemonSet pod, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="insight-logs/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="loki-dev-ssd/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="promtail-ops/<_>" checks="[pod is a DaemonSet pod, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:02:27.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="pyroscope-ebpf/<_>" checks="pod is a DaemonSet pod"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="[pod node selector does not match the node label, <_> <_> <_> <_> <_> <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="[pod node selector does not match the node label, insufficient <_>, insufficient <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="[pod node selector does not match the node label, insufficient <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="[pod node selector does not match the node label, pod does not tolerate taints on the node, insufficient <_>, insufficient <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="[pod node selector does not match the node label, pod does not tolerate taints on the node, insufficient <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="<_>/<_>" node:="<_>" error:="insufficient cpu"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="loki-dev-005/querier-burst-6b5f6db455-5zvkm" node:="<_>" error:="[insufficient <_>, insufficient <_>]"`,
				`I0507 12:02:27.<_>       1 node.go:157] "Pod does not fit on any other node" pod:="loki-dev-005/querier-burst-6b5f6db455-5zvkm" node:="<_>" error:="pod node selector does not match the node label"`,
				`I0507 12:02:27.<_>       1 node.go:339] "no Pod antiaffinity rule found" pod="<_>/<_>"`,
				`I0507 12:04:17.595169       1 descheduler.go:155] Building a pod evictor`,
				`I0507 12:04:17.596431       1 nodeutilization.go:204] "Node is underutilized" node="gke-dev-eu-west-3-main-n2s8-1-1dd39c-d1c92061-4z2l" usage={"cpu":"984m","memory":"611Mi","pods":"16"} usagePercentage={"cpu":12.44,"memory":2.15,"pods":25}`,
				`I0507 12:04:17.596484       1 highnodeutilization.go:107] "Criteria for a node below target utilization" CPU=50 Mem=50 Pods=100`,
				`I0507 12:04:17.596504       1 highnodeutilization.go:108] "Number of underutilized nodes" totalNumber=1`,
				`I0507 12:04:17.596528       1 nodeutilization.go:260] "Total capacity to be moved" CPU=5060 Mem=112216292800 Pods=163`,
				`I0507 12:04:17.596651       1 defaultevictor.go:202] "Pod fails the following checks" pod="kube-system/metrics-server-v0.6.3-68f5b7c4d5-t5mz8" checks="[pod has system critical priority, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:04:17.596803       1 defaultevictor.go:202] "Pod fails the following checks" pod="gadget/gadget-zjjts" checks="[pod is a DaemonSet pod, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:04:17.<_>       1 nodeutilization.go:207] "Node is overutilized" node="<_>" usage={"cpu":"<_>","memory":"<_>","pods":"<_>"} usagePercentage={"cpu":<_>.<_>,"memory":<_>.<_>,"pods":<_>.<_>}`,
				`I0507 12:04:17.<_>       1 nodeutilization.go:207] "Node is overutilized" node="<_>" usage={"cpu":"<_>","memory":"<_>","pods":"<_>"} usagePercentage={"cpu":<_>.<_>,"memory":<_>.<_>,"pods":<_>}`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="agent-logs/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="conntrack-exporter/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="goldpinger/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="kube-system/<_>" checks="[pod has system critical priority, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="kube-system/<_>" checks="[pod is a DaemonSet pod, pod has system critical priority, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="kube-system/<_>" checks="[pod is a DaemonSet pod, pod has system critical priority, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="kube-system/<_>" checks="[pod is a mirror pod, pod is a static pod, pod has system critical priority, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="netfilter-exporter/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="node-exporter/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold, pod has local storage and descheduler is not configured with evictLocalStoragePods]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="promtail-ops/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 defaultevictor.go:202] "Pod fails the following checks" pod="startup/<_>" checks="[pod is a DaemonSet pod, pod has higher priority than specified priority class threshold]"`,
				`I0507 12:<_>:<_>.<_>       1 descheduler.go:<_>] "Number of evicted pods" totalEvicted=<_>`,
				`I0507 12:<_>:<_>.<_>       1 nodeutilization.go:<_>] "Evicting pods from node" node="<_>" usage={"cpu":"<_>","memory":"<_>","pods":"<_>"}`,
				`I0507 12:<_>:<_>.<_>       1 nodeutilization.go:<_>] "No removable pods on node, try next node" node="<_>"`,
				`I0507 12:<_>:<_>.<_>       1 nodeutilization.go:<_>] "Pods on node" node="<_>" allPods=<_> nonRemovablePods=<_> removablePods=<_>`,
				`I0507 12:<_>:<_>.<_>       1 profile.go:<_>] "Total number of pods evicted" extension point="Balance" evictedPods=<_>`,
				`I0507 12:<_>:<_>.<_>       1 reflector.go:<_>] k8s.io/client-go/informers/factory.go:<_>: Watch close - *v1.<_> total <_> items received`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/vault.txt",
			patterns: []string{
				`2024-05-07T10:56:38.667Z [INFO]  expiration: revoked lease: lease_id=auth/gcp/login/h4c031a99aa555040a0dd99864d828e946c6d4e31f4f5178757183def61f9d104`,
				`2024-05-07T10:<_>:<_>.<_> [INFO]  expiration: revoked lease: lease_id=auth/kubernetes/<_>/login/<_>`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/calico.txt",
			patterns: []string{
				`2024-05-08 15:23:56.403 [DEBUG][615489] felix/table.go 699: Finished loading iptables state ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:56.403 [INFO][615489] felix/summary.go 100: Summarising 1 dataplane reconciliation loops over 600ms: avg=119ms longest=119ms (resync-filter-v4)`,
				`2024-05-08 15:23:56.614 [DEBUG][76] felix/int_dataplane.go 1777: Refreshing routes`,
				`2024-05-08 15:23:56.615 [DEBUG][76] felix/route_rule.go 179: Queueing a resync of routing rules. ipVersion=4`,
				`2024-05-08 15:23:56.615 [DEBUG][76] felix/route_table.go 480: Queueing a resync of routing table. ifaceRegex="<_>.<_>" ipVersion=0x4 tableIndex=<_>`,
				`2024-05-08 15:23:56.615 [DEBUG][76] felix/route_table.go 533: Check interfaces matching regex`,
				`2024-05-08 15:23:56.615 [DEBUG][76] felix/wireguard.go 605: Queueing a resync of wireguard configuration ipVersion=0x4`,
				`2024-05-08 15:23:56.615 [DEBUG][76] felix/wireguard.go 654: Wireguard is not in-sync - verifying wireguard configuration is removed ipVersion=0x4`,
				`2024-05-08 15:23:56.617 [DEBUG][76] felix/wireguard.go 1503: Wireguard is disabled and does not exist ifaceName="wireguard.cali" ipVersion=0x4`,
				`2024-05-08 15:23:56.619 [DEBUG][76] felix/route_table.go 584: Flag no OIF for full re-sync`,
				`2024-05-08 15:23:56.619 [DEBUG][76] felix/route_table.go 614: Synchronised routes on interface ifaceName="*NoOIF*" ifaceRegex="^wireguard.cali$" ipVersion=0x4 tableIndex=1`,
				`2024-05-08 15:23:56.619 [DEBUG][76] felix/route_table.go 661: Syncing interface routes ifaceName="*NoOIF*" ifaceRegex="^wireguard.cali$" ipVersion=0x4 tableIndex=1`,
				`2024-05-08 15:23:56.619 [DEBUG][76] felix/route_table.go 686: Reconcile against kernel programming ifaceName="*NoOIF*" ifaceRegex="^wireguard.cali$" ipVersion=0x4 tableIndex=1`,
				`2024-05-08 15:23:56.624 [INFO][76] felix/summary.go 100: Summarising 1 dataplane reconciliation loops over 200ms: avg=10ms longest=10ms (resync-routes-v4,resync-routes-v4,resync-rules-v4,resync-wg)`,
				`2024-05-08 15:23:56.<_> [DEBUG][615489] felix/table.go 677: Skipping expected chain chainName="<_>" ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:56.<_> [DEBUG][615489] felix/table.go 677: Skipping expected chain chainName="<_>.<_>" ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:56.<_> [DEBUG][615489] felix/table.go 677: Skipping expected chain chainName="cali-pro-ksa.<_>.<_>" ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 557: Resync: found calico-owned interface ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 614: Synchronised routes on interface ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 661: Syncing interface routes ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 686: Reconcile against kernel programming ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 880: Processing route: 254 <_> 10.68.10.<_>/32 ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:56.<_> [DEBUG][76] felix/route_table.go 915: Route is correct dest=10.68.10.<_>/32 ifaceName="<_>" ifaceRegex="^azv.*" ipVersion=0x4 tableIndex=0`,
				`2024-05-08 15:23:57.942 [WARNING][56] felix/table.go 654: Detected out-of-sync inserts, marking for resync actualRuleIDs=[]string{"", "", "", "", "", "", "", "", "", "", "", "", "tVnHkvAo15HuiPy0", "", ""} chainName="OUTPUT" expectedRuleIDs=[]string{"tVnHkvAo15HuiPy0", "", "", "", "", "", "", "", "", "", "", "", "", "", ""} ipVersion=0x4 table="raw"`,
				`2024-05-08 15:23:57.942 [WARNING][56] felix/table.go 654: Detected out-of-sync inserts, marking for resync actualRuleIDs=[]string{"", "", "", "", "6gwbT8clXdHdC1b1"} chainName="PREROUTING" expectedRuleIDs=[]string{"6gwbT8clXdHdC1b1", "", "", "", ""} ipVersion=0x4 table="raw"`,
				`2024-05-08 15:23:57.969 [WARNING][56] felix/table.go 654: Detected out-of-sync inserts, marking for resync actualRuleIDs=[]string{"", "", "", "", "Cz_u1IQiXIMmKD4c", "", "", "", "", "", "", "", "", "", "", "", ""} chainName="INPUT" expectedRuleIDs=[]string{"Cz_u1IQiXIMmKD4c", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""} ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:57.969 [WARNING][56] felix/table.go 654: Detected out-of-sync inserts, marking for resync actualRuleIDs=[]string{"", "", "", "", "tVnHkvAo15HuiPy0", "", "", "", "", ""} chainName="OUTPUT" expectedRuleIDs=[]string{"tVnHkvAo15HuiPy0", "", "", "", "", "", "", "", "", ""} ipVersion=0x4 table="filter"`,
				`2024-05-08 15:23:58.169 [INFO][2333] felix/summary.go 100: Summarising 35 dataplane reconciliation loops over 1m2s: avg=12ms longest=46ms (resync-filter-v4,resync-filter-v6,resync-mangle-v4,resync-mangle-v6,update-filter-v4,update-filter-v6)`,
				`2024-05-08 15:23:58.566 [DEBUG][3576126] felix/int_dataplane.go 957: Examining link for MTU calculation mtu=1500 name="eth0"`,
				`2024-05-08 15:23:58.680 [DEBUG][216945] felix/int_dataplane.go 1785: Reschedule kick received`,
				`2024-05-08 15:23:58.681 [DEBUG][216945] felix/feature_detect.go 112: Refreshing detected iptables features`,
				`2024-05-08 15:23:58.681 [DEBUG][216945] felix/table.go 944: Invalidating dataplane cache ipVersion=0x4 reason="refresh timer" table="nat"`,
				`2024-05-08 15:23:58.684 [DEBUG][216945] felix/feature_detect.go 242: Ran iptables --version rawVersion="iptables v1.8.4 (legacy)\n"`,
				`2024-05-08 15:23:58.684 [DEBUG][216945] felix/feature_detect.go 255: Parsed iptables version version=1.8.4`,
				`2024-05-08 15:23:58.684 [DEBUG][216945] felix/table.go 604: Loading current iptables state and checking it is correct. ipVersion=0x4 table="nat"`,
				`2024-05-08 15:23:58.684 [DEBUG][216945] felix/versionparse.go 110: Raw kernel version rawVersion="Linux version 5.15.0-1057-azure (buildd@lcy02-amd64-033) (gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #65-Ubuntu SMP Fri Feb 9 18:39:24 UTC 2024\n"`,
				`2024-05-08 15:23:58.684 [DEBUG][216945] felix/versionparse.go 118: Parsed kernel version version=5.15.0-1057`,
				`2024-05-08 15:23:58.715 [DEBUG][216945] felix/table.go 851: Parsing line ipVersion=0x4 line="# Generated by iptables-nft-save v1.8.4 on Wed May  8 15:23:58 2024" table="nat"`,
				`2024-05-08 15:23:58.716 [DEBUG][216945] felix/table.go 851: Parsing line ipVersion=0x4 line="*nat" table="nat"`,
				`2024-05-08 15:23:58.716 [DEBUG][216945] felix/table.go 881: Not an append, skipping ipVersion=0x4 line="# Generated by iptables-nft-save v1.8.4 on Wed May  8 15:23:58 2024" table="nat"`,
				`2024-05-08 15:23:58.716 [DEBUG][216945] felix/table.go 881: Not an append, skipping ipVersion=0x4 line="*nat" table="nat"`,
				`2024-05-08 15:23:58.<_> [DEBUG][216945] felix/table.go 851: Parsing line ipVersion=0x4 line=":<_> <_> [0:0]" table="nat"`,
				`2024-05-08 15:23:58.<_> [DEBUG][216945] felix/table.go 870: Found forward-reference chainName="<_>" ipVersion=0x4 line=":<_> <_> [0:0]" table="nat"`,
				`2024-05-08 15:23:58.<_> [DEBUG][3576126] felix/int_dataplane.go 954: Skipping interface for MTU detection mtu=<_> name="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/endpoint_mgr.go 443: Reporting endpoint status. dirtyEndpoints=set.Set{}`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/health.go 167: Health: <_>`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/health.go 196: Checking state of reporter reporter=&health.reporterState{name:"<_>", reports:health.HealthReport{Live:true, Ready:true, Detail:""}, timeout:<_>, latest:health.HealthReport{Live:true, Ready:true, Detail:""}, timestamp:time.Time{wall:<_>, ext:<_>, loc:(*time.Location)(0x4ce3aa0)}}`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/health.go 245: Calculated health summary healthResult=&health.HealthReport{Live:true, Ready:true, Detail:"+------------------+---------+----------------+-----------------+--------+\n|    COMPONENT     | TIMEOUT |    LIVENESS    |    READINESS    | DETAIL |\n+------------------+---------+----------------+-----------------+--------+\n| async_calc_graph | 20s     | reporting live | reporting ready |        |\n| felix-startup    | 0s      | reporting live | reporting ready |        |\n| int_dataplane    | 1m30s   | reporting live | reporting ready |        |\n+------------------+---------+----------------+-----------------+--------+"}`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/health.go <_>: GET /<_>`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/int_dataplane.go 1773: Refreshing IP sets state`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/int_dataplane.go 1807: Applying dataplane updates`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/int_dataplane.go 2080: Asked to reschedule. delay=<_>.<_>`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 234: Asked to resync with the dataplane on next update. family="inet"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 314: Resyncing ipsets with dataplane. family="inet"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 366: Finished IPSets resync family="inet" numInconsistenciesFound=0 resyncDuration=<_>.<_>`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 426: Parsing IP set. family="inet" setName="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 467: Found member in dataplane canon=<_>.<_>.<_>.<_> family="inet" member="<_>.<_>.<_>.<_>" setID="this-host"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 589: Whitelisting IP sets. ID="<_>" family="inet" mainName="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 607: Skipping expected Calico IP set. family="inet" setName="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/ipsets.go 643: No dirty IP sets. family="inet"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/sync_client.go 347: Ping received from Typha connID=0x0 connection=&discovery.Typha{Addr:"", IP:"", NodeName:(*string)(nil)} type=""`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/sync_client.go 356: Pong sent to Typha connID=0x0 connection=&discovery.Typha{Addr:"", IP:"", NodeName:(*string)(nil)} type=""`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/sync_client.go 434: New message from Typha. connID=0x0 connection=&discovery.Typha{Addr:"", IP:"", NodeName:(*string)(nil)} envelope=syncproto.Envelope{Message:syncproto.MsgPing{Timestamp:time.Date(2024, time.May, 8, 15, 23, <_>, <_>, time.Local)}} type=""`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/table.go 1233: In nftables mode, restarting transaction between updates and deletions. ipVersion=0x4 table="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/table.go 1263: Update ended up being no-op, skipping call to ip(6)tables-restore. ipVersion=0x4 table="<_>"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/wireguard.go 652: Wireguard is not enabled, skipping sync ipVersion=0x4`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1004: Updating ipsetIDsToMembers cache. family=4`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1043: Processing pending diff state. cs=&intdataplane.xdpSystemState{IfaceNameToData:map[string]intdataplane.xdpIfaceData{}, XDPEligiblePolicies:map[proto.PolicyID]intdataplane.xdpRules{}} family=4`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1270: Finished processing pending diff state. bpfActions=intdataplane.xdpBPFActions{CreateMap:set.Typed[string]{}, RemoveMap:set.Typed[string]{}, AddToMap:map[string]map[string]uint32{}, RemoveFromMap:map[string]map[string]uint32{}, InstallXDP:set.Typed[string]{}, UninstallXDP:set.Typed[string]{}, MembersToDrop:map[string]map[string]uint32{}, MembersToAdd:map[string]map[string]uint32{}} family=4 newCS=&intdataplane.xdpSystemState{IfaceNameToData:map[string]intdataplane.xdpIfaceData{}, XDPEligiblePolicies:map[proto.PolicyID]intdataplane.xdpRules{}}`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1605: Getting member changes. family=4 oldMembers=map[string]set.Set[string]{}`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1798: Processing BPF actions. family="ipv4"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 1932: Finished processing BPF actions. family="ipv4"`,
				`2024-05-08 15:23:<_>.<_> [DEBUG][<_>] felix/xdp_state.go 968: Processing member updates. family=4`,
				`2024-05-08 15:23:<_>.<_> [INFO][<_>] felix/summary.go 100: Summarising <_> dataplane reconciliation loops over <_>.<_>: avg=<_> longest=<_> (<_>)`,
				"bird: Netlink: No route to host",
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/grafana-ruler.txt",
			patterns: []string{
				`level=debug ts=2024-05-29T13:44:15.804597912Z caller=remote_instance_store.go:51 user=297794 slug=leanix msg="calling SaveAlertInstance"`,
				`level=debug ts=2024-05-29T13:44:15.<_> caller=remote_instance_store.go:51 user=396586 slug=opengov msg="calling SaveAlertInstance"`,
				`level=debug ts=2024-05-29T13:44:15.<_> caller=remote_instance_store.go:51 user=<_> slug=<_> msg="calling SaveAlertInstance"`,
				`logger=ngalert.scheduler user=102553 slug=flownative version=1 fingerprint=4ad9e35be0f80ca3 attempt=1 now=2024-05-29T13:44:10Z t=2024-05-29T13:44:15.79499903Z level=debug msg="Alert rule evaluated" results="[{Instance: State:Normal Error:<nil> Results:map[] Values:map[] EvaluatedAt:2024-05-29 13:44:10 +0000 UTC EvaluationDuration:5.794695854s EvaluationString:}]" duration=116.038803ms`,
				`logger=ngalert.scheduler user=473762 slug=intentiq version=35 fingerprint=0bc4b6f46a852420 attempt=1 now=2024-05-29T13:44:10Z t=2024-05-29T13:44:15.788200731Z level=debug msg="Alert rule evaluated" results="[{Instance:datasource_uid=grafanacloud-prom, ref_id=A State:NoData Error:<nil> Results:map[] Values:map[] EvaluatedAt:2024-05-29 13:44:10 +0000 UTC EvaluationDuration:5.787878355s EvaluationString:}]" duration=15.345212ms`,
				`logger=ngalert.scheduler user=70430 slug=dapperlabs version=1 fingerprint=65a68c433031b4e0 attempt=1 now=2024-05-29T13:44:10Z t=2024-05-29T13:44:15.790598463Z level=debug msg="Alert rule evaluated" results="[{Instance: State:Normal Error:<nil> Results:map[] Values:map[] EvaluatedAt:2024-05-29 13:44:10 +0000 UTC EvaluationDuration:5.78875161s EvaluationString:}]" duration=1.693079007s`,
				`logger=ngalert.state.manager user=102553 slug=flownative instance= t=2024-05-29T13:44:15.795103234Z level=debug msg="Setting next state" handler=resultNormal`,
				`logger=ngalert.state.manager user=15338 slug=rstsoftwarerc instance= t=2024-05-29T13:44:15.790951656Z level=debug msg="Keeping state" state=Alerting previous_ends_at=2024-05-29T13:47:00Z next_ends_at=2024-05-29T13:48:00Z`,
				`logger=ngalert.state.manager user=172772 slug=ppbtradingtribe instance="datasource_uid=p06gSxS7k, ref_id=A" t=2024-05-29T13:44:15.793080651Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=172772 slug=ppbtradingtribe t=2024-05-29T13:44:15.79304032Z level=debug msg="State manager processing evaluation results" resultCount=1`,
				`logger=ngalert.state.manager user=228733 slug=csmoney instance="datasource_uid=grafanacloud-logs, ref_id=A" t=2024-05-29T13:44:15.796750449Z level=debug msg="Setting next state" handler=resultNoData`,
				`logger=ngalert.state.manager user=371756 slug=asapp instance="company_marker=dish" t=2024-05-29T13:44:15.788780219Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=371756 slug=asapp instance="company_marker=optimumfixed" t=2024-05-29T13:44:15.788904162Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=371756 slug=asapp instance="company_marker=rcn" t=2024-05-29T13:44:15.789011178Z level=debug msg="Setting next state" handler=resultNormal`,
				`logger=ngalert.state.manager user=412141 slug=sharethrough instance="datasource_uid=pFBylkiVz, ref_id=Swap Usage for Alert" t=2024-05-29T13:44:15.792756002Z level=debug msg="Setting next state" handler=resultNoData`,
				`logger=ngalert.state.manager user=412141 slug=sharethrough instance="datasource_uid=pFBylkiVz, ref_id=Swap Usage for Alert" t=2024-05-29T13:44:15.792775073Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=430961 slug=solifi instance= t=2024-05-29T13:44:15.799932951Z level=debug msg="Setting next state" handler=resultNormal`,
				`logger=ngalert.state.manager user=430961 slug=solifi instance= t=2024-05-29T13:44:15.799945019Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=473762 slug=intentiq instance="datasource_uid=grafanacloud-prom, ref_id=A" t=2024-05-29T13:44:15.<_> level=debug msg="Execution no data state is Normal" handler=resultNormal previous_handler=resultNoData`,
				`logger=ngalert.state.manager user=473762 slug=intentiq instance="datasource_uid=grafanacloud-prom, ref_id=A" t=2024-05-29T13:44:15.<_> level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=473762 slug=intentiq instance="datasource_uid=grafanacloud-prom, ref_id=A" t=2024-05-29T13:44:15.<_> level=debug msg="Setting next state" handler=resultNoData`,
				`logger=ngalert.state.manager user=473762 slug=intentiq t=2024-05-29T13:44:15.788261794Z level=debug msg="State manager processing evaluation results" resultCount=1`,
				`logger=ngalert.state.manager user=630397 slug=tatin instance= t=2024-05-29T13:44:15.795542988Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=679029 slug=joveoprodaws instance="datasource_uid=grafanacloud-logs, ref_id=A" t=2024-05-29T13:44:15.800327814Z level=debug msg="Setting next state" handler=resultNoData`,
				`logger=ngalert.state.manager user=692010 slug=mercariusprod instance="datasource_uid=gfds-prometheus-wrapper, ref_id=B" t=2024-05-29T13:44:15.791100679Z level=debug msg="Execution no data state is Normal" handler=resultNormal previous_handler=resultNoData`,
				`logger=ngalert.state.manager user=692010 slug=mercariusprod instance="datasource_uid=gfds-prometheus-wrapper, ref_id=B" t=2024-05-29T13:44:15.791114955Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=692010 slug=mercariusprod instance="datasource_uid=gfds-prometheus-wrapper, ref_id=B" t=2024-05-29T13:44:15.791129917Z level=debug msg="Setting next state" handler=resultNoData`,
				`logger=ngalert.state.manager user=84535 slug=arweave instance= t=2024-05-29T13:44:15.796640981Z level=debug msg="Setting next state" handler=resultNormal`,
				`logger=ngalert.state.manager user=84535 slug=arweave t=2024-05-29T13:44:15.796542294Z level=debug msg="State manager processing evaluation results" resultCount=1`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=<_>, instance=172.30.<_>.<_>:8080, job=integrations/kubernetes/kube-state-metrics, namespace=<_>, pod=<_>, uid=<_>" t=2024-05-29T13:44:15.<_> level=debug msg="Setting next state" handler=resultNormal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=consul, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=wcs9-tds-devus-vault-con-74f6c575b8-6d879, uid=f5320297-1117-400f-9704-d4f43fa1127d" t=2024-05-29T13:44:15.78870732Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=crs-app, instance=172.30.<_>.<_>:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=<_>, uid=<_>" t=2024-05-29T13:44:15.<_> level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=frontend, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=tdsdevcaauth-exo-frontend-5c569cbc88-fr7t4, uid=2b8456c8-297f-4763-8f00-f8076b542d7c" t=2024-05-29T13:44:15.790564871Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=node, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds-devops, pod=exo-devca-cicd-288-zcl2b-9ws4z-nzgt7, uid=ca99b6a7-f08f-475a-adf6-dcf8c8936eed" t=2024-05-29T13:44:15.791738618Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=search-app-master, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=tdsdevauthsearch-app-master-65969fb8d5-c7nl4, uid=c4f14b2b-581a-4543-a848-af6e25ada58a" t=2024-05-29T13:44:15.79227249Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=search-app-repeater, instance=172.30.<_>.<_>:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=<_>, uid=<_>" t=2024-05-29T13:44:15.<_> level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=tdsdevauthts-utils, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=tdsdevauthts-utils-7f54f8d7b4-njddr, uid=352d7df2-7832-41f3-ad3e-cbe1a060c968" t=2024-05-29T13:44:15.793846886Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=tdsqalivets-utils, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=tdsqalivets-utils-75b748978f-r2vkj, uid=1d39d0d7-d483-427b-ba91-45d897674698" t=2024-05-29T13:44:15.794284465Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=ts-app, instance=172.30.<_>.<_>:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=<_>, uid=<_>" t=2024-05-29T13:44:15.<_> level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager user=893151 slug=cmtdsnp instance="cluster=tds-np-cluster, container=ts-web, instance=172.30.43.160:8080, job=integrations/kubernetes/kube-state-metrics, namespace=tds, pod=tdsqaauthts-web-57f5b6f56b-bdmh9, uid=8f6b5224-94ce-4f5d-ba08-03f9fc2f572f" t=2024-05-29T13:44:15.795397351Z level=debug msg="Keeping state" state=Normal`,
				`logger=ngalert.state.manager.persist user=14927 slug=rstsoftware t=2024-05-29T13:44:15.798496844Z level=debug msg="Saving alert states done" count=1 max_state_save_concurrency=1 duration=26.340653ms`,
				`logger=ngalert.state.manager.persist user=20177 slug=paddledash t=2024-05-29T13:44:15.806655602Z level=debug msg="Saving alert states" count=1 max_state_save_concurrency=1`,
				`logger=ngalert.state.manager.persist user=<_> slug=<_> t=2024-05-29T13:44:15.<_> level=debug msg="Saving alert states" count=<_> max_state_save_concurrency=1`,
			},
		},
		{
			drain:     New(DefaultConfig(), nil),
			inputFile: "testdata/systemd-journal.txt",
			patterns: []string{
				`2024-05-28 15:23:06.015 [INFO][6691] k8s.go 383: Populated endpoint ContainerID="342afa6a6f19381653bcbb0e0572a73473e3d6ed2a559fbd5ec53f5d1c1aa2bd" Namespace="insight-logs" Pod="promtail-insight-logs-kjh2j" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--insight--logs--kjh2j-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--insight--logs--kjh2j-eth0", GenerateName:"promtail-insight-logs-", Namespace:"insight-logs", SelfLink:"", UID:"adc62dfa-dfde-4daf-9e87-60509df05aba", ResourceVersion:"4941512003", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 22, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"controller-revision-hash":"dcf8588df", "insights":"true", "name":"promtail-insight-logs", "pod-template-generation":"38", "projectcalico.org/namespace":"insight-logs", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"promtail-insight-logs"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"", Pod:"promtail-insight-logs-kjh2j", Endpoint:"eth0", ServiceAccountName:"promtail-insight-logs", IPNetworks:[]string{"10.132.121.136/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.insight-logs", "ksa.insight-logs.promtail-insight-logs"}, InterfaceName:"califdb2b3232cc", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:06.042 [INFO][6679] k8s.go 383: Populated endpoint ContainerID="7ccab12f2be8afc73db5920869bc7901251a553a7405b3aaeb6c6be29faa1314" Namespace="loki-dev-ssd" Pod="promtail-loki-dev-ssd-k8rlx" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--loki--dev--ssd--k8rlx-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--loki--dev--ssd--k8rlx-eth0", GenerateName:"promtail-loki-dev-ssd-", Namespace:"loki-dev-ssd", SelfLink:"", UID:"9ffe7e11-818b-44b4-80c6-a6200ecbe5a7", ResourceVersion:"4941512019", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 22, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"controller-revision-hash":"5dc999c76d", "name":"promtail-loki-dev-ssd", "pod-template-generation":"102", "projectcalico.org/namespace":"loki-dev-ssd", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"promtail-loki-dev-ssd"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"", Pod:"promtail-loki-dev-ssd-k8rlx", Endpoint:"eth0", ServiceAccountName:"promtail-loki-dev-ssd", IPNetworks:[]string{"10.132.121.138/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.loki-dev-ssd", "ksa.loki-dev-ssd.promtail-loki-dev-ssd"}, InterfaceName:"cali7bceaac748f", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:06.179 [INFO][6691] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="342afa6a6f19381653bcbb0e0572a73473e3d6ed2a559fbd5ec53f5d1c1aa2bd" Namespace="insight-logs" Pod="promtail-insight-logs-kjh2j" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--insight--logs--kjh2j-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--insight--logs--kjh2j-eth0", GenerateName:"promtail-insight-logs-", Namespace:"insight-logs", SelfLink:"", UID:"adc62dfa-dfde-4daf-9e87-60509df05aba", ResourceVersion:"4941512003", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 22, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"controller-revision-hash":"dcf8588df", "insights":"true", "name":"promtail-insight-logs", "pod-template-generation":"38", "projectcalico.org/namespace":"insight-logs", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"promtail-insight-logs"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"342afa6a6f19381653bcbb0e0572a73473e3d6ed2a559fbd5ec53f5d1c1aa2bd", Pod:"promtail-insight-logs-kjh2j", Endpoint:"eth0", ServiceAccountName:"promtail-insight-logs", IPNetworks:[]string{"10.132.121.136/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.insight-logs", "ksa.insight-logs.promtail-insight-logs"}, InterfaceName:"califdb2b3232cc", MAC:"16:65:47:4d:29:6a", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:06.189 [INFO][6679] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="7ccab12f2be8afc73db5920869bc7901251a553a7405b3aaeb6c6be29faa1314" Namespace="loki-dev-ssd" Pod="promtail-loki-dev-ssd-k8rlx" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--loki--dev--ssd--k8rlx-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--loki--dev--ssd--k8rlx-eth0", GenerateName:"promtail-loki-dev-ssd-", Namespace:"loki-dev-ssd", SelfLink:"", UID:"9ffe7e11-818b-44b4-80c6-a6200ecbe5a7", ResourceVersion:"4941512019", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 22, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"controller-revision-hash":"5dc999c76d", "name":"promtail-loki-dev-ssd", "pod-template-generation":"102", "projectcalico.org/namespace":"loki-dev-ssd", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"promtail-loki-dev-ssd"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"7ccab12f2be8afc73db5920869bc7901251a553a7405b3aaeb6c6be29faa1314", Pod:"promtail-loki-dev-ssd-k8rlx", Endpoint:"eth0", ServiceAccountName:"promtail-loki-dev-ssd", IPNetworks:[]string{"10.132.121.138/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.loki-dev-ssd", "ksa.loki-dev-ssd.promtail-loki-dev-ssd"}, InterfaceName:"cali7bceaac748f", MAC:"3a:fc:37:ae:56:71", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:09.688 [INFO][7450] k8s.go 383: Populated endpoint ContainerID="818f06bc77a238fe44267b763aaf0e239986360fa2eba970ef5dcaf1c697c008" Namespace="mimir-dev-11" Pod="querier-burst-backup-784fb4bfd-8jgmn" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-querier--burst--backup--784fb4bfd--8jgmn-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-querier--burst--backup--784fb4bfd--8jgmn-eth0", GenerateName:"querier-burst-backup-784fb4bfd-", Namespace:"mimir-dev-11", SelfLink:"", UID:"e570dbea-c552-4583-89a6-da8d28f4af11", ResourceVersion:"4941513736", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 21, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"gossip_ring_member":"true", "name":"querier-burst-backup", "pod-template-hash":"784fb4bfd", "poddisruptionbudget-group":"non-spot-querier", "projectcalico.org/namespace":"mimir-dev-11", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default", "service":"querier"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"", Pod:"querier-burst-backup-784fb4bfd-8jgmn", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.132.121.139/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.mimir-dev-11", "ksa.mimir-dev-11.default"}, InterfaceName:"cali68b627e98b5", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2387, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"gossip-ring", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f0a, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:09.711 [INFO][7450] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="818f06bc77a238fe44267b763aaf0e239986360fa2eba970ef5dcaf1c697c008" Namespace="mimir-dev-11" Pod="querier-burst-backup-784fb4bfd-8jgmn" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-querier--burst--backup--784fb4bfd--8jgmn-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-querier--burst--backup--784fb4bfd--8jgmn-eth0", GenerateName:"querier-burst-backup-784fb4bfd-", Namespace:"mimir-dev-11", SelfLink:"", UID:"e570dbea-c552-4583-89a6-da8d28f4af11", ResourceVersion:"4941513736", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 21, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"gossip_ring_member":"true", "name":"querier-burst-backup", "pod-template-hash":"784fb4bfd", "poddisruptionbudget-group":"non-spot-querier", "projectcalico.org/namespace":"mimir-dev-11", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default", "service":"querier"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-main-n2s16-3-1dd-9b502d96-n2g6", ContainerID:"818f06bc77a238fe44267b763aaf0e239986360fa2eba970ef5dcaf1c697c008", Pod:"querier-burst-backup-784fb4bfd-8jgmn", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.132.121.139/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.mimir-dev-11", "ksa.mimir-dev-11.default"}, InterfaceName:"cali68b627e98b5", MAC:"02:7d:84:f3:5f:a8", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2387, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"gossip-ring", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f0a, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:22.529 [INFO][2064676] k8s.go 383: Populated endpoint ContainerID="af89a748262ca5af9558bd0dfb8f42af690c624b21ff9e5cf0d899fa6c145acb" Namespace="hosted-grafana" Pod="benchloadtestingxxl3-grafana-6f5fd85466-nhjkb" WorkloadEndpoint="gke--dev--us--central--0--hg--n2s8--6--1dd39c--3bfd06e9--8594-k8s-benchloadtestingxxl3--grafana--6f5fd85466--nhjkb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--hg--n2s8--6--1dd39c--3bfd06e9--8594-k8s-benchloadtestingxxl3--grafana--6f5fd85466--nhjkb-eth0", GenerateName:"benchloadtestingxxl3-grafana-6f5fd85466-", Namespace:"hosted-grafana", SelfLink:"", UID:"cea59393-2875-4737-9087-06d9189a8813", ResourceVersion:"4941514872", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 23, 22, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"grafana", "autoscaling":"false", "conprof":"true", "insights":"true", "instanceId":"7255", "multiReplica":"true", "name":"grafana", "org":"benchloadtestingxxl3", "plan":"gcloud", "pod-template-hash":"6f5fd85466", "projectcalico.org/namespace":"hosted-grafana", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"hg-instance", "resource_version":"8354868", "slug":"benchloadtestingxxl3", "stackId":"4206"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-hg-n2s8-6-1dd39c-3bfd06e9-8594", ContainerID:"", Pod:"benchloadtestingxxl3-grafana-6f5fd85466-nhjkb", Endpoint:"eth0", ServiceAccountName:"hg-instance", IPNetworks:[]string{"10.132.163.38/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.hosted-grafana", "ksa.hosted-grafana.hg-instance"}, InterfaceName:"cali168efaf2aff", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2710, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"profiling", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x17ac, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:22.555 [INFO][2064676] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="af89a748262ca5af9558bd0dfb8f42af690c624b21ff9e5cf0d899fa6c145acb" Namespace="hosted-grafana" Pod="benchloadtestingxxl3-grafana-6f5fd85466-nhjkb" WorkloadEndpoint="gke--dev--us--central--0--hg--n2s8--6--1dd39c--3bfd06e9--8594-k8s-benchloadtestingxxl3--grafana--6f5fd85466--nhjkb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"gke--dev--us--central--0--hg--n2s8--6--1dd39c--3bfd06e9--8594-k8s-benchloadtestingxxl3--grafana--6f5fd85466--nhjkb-eth0", GenerateName:"benchloadtestingxxl3-grafana-6f5fd85466-", Namespace:"hosted-grafana", SelfLink:"", UID:"cea59393-2875-4737-9087-06d9189a8813", ResourceVersion:"4941514872", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 23, 22, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"grafana", "autoscaling":"false", "conprof":"true", "insights":"true", "instanceId":"7255", "multiReplica":"true", "name":"grafana", "org":"benchloadtestingxxl3", "plan":"gcloud", "pod-template-hash":"6f5fd85466", "projectcalico.org/namespace":"hosted-grafana", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"hg-instance", "resource_version":"8354868", "slug":"benchloadtestingxxl3", "stackId":"4206"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"gke-dev-us-central-0-hg-n2s8-6-1dd39c-3bfd06e9-8594", ContainerID:"af89a748262ca5af9558bd0dfb8f42af690c624b21ff9e5cf0d899fa6c145acb", Pod:"benchloadtestingxxl3-grafana-6f5fd85466-nhjkb", Endpoint:"eth0", ServiceAccountName:"hg-instance", IPNetworks:[]string{"10.132.163.38/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.hosted-grafana", "ksa.hosted-grafana.hg-instance"}, InterfaceName:"cali168efaf2aff", MAC:"1e:0a:b3:41:0a:38", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2710, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"profiling", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x17ac, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:<_>.<_> [INFO][<_>] dataplane_linux.go 520: CleanUpNamespace called with no netns name, ignoring. ContainerID="<_>" iface="eth0" netns=""`,
				`2024-05-28 15:23:<_>.<_> [INFO][<_>] k8s.go 383: Populated endpoint ContainerID="<_>" Namespace="hosted-grafana" Pod="<_>" WorkloadEndpoint="<_>" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"<_>", GenerateName:"<_>", Namespace:"hosted-grafana", SelfLink:"", UID:"<_>", ResourceVersion:"<_>", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 23, <_>, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"grafana", "conprof":"true", "insights":"true", "instanceId":"<_>", "multiReplica":"false", "name":"grafana", "org":"<_>", "plan":"<_>", "pod-template-hash":"<_>", "projectcalico.org/namespace":"hosted-grafana", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"hg-instance", "resource_version":"<_>", "slug":"<_>", "stackId":"<_>"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"<_>", ContainerID:"", Pod:"<_>", Endpoint:"eth0", ServiceAccountName:"hg-instance", IPNetworks:[]string{"10.132.<_>.<_>/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.hosted-grafana", "ksa.hosted-grafana.hg-instance"}, InterfaceName:"<_>", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2710, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"profiling", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x17ac, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:<_>.<_> [INFO][<_>] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="<_>" Namespace="hosted-grafana" Pod="<_>" WorkloadEndpoint="<_>" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"<_>", GenerateName:"<_>", Namespace:"hosted-grafana", SelfLink:"", UID:"<_>", ResourceVersion:"<_>", Generation:0, CreationTimestamp:time.Date(2024, time.May, 28, 15, 23, <_>, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"grafana", "conprof":"true", "insights":"true", "instanceId":"<_>", "multiReplica":"false", "name":"grafana", "org":"<_>", "plan":"<_>", "pod-template-hash":"<_>", "projectcalico.org/namespace":"hosted-grafana", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"hg-instance", "resource_version":"<_>", "slug":"<_>", "stackId":"<_>"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"<_>", ContainerID:"<_>", Pod:"<_>", Endpoint:"eth0", ServiceAccountName:"hg-instance", IPNetworks:[]string{"10.132.<_>.<_>/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.hosted-grafana", "ksa.hosted-grafana.hg-instance"}, InterfaceName:"<_>", MAC:"<_>:<_>:<_>:<_>:<_>:<_>", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"http-metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x50, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"grpc", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x2710, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"profiling", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x17ac, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil)}}`,
				`2024-05-28 15:23:<_>.<_> [INFO][<_>] utils.go 107: File /var/lib/calico/mtu does not exist`,
				`2024-05-28 15:23:<_>.<_> [WARNING][<_>] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] dataplane_linux.go 473: Disabling IPv4 forwarding ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] dataplane_linux.go 524: Deleting workload's device in netns. ContainerID="<_>" iface="eth0" netns="/var/run/netns/<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] dataplane_linux.go 535: Entered netns, deleting veth. ContainerID="<_>" iface="eth0" netns="/var/run/netns/<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] dataplane_linux.go 569: Deleted device in netns. ContainerID="<_>" after=<_>.<_> iface="eth0" netns="/var/run/netns/<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] dataplane_linux.go 68: Setting the host side veth name to <_> ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 384: Calico CNI using IPs: [10.<_>.<_>.<_>/32] ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 489: Wrote updated endpoint to datastore ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 576: Cleaning up netns ContainerID="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 583: Releasing IP address(es) ContainerID="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 589: Teardown processing complete. ContainerID="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] utils.go 195: Calico CNI releasing IP address ContainerID="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] utils.go 213: Using dummy podCidrs to release the IPs ContainerID="<_>" podCidrv4="0.0.0.0/0" podCidrv6="::/0"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] utils.go 344: Calico CNI passing podCidr to host-local IPAM: 0.0.0.0/0 ContainerID="<_>"`,
				`2024-05-28 15:<_>:<_>.<_> [INFO][<_>] utils.go 344: Calico CNI passing podCidr to host-local IPAM: 10.<_>.<_>.<_>/<_> ContainerID="<_>" Namespace="<_>" Pod="<_>" WorkloadEndpoint="<_>"`,
				`2024-05-28T15:23:10.907544Z INFO TelemetryEventsCollector ExtHandler Collected 2 events for extension: Microsoft.Azure.Extensions.CustomScript`,
				`2024-05-28T15:23:12.281370Z INFO ExtHandler ExtHandler [HEARTBEAT] Agent WALinuxAgent-2.10.0.8 is running as the goal state agent [DEBUG HeartbeatCounter: 2903;HeartbeatId: 4F38E7DD-733D-4E31-8287-4D4CBE1CCB20;DroppedPackets: 0;UpdateGSErrors: 0;AutoUpdate: 1]`,
				`<_>.scope: Consumed <_>.<_> CPU time.`,
				`<_>.scope: Deactivated successfully.`,
				`<_>.service: Deactivated successfully.`,
				`<_>.slice: Consumed <_>.<_> CPU time.`,
				`<_>: Caught tx_queue_len zero misconfig`,
				`<_>: Gained <_>`,
				`<_>: Link <_>`,
				`<_>: Lost carrier`,
				`<_>\<_>\<_>\<_>\<_>.io\<_>\<_>\<_>\<_>.mount: Deactivated successfully.`,
				`AVC apparmor="DENIED" operation="ptrace" profile="cri-containerd.apparmor.d" pid=<_> comm="pidof" requested_mask="read" denied_mask="read" peer="unconfined"`,
				`CallMethodAndBlockWithTimeout(...): Domain=dbus, Code=org.freedesktop.DBus.Error.ServiceUnknown, Message=Error calling D-Bus method: org.chromium.SessionManagerInterface.RetrieveActiveSessions: The name org.chromium.SessionManager was not provided by any .service files`,
				`Changing forwarded IPs for 42:01:0a:80:<_>:<_> from ["34.67.102.156" "34.69.173.122" "34.71.94.187" "34.132.141.30" "34.133.120.161" "34.171.24.60" "34.172.42.155" "35.188.127.216" "35.193.99.154" "35.202.202.155" "35.222.85.117" "35.239.24.222" "104.198.188.146"] to ["34.67.102.156" "34.69.173.122" "34.71.94.187" "34.132.141.30" "34.133.120.161" "34.171.24.60" "34.172.42.155" "35.188.127.216" "35.193.99.154" "35.202.202.155" "35.222.85.117" "35.239.24.222" "35.239.149.219" "104.198.188.146"] by adding ["35.239.149.219"]`,
				`Changing forwarded IPs for 42:01:0a:80:<_>:<_> from ["34.67.102.156" "34.69.173.122" "34.71.94.187" "34.132.141.30" "34.133.120.161" "34.171.24.60" "34.172.42.155" "35.188.127.216" "35.193.99.154" "35.202.202.155" "35.222.85.117" "35.239.24.222" "<_>.<_>.<_>.<_>" <_> <_> <_>"34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "34.<_>.<_>.<_>" "<_>.<_>.<_>.<_>" "35.<_>.<_>.<_>" "35.193.<_>.<_>" "35.<_>.<_>.<_>" "35.<_>.<_>.<_>" "35.<_>.<_>.<_>" "35.239.<_>.<_>" "<_>.<_>.<_>.<_>" <_>.<_> <_> <_> <_>"<_>.<_>.<_>.<_>" "35.<_>.<_>.<_>"]`,
				`ContainerID="342afa6a6f19381653bcbb0e0572a73473e3d6ed2a559fbd5ec53f5d1c1aa2bd" Namespace="insight-logs" Pod="promtail-insight-logs-kjh2j" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--insight--logs--kjh2j-"`,
				`ContainerID="7ccab12f2be8afc73db5920869bc7901251a553a7405b3aaeb6c6be29faa1314" Namespace="loki-dev-ssd" Pod="promtail-loki-dev-ssd-k8rlx" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-promtail--loki--dev--ssd--k8rlx-"`,
				`ContainerID="818f06bc77a238fe44267b763aaf0e239986360fa2eba970ef5dcaf1c697c008" Namespace="mimir-dev-11" Pod="querier-burst-backup-784fb4bfd-8jgmn" WorkloadEndpoint="gke--dev--us--central--0--main--n2s16--3--1dd--9b502d96--n2g6-k8s-querier--burst--backup--784fb4bfd--8jgmn-"`,
				`ContainerID="<_>" Namespace="hosted-grafana" Pod="<_>" WorkloadEndpoint="<_>"`,
				`Created slice <_>.slice.`,
				`DHCPACK from 10.60.0.1 (xid=0xf58de59)`,
				`DHCPREQUEST on eth0 to 10.60.0.1 port 67 (xid=0xf58de59)`,
				`E0528 15:23:09.<_>    4626 prober.go:104] "Probe errored" err="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task 51f71e1ac59ec5db0aa744684bbf3eda9c379b440682dc0cbd27710c622ed895 not found: not found" probeType="Readiness" pod="hosted-grafana/dev05devuseast0test-grafana-544ffc4d8f-d5k6v" podUID="7dc6279d-efc6-44a1-876c-6d15464889bb" containerName="grafana"`,
				`E0528 15:23:13.644531    4657 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2: not found" image="us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2"`,
				`E0528 15:23:13.797171    4657 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/10.1.0-ephemeral-oss-77506-8314-2: 403 Forbidden" image="us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2"`,
				`E0528 15:23:13.797416    4657 kuberuntime_manager.go:1256] container &Container{Name:grafana,Image:us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2,Command:[/bin/sh],Args:[-c set -e; while [ "$(pidof hgrun-pause)" = "" ]; do sleep 0.5; done;`,
				`E0528 15:23:13.797475    4657 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"grafana\" with ErrImagePull: \"[rpc error: code = NotFound desc = failed to pull and unpack image \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\\\": failed to resolve reference \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\\\": us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2: not found, failed to pull and unpack image \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\\\": failed to resolve reference \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\\\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/10.1.0-ephemeral-oss-77506-8314-2: 403 Forbidden]\"" pod="hosted-grafana/ephemeral1511182177506ashharr-grafana-784dcb4f96-2kg67" podUID="0a16466c-0e3b-4ee0-a51e-aeea181e1cce"`,
				`E0528 15:23:<_>.<_>    4626 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task 51f71e1ac59ec5db0aa744684bbf3eda9c379b440682dc0cbd27710c622ed895 not found: not found" containerID="51f71e1ac59ec5db0aa744684bbf3eda9c379b440682dc0cbd27710c622ed895" cmd=["/bin/hgrun","check"]`,
				`E0528 15:23:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"gcom-sync\" with ErrImagePull: \"rpc error: code = NotFound desc = failed to pull and unpack image \\\"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\\\": failed to resolve reference \\\"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\\\": us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c: not found\"" pod="faro/<_>" podUID="<_>"`,
				`E0528 15:23:<_>.<_>    <_> remote_image.go:180] "PullImage from image service failed" err="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\": failed to resolve reference \"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\": us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c: not found" image="us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c"`,
				`E0528 15:<_>:<_>.<_>    <_> cpu_manager.go:395] "RemoveStaleState: removing container" podUID="<_>" containerName="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> kubelet.go:2006] failed to "KillPodSandbox" for "<_>" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"`,
				`E0528 15:<_>:<_>.<_>    <_> kuberuntime_manager.go:1375] "Failed to stop sandbox" podSandboxID={"Type":"containerd","ID":"<_>"}`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"<_>\" with KillPodSandboxError: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"" pod="grafana-com/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with CrashLoopBackOff: \"back-off <_> restarting failed container=<_> pod=<_>(<_>)\"" pod="<_>/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"<_>\" with CreateContainerConfigError: \"secret \\\"<_>\\\" not found\"" pod="<_>/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"gcom-sync\" with ImagePullBackOff: \"Back-off pulling image \\\"us.gcr.io/kubernetes-dev/frontend-monitoring:<_>\\\"\"" pod="faro/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"grafana\" with ImagePullBackOff: \"Back-off pulling image \\\"us.gcr.io/hosted-grafana/hosted-grafana-pro:<_>.1.<_>\\\"\"" pod="hosted-grafana/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pdc\" with ErrImageNeverPull: \"Container image \\\"us.gcr.io/hosted-grafana/pdc:0.1.415\\\" is not present with pull policy of Never\"" pod="pdc/<_>" podUID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> prober.go:239] "Unable to write all bytes from execInContainer" err="short write" expectedBytes=<_> actualBytes=10240`,
				`E0528 15:<_>:<_>.<_>    <_> remote_runtime.go:222] "StopPodSandbox from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" podSandboxID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found" containerID="<_>"`,
				`E0528 15:<_>:<_>.<_>    <_> remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 30s exceeded: context deadline exceeded" containerID="<_>" cmd=["/bin/hgrun","check"]`,
				`Error calling D-Bus proxy call to interface '/org/chromium/SessionManager': Error calling D-Bus method: org.chromium.SessionManagerInterface.RetrieveActiveSessions: The name org.chromium.SessionManager was not provided by any .service files`,
				`Failed to call method: org.chromium.SessionManagerInterface.RetrieveActiveSessions: object_path= /org/chromium/SessionManager: org.freedesktop.DBus.Error.ServiceUnknown: The name org.chromium.SessionManager was not provided by any .service files`,
				`Finished crash-sender.service.`,
				`I0528 15:23:06.652439    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/pdcsi-node-5vg2h" podStartSLOduration=2.423654998 podCreationTimestamp="2024-05-28 15:22:47 +0000 UTC" firstStartedPulling="2024-05-28 15:22:48.726429288 +0000 UTC m=+5.133798147" lastFinishedPulling="2024-05-28 15:23:05.955151801 +0000 UTC m=+22.362520664" observedRunningTime="2024-05-28 15:23:06.651361179 +0000 UTC m=+23.058730039" watchObservedRunningTime="2024-05-28 15:23:06.652377515 +0000 UTC m=+23.059746454"`,
				`I0528 15:23:06.987655    4809 plugin_watcher.go:194] "Adding socket path or updating timestamp to desired state cache" path="/var/lib/kubelet/plugins_registry/pd.csi.storage.gke.io-reg.sock"`,
				`I0528 15:23:07.955797    4809 reconciler.go:161] "OperationExecutor.RegisterPlugin started" plugin={"SocketPath":"/var/lib/kubelet/plugins_registry/pd.csi.storage.gke.io-reg.sock","Timestamp":"2024-05-28T15:23:06.987709292Z","Handler":null,"Name":""}`,
				`I0528 15:23:07.957695    4809 csi_plugin.go:99] kubernetes.io/csi: Trying to validate a new CSI Driver with name: pd.csi.storage.gke.io endpoint: /var/lib/kubelet/plugins/pd.csi.storage.gke.io/csi.sock versions: 1.0.0`,
				`I0528 15:23:07.957744    4809 csi_plugin.go:112] kubernetes.io/csi: Register new plugin with name: pd.csi.storage.gke.io at endpoint: /var/lib/kubelet/plugins/pd.csi.storage.gke.io/csi.sock`,
				`I0528 15:23:08.996989    4626 prober.go:107] "Probe failed" probeType="Readiness" pod="hosted-grafana/dev05devuseast0test-grafana-544ffc4d8f-d5k6v" podUID="7dc6279d-efc6-44a1-876c-6d15464889bb" containerName="grafana" probeResult="failure" output=<`,
				`I0528 15:23:09.725834    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="beyla-network/beyla-network-gnz47" podStartSLOduration=4.59949075 podCreationTimestamp="2024-05-28 15:22:59 +0000 UTC" firstStartedPulling="2024-05-28 15:23:02.756231678 +0000 UTC m=+19.163600536" lastFinishedPulling="2024-05-28 15:23:08.882463071 +0000 UTC m=+25.289831934" observedRunningTime="2024-05-28 15:23:09.648649974 +0000 UTC m=+26.056018841" watchObservedRunningTime="2024-05-28 15:23:09.725722148 +0000 UTC m=+26.133091212"`,
				`I0528 15:23:14.364144    4639 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"iptables-volume\" (UniqueName: \"kubernetes.io/configmap/89e55102-61a1-4c00-9362-e5bb2ab810f4-iptables-volume\") pod \"89e55102-61a1-4c00-9362-e5bb2ab810f4\" (UID: \"89e55102-61a1-4c00-9362-e5bb2ab810f4\") "`,
				`I0528 15:23:14.465236    4639 reconciler_common.go:300] "Volume detached for volume \"iptables-volume\" (UniqueName: \"kubernetes.io/configmap/89e55102-61a1-4c00-9362-e5bb2ab810f4-iptables-volume\") on node \"gke-dev-us-central-0-hg-n2s8-6-1dd39c-3bfd06e9-s8rt\" DevicePath \"\""`,
				`I0528 15:23:15.684443    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="agent-logs/agent-6dpnx" podStartSLOduration=19.381185265 podCreationTimestamp="2024-05-28 15:22:47 +0000 UTC" firstStartedPulling="2024-05-28 15:23:06.130405142 +0000 UTC m=+22.537774005" lastFinishedPulling="2024-05-28 15:23:15.433591478 +0000 UTC m=+31.840960349" observedRunningTime="2024-05-28 15:23:15.682811449 +0000 UTC m=+32.090180326" watchObservedRunningTime="2024-05-28 15:23:15.684371609 +0000 UTC m=+32.091740468"`,
				`I0528 15:23:17.<_>    5559 reconciler_common.go:300] "Volume detached for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") on node \"ip-10-60-3-93.us-east-2.compute.internal\" DevicePath \"\""`,
				`I0528 15:23:18.681373    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="ge-logs/promtail-x89kz" podStartSLOduration=19.565336207 podCreationTimestamp="2024-05-28 15:22:47 +0000 UTC" firstStartedPulling="2024-05-28 15:23:06.277869401 +0000 UTC m=+22.685238258" lastFinishedPulling="2024-05-28 15:23:18.393808665 +0000 UTC m=+34.801177586" observedRunningTime="2024-05-28 15:23:18.675235062 +0000 UTC m=+35.082603930" watchObservedRunningTime="2024-05-28 15:23:18.681275535 +0000 UTC m=+35.088644401"`,
				`I0528 15:23:18.<_>    <_> pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="hosted-grafana/<_>" podStartSLOduration=<_>.<_> podCreationTimestamp="2024-05-28 15:23:<_> +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-28 15:23:18.<_> +0000 UTC m=<_>.<_>" watchObservedRunningTime="2024-05-28 15:23:18.<_> +0000 UTC m=<_>.<_>"`,
				`I0528 15:23:19.683223    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="insight-logs/promtail-insight-logs-kjh2j" podStartSLOduration=19.663039641 podCreationTimestamp="2024-05-28 15:22:47 +0000 UTC" firstStartedPulling="2024-05-28 15:23:06.28867364 +0000 UTC m=+22.696042497" lastFinishedPulling="2024-05-28 15:23:19.308797013 +0000 UTC m=+35.716165877" observedRunningTime="2024-05-28 15:23:19.68163685 +0000 UTC m=+36.089005717" watchObservedRunningTime="2024-05-28 15:23:19.683163021 +0000 UTC m=+36.090531886"`,
				`I0528 15:23:20.687079    4809 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="startup/startup-bqr6q" podStartSLOduration=19.563954468 podCreationTimestamp="2024-05-28 15:22:47 +0000 UTC" firstStartedPulling="2024-05-28 15:23:06.289991666 +0000 UTC m=+22.697360526" lastFinishedPulling="2024-05-28 15:23:20.413058423 +0000 UTC m=+36.820427283" observedRunningTime="2024-05-28 15:23:20.685592448 +0000 UTC m=+37.092961312" watchObservedRunningTime="2024-05-28 15:23:20.687021225 +0000 UTC m=+37.094390090"`,
				`I0528 15:23:22.081660    4612 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"`,
				`I0528 15:23:22.082259    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="ge-logs/promtail-mdh5l" podUID="f76c13c6-7cf6-40aa-a15f-ff685f25dff4" containerName="promtail" containerID="containerd://95bad974db8a05eeecdad9bf5a5412830bcd05fce6c3f850eaa5a976c0615898" gracePeriod=30`,
				`I0528 15:23:22.124456    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="netfilter-exporter/netfilter-exporter-7gbjm" podUID="59844c4c-c7b6-4a68-821d-55f397da1301" containerName="netfilter-exporter" containerID="containerd://17c2ed42944322f0c406bc776dac8aac1c2a25a9d4b288ad3fbad7bed1c2f2a5" gracePeriod=30`,
				`I0528 15:23:22.131286    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="agent-logs/agent-z4t7t" podUID="18daf969-3a85-4ff9-af58-a3570a69b27b" containerName="agent" containerID="containerd://6f84f7140b2940949fe3bade41d692eaa910b8b92a8980caff8195b15027b46e" gracePeriod=30`,
				`I0528 15:23:22.132330    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="goldpinger/goldpinger-ttlf4" podUID="cb51476d-978a-4a60-8927-ca6f86596112" containerName="goldpinger" containerID="containerd://b4d9c569915162af90fb8c66ac5332c0f9a7e2da63cd04dde1bba92d33891c13" gracePeriod=30`,
				`I0528 15:23:22.134177    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="startup/startup-xf5tg" podUID="e725c899-1196-4fab-93aa-26299fff652a" containerName="startup" containerID="containerd://01dc515dba464504fed035a94a87a3400f5ecfddb9bc5380f34e64138c579c70" gracePeriod=30`,
				`I0528 15:23:22.146057    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="insight-logs/promtail-insight-logs-xk5gj" podUID="f40511cd-f2fa-4069-9341-01b779b2d6ab" containerName="promtail" containerID="containerd://1bcbb5a28d935adbcf407cffb387e1053262fa1e31acb82f017920ed3f065507" gracePeriod=30`,
				`I0528 15:23:22.158389    4612 plugin_watcher.go:206] "Removing socket path from desired state cache" path="/var/lib/kubelet/plugins_registry/pd.csi.storage.gke.io-reg.sock"`,
				`I0528 15:23:22.296569    4740 log_monitor.go:159] New status generated: &{Source:node-registration-checker-monitor Events:[{Severity:warn Timestamp:2024-05-28 15:23:21.90945 +0000 UTC Reason:NodeRegistrationCheckerDidNotRunChecks Message:Tue May 28 15:23:21 UTC 2024 - **     Node ready and registered. **}] Conditions:[]}`,
				`I0528 15:23:22.524372    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-cni-bin-dir\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.524425    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"var-run-calico\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-var-run-calico\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.524563    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-lib-modules\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.524625    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-var-lib-calico\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.524667    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-xtables-lock\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.524743    4612 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-cni-net-dir\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.625378    4612 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-lib-modules\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.625447    4612 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/dd46de88-a704-4532-a4eb-7f284e426ce0-var-lib-calico\") pod \"calico-node-4gjqc\" (UID: \"dd46de88-a704-4532-a4eb-7f284e426ce0\") " pod="kube-system/calico-node-4gjqc"`,
				`I0528 15:23:22.<_>    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="<_>/<_>" podUID="<_>" containerName="<_>" containerID="containerd://<_>" gracePeriod=30`,
				`I0528 15:23:22.<_>    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="kube-system/<_>" podUID="<_>" containerName="<_>" containerID="containerd://<_>" gracePeriod=<_>`,
				`I0528 15:23:22.<_>    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="promtail-ops/<_>" podUID="<_>" containerName="<_>" containerID="containerd://<_>" gracePeriod=30`,
				`I0528 15:23:22.<_>    4612 kuberuntime_container.go:745] "Killing container with a grace period" pod="pyroscope-ebpf/profiler-vdnbc" podUID="e7f45cf8-71c3-425f-ae1f-a0f1938d88d5" containerName="<_>" containerID="containerd://<_>" gracePeriod=30`,
				`I0528 15:23:<_>.<_>    <_> azure_credentials.go:220] image(us.gcr.io/kubernetes-dev/frontend-monitoring) is not from ACR, return empty authentication`,
				`I0528 15:23:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (probe)" probe="startup" status="<_>" pod="agent-logs/<_>"`,
				`I0528 15:23:<_>.<_>    <_> kuberuntime_container.go:745] "Killing container with a grace period" pod="hosted-grafana/<_>" podUID="<_>" containerName="<_>" containerID="containerd://<_>" gracePeriod=<_>`,
				`I0528 15:23:<_>.<_>    <_> pod_container_deletor.go:80] "Container not found in pod's containers" containerID="<_>"`,
				`I0528 15:23:<_>.<_>    <_> pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="promtail-ops/<_>" podStartSLOduration=<_>.<_> podCreationTimestamp="2024-05-28 15:22:<_> +0000 UTC" firstStartedPulling="2024-05-28 15:<_>:<_>.<_> +0000 UTC m=+22.<_>" lastFinishedPulling="2024-05-28 15:23:<_>.<_> +0000 UTC m=<_>.<_>" observedRunningTime="2024-05-28 15:23:<_>.<_> +0000 UTC m=<_>.<_>" watchObservedRunningTime="2024-05-28 15:23:<_>.<_> +0000 UTC m=<_>.<_>"`,
				`I0528 15:23:<_>.<_>    <_> reconciler_common.go:300] "Volume detached for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") on node \"<_>\" DevicePath \"\""`,
				`I0528 15:23:<_>.<_>    <_> sysinfo.go:232] Found node without cache information, nodeDir: /sys/devices/system/node/node0`,
				`I0528 15:<_>:<_>.<_>    <_> generic.go:334] "Generic (PLEG): container finished" podID="<_>" containerID="<_>" exitCode=<_>`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (PLEG): event for pod" pod="<_>/<_>" event={"ID":"<_>","Type":"<_>","Data":"<_>"}`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (probe)" probe="liveness" status="unhealthy" pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (probe)" probe="readiness" status="" pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop (probe)" probe="readiness" status="ready" pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop ADD" source="api" pods=["<_>/<_>"]`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop DELETE" source="api" pods=["<_>/<_>"]`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop REMOVE" source="api" pods=["<_>/<_>"]`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet.go:<_>] "SyncLoop UPDATE" source="api" pods=["<_>/<_>"]`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet_getters.go:187] "Pod status updated" pod="kube-system/<_>" status="Running"`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet_pods.go:906] "Unable to retrieve pull secret, the image pull may not succeed." pod="<_>/<_>" secret="" err="secret \"<_>\" not found"`,
				`I0528 15:<_>:<_>.<_>    <_> kubelet_volumes.go:<_>] "Cleaned up orphaned pod volumes dir" podUID="<_>" path="/var/lib/kubelet/pods/<_>/volumes"`,
				`I0528 15:<_>:<_>.<_>    <_> memory_manager.go:346] "RemoveStaleState removing state" podUID="<_>" containerName="<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> operation_generator.go:722] "MountVolume.SetUp succeeded for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") pod \"<_>\" (UID: \"<_>\") " pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> operation_generator.go:<_>] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/<_>" (OuterVolumeSpecName: "<_>") pod "<_>" (UID: "<_>"). InnerVolumeSpecName "<_>". PluginName "kubernetes.io/configmap", VolumeGidValue ""`,
				`I0528 15:<_>:<_>.<_>    <_> operation_generator.go:<_>] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/<_>" (OuterVolumeSpecName: "<_>") pod "<_>" (UID: "<_>"). InnerVolumeSpecName "<_>". PluginName "kubernetes.io/projected", VolumeGidValue ""`,
				`I0528 15:<_>:<_>.<_>    <_> operation_generator.go:<_>] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/<_>" (OuterVolumeSpecName: "<_>") pod "<_>" (UID: "<_>"). InnerVolumeSpecName "<_>". PluginName "kubernetes.io/secret", VolumeGidValue ""`,
				`I0528 15:<_>:<_>.<_>    <_> pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="hosted-grafana/<_>" podStartSLOduration=<_>.<_> podCreationTimestamp="2024-05-28 15:<_>:<_> +0000 UTC" firstStartedPulling="2024-05-28 15:<_>:<_>.<_> +0000 UTC m=<_>.<_>" lastFinishedPulling="2024-05-28 15:<_>:<_>.<_> +0000 UTC m=<_>.<_>" observedRunningTime="2024-05-28 15:<_>:<_>.<_> +0000 UTC m=<_>.<_>" watchObservedRunningTime="2024-05-28 15:<_>:<_>.<_> +0000 UTC m=<_>.<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> prober.go:107] "Probe failed" probeType="Readiness" pod="<_>/<_>" podUID="<_>" containerName="<_>" probeResult="failure" output="Get \"http://10.132.<_>.<_>:<_>/<_>\": dial tcp 10.132.<_>.<_>:<_>: connect: connection refused"`,
				`I0528 15:<_>:<_>.<_>    <_> prober.go:107] "Probe failed" probeType="Readiness" pod="<_>/<_>" podUID="<_>" containerName="<_>" probeResult="failure" output="HTTP probe failed with statuscode: <_>"`,
				`I0528 15:<_>:<_>.<_>    <_> prober.go:107] "Probe failed" probeType="Readiness" pod="hosted-grafana/<_>" podUID="<_>" containerName="grafana" probeResult="failure" output="command \"/bin/hgrun check\" timed out"`,
				`I0528 15:<_>:<_>.<_>    <_> provider.go:102] Refreshing cache for provider: *credentialprovider.defaultDockerConfigProvider`,
				`I0528 15:<_>:<_>.<_>    <_> provider.go:82] Docker config file not found: couldn't find valid .dockercfg after checking in [/var/lib/kubelet   /]`,
				`I0528 15:<_>:<_>.<_>    <_> reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") pod \"<_>\" (UID: \"<_>\") "`,
				`I0528 15:<_>:<_>.<_>    <_> reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") pod \"<_>\" (UID: \"<_>\") " pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"<_>\" (UniqueName: \"kubernetes.io/<_>/<_>\") pod \"<_>\" (UID: \"<_>\") " pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> scope.go:117] "RemoveContainer" containerID="<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> state_mem.go:107] "Deleted CPUSet assignment" podUID="<_>" containerName="<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> topology_manager.go:215] "Topology Admit Handler" podUID="<_>" podNamespace="<_>" podName="<_>"`,
				`I0528 15:<_>:<_>.<_>    <_> util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="<_>/<_>"`,
				`I0528 15:<_>:<_>.<_> <_> cache.go:40] re-using cached key and certificate`,
				`IPv4: martian source 10.132.<_>.<_> from 10.132.<_>.<_>, on dev eth0`,
				`IPv6: ADDRCONF(NETDEV_CHANGE): <_>: link becomes ready`,
				`Removed slice <_>.slice.`,
				`Removed slice libcontainer container kubepods-burstable-pod991d43dd_f3aa_4e01_bba9_abb3595bb8b7.slice.`,
				`Started <_>.scope.`,
				`Started libcontainer container <_>.`,
				`Starting crash-sender.service...`,
				`Tue May 28 15:23:21 UTC 2024 - **     Kubelet is healthy. **`,
				`Tue May 28 15:23:21 UTC 2024 - **     Node ready and registered. **`,
				`Tue May 28 15:23:21 UTC 2024 - **   Checking if Kubelet is healthy... **`,
				`Tue May 28 15:23:21 UTC 2024 - **   Checking node status with the K8s API Server... **`,
				`Tue May 28 15:23:21 UTC 2024 - ** Checking if the node is registered... **`,
				`Tue May 28 15:23:21 UTC 2024 - ** Completed running Node Registration Checker **`,
				`W0528 15:23:19.533168    5579 manager.go:1159] Failed to process watch event {EventType:0 Name:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2aab6299_1dcd_434f_9964_51e5bcb8aeeb.slice/cri-containerd-aa68fc477fbbf3eb6aca68f55af1f26417322a0ac8da2955523d807f5b85c00d.scope WatchSource:0}: containerd task is in unknown state`,
				`W0528 15:23:<_>.<_>    <_> machine.go:65] Cannot read vendor id correctly, set empty.`,
				`XMT: Solicit on eth0, interval <_>.`,
				`bound to 10.60.6.238 -- renewal in 1773 seconds.`,
				`crash-sender.service: Deactivated successfully.`,
				`dhclient: Locked /run/dhclient/resolv.lock`,
				`ena 0000:00:08.0 eth3: Local page cache is disabled for less than 16 channels`,
				`ena 0000:00:08.0: ENA Large LLQ is disabled`,
				`ena 0000:00:08.0: ENA controller version: 0.0.1 implementation version 1`,
				`ena 0000:00:08.0: ENA device version: 0.10`,
				`ena 0000:00:08.0: Elastic Network Adapter (ENA) found at mem c0118000, mac addr 02:d9:cd:de:22:1f`,
				`ena 0000:00:08.0: enabling device (0000 -> 0002)`,
				`err="failed to get container status \"<_>\": rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found"`,
				`ll header: 00000000: 42 01 0a 80 <_> <_> 42 01 0a 80 00 01 08 00`,
				`ln --force -s /proc/$(pidof hgrun-pause)/root/bin/hgrun /bin/hgrun;`,
				`pci 0000:00:08.0: BAR 2: assigned [mem 0xc0300000-0xc03fffff pref]`,
				`pci 0000:00:08.0: BAR <_>: assigned [mem <_>]`,
				`pci 0000:00:08.0: [1d0f:ec20] type 00 class 0x020000`,
				`pci 0000:00:08.0: reg 0x18: [mem 0x00000000-0x000fffff pref]`,
				`pci 0000:00:08.0: reg <_>: [mem <_>]`,
				`returns container id \"<_>\""`,
				`returns sandbox id \"<_>\""`,
				`run-containerd-io.containerd.grpc.v1.<_>.mount: Deactivated successfully.`,
				`run-containerd-io.containerd.runtime.v2.task-k8s.<_>.mount: Deactivated successfully.`,
				`run-containerd-runc-k8s.<_>.<_>.mount: Deactivated successfully.`,
				`run-netns-cni\<_>\<_>\<_>\<_>\<_>.mount: Deactivated successfully.`,
				`start failed in pod <_>(<_>): CreateContainerConfigError: secret "<_>" not found`,
				`start failed in pod <_>(<_>): CreateContainerConfigError: secret "dynatrace-datasource-grafana-app-system-cap" not found`,
				`start failed in pod <_>(<_>): CreateContainerConfigError: secret "fleet-management-user-fleet-management" not found`,
				`start failed in pod <_>(<_>): CreateContainerConfigError: secret "ruler-alertmanager-token" not found`,
				`start failed in pod <_>(<_>): ErrImageNeverPull: Container image "us.gcr.io/hosted-grafana/pdc:0.1.415" is not present with pull policy of Never`,
				`start failed in pod <_>(<_>): ErrImagePull: rpc error: code = NotFound desc = failed to pull and unpack image "us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c": failed to resolve reference "us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c": us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c: not found`,
				`start failed in pod ephemeral1511182177506ashharr-grafana-784dcb4f96-2kg67_hosted-grafana(0a16466c-0e3b-4ee0-a51e-aeea181e1cce): ErrImagePull: [rpc error: code = NotFound desc = failed to pull and unpack image "us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2": failed to resolve reference "us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2": us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2: not found, failed to pull and unpack image "us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2": failed to resolve reference "us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/10.1.0-ephemeral-oss-77506-8314-2: 403 Forbidden]`,
				`start failed in pod testcrossplane-exporter-c67cfc58f-vbzl4_crossplane-playground(3d49134d-3378-4ec3-824c-5ff4ea2590a5): CreateContainerConfigError: secret "testcrossplane-user-exporter" not found`,
				`start failed in pod trial-crossplane-mysql-flex-exporter-94556f4dd-4bmtj_crossplane-playground(e75e660f-2b13-4f5a-8fcc-b98c38d04f26): CreateContainerConfigError: secret "trial-crossplane-mysql-flex-user-exporter" not found`,
				`time="2024-05-28T15:23:08.862061456Z" level=info msg="stop pulling image docker.io/grafana/beyla:1.5.2: active requests=0, bytes read=61410141"`,
				`time="2024-05-28T15:23:08.881250601Z" level=info msg="Pulled image \"docker.io/grafana/beyla:1.5.2\" with image id \"sha256:2433aeb0f5e82e93bee4db73be1018415034593bf2da0de705f2725e44e7d8ed\", repo tag \"docker.io/grafana/beyla:1.5.2\", repo digest \"docker.io/grafana/beyla@sha256:30ff364149ed5211c8a0f6ea02f63f546a7989b20c984c5dcc0d5e350ea0336a\", size \"61402149\" in 6.124634608s"`,
				`time="2024-05-28T15:23:08.881770821Z" level=info msg="PullImage \"docker.io/grafana/beyla:1.5.2\" returns image reference \"sha256:2433aeb0f5e82e93bee4db73be1018415034593bf2da0de705f2725e44e7d8ed\""`,
				`time="2024-05-28T15:23:08.882990958Z" level=info msg="PullImage \"grafana/agent:main-315599b\""`,
				`time="2024-05-28T15:23:12.282244712Z" level=error msg="failed to handle container TaskExit event container_id:\"a9987625a46821d221bdcffdeb340bccddc6df9c34857ccd7f396898483aa9b9\" id:\"a9987625a46821d221bdcffdeb340bccddc6df9c34857ccd7f396898483aa9b9\" pid:2814948 exit_status:143 exited_at:{seconds:1716909792 nanos:253836231}" error="failed to stop container: unknown error after kill: runc did not terminate successfully: exit status 1: lstat /sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod89e55102_61a1_4c00_9362_e5bb2ab810f4.slice/cri-containerd-a9987625a46821d221bdcffdeb340bccddc6df9c34857ccd7f396898483aa9b9.scope: no such file or directory\n: unknown"`,
				`time="2024-05-28T15:23:12.818871767Z" level=info msg="stop pulling image docker.io/grafana/alloy-dev:v1.2.0-devel-8c46be32d: active requests=0, bytes read=169082604"`,
				`time="2024-05-28T15:23:12.832013280Z" level=info msg="Pulled image \"grafana/alloy-dev:v1.2.0-devel-8c46be32d\" with image id \"sha256:65b9c0b4d3dc49dfa8c2f7bf8afd90d9f9bc427c47195c66e7c1ad1f3adab7a8\", repo tag \"docker.io/grafana/alloy-dev:v1.2.0-devel-8c46be32d\", repo digest \"docker.io/grafana/alloy-dev@sha256:f85b618ffb8525b14ff3393aa9308b1e630c16d7a230e0c5afc43f8e480af9a3\", size \"169075748\" in 6.862764316s"`,
				`time="2024-05-28T15:23:12.832079070Z" level=info msg="PullImage \"grafana/alloy-dev:v1.2.0-devel-8c46be32d\" returns image reference \"sha256:65b9c0b4d3dc49dfa8c2f7bf8afd90d9f9bc427c47195c66e7c1ad1f3adab7a8\""`,
				`time="2024-05-28T15:23:12.833550566Z" level=info msg="PullImage \"grafana/agent:v0.19.0\""`,
				`time="2024-05-28T15:23:13.050083841Z" level=error msg="collecting metrics for aaac6774af65a3704d8b61ae28b72b25740db5c26178238adad72f087145c1c6" error="ttrpc: closed: unknown"`,
				`time="2024-05-28T15:23:13.644202556Z" level=error msg="PullImage \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\" failed" error="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2: not found"`,
				`time="2024-05-28T15:23:13.796858126Z" level=error msg="PullImage \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\" failed" error="failed to pull and unpack image \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": failed to resolve reference \"us.gcr.io/hosted-grafana/hosted-grafana-pro:10.1.0-ephemeral-oss-77506-8314-2\": unexpected status from HEAD request to https://us.gcr.io/v2/hosted-grafana/hosted-grafana-pro/manifests/10.1.0-ephemeral-oss-77506-8314-2: 403 Forbidden"`,
				`time="2024-05-28T15:23:14.020080940Z" level=info msg="TaskExit event container_id:\"a9987625a46821d221bdcffdeb340bccddc6df9c34857ccd7f396898483aa9b9\" id:\"a9987625a46821d221bdcffdeb340bccddc6df9c34857ccd7f396898483aa9b9\" pid:2814948 exit_status:143 exited_at:{seconds:1716909792 nanos:253836231}"`,
				`time="2024-05-28T15:23:14.343886976Z" level=error msg="failed to handle container TaskExit event container_id:\"ee06f3b89458c55549ca763806338d84e666c5bd683a21a89575c4bba96ac6a9\" id:\"ee06f3b89458c55549ca763806338d84e666c5bd683a21a89575c4bba96ac6a9\" pid:12333 exited_at:{seconds:1716909784 nanos:342262304}" error="failed to stop container: failed to delete task: context deadline exceeded: unknown"`,
				`time="2024-05-28T15:23:15.266509238Z" level=info msg="PullImage \"grafana/grafana-image-renderer:3.9.1\""`,
				`time="2024-05-28T15:23:15.387799983Z" level=info msg="TaskExit event container_id:\"ee06f3b89458c55549ca763806338d84e666c5bd683a21a89575c4bba96ac6a9\" id:\"ee06f3b89458c55549ca763806338d84e666c5bd683a21a89575c4bba96ac6a9\" pid:12333 exited_at:{seconds:1716909784 nanos:342262304}"`,
				`time="2024-05-28T15:23:15.410946132Z" level=info msg="stop pulling image docker.io/grafana/grafana-image-renderer:3.9.1: active requests=0, bytes read=0"`,
				`time="2024-05-28T15:23:15.417826477Z" level=info msg="stop pulling image docker.io/grafana/agent:main-315599b: active requests=0, bytes read=206188459"`,
				`time="2024-05-28T15:23:15.422960076Z" level=info msg="Pulled image \"grafana/grafana-image-renderer:3.9.1\" with image id \"sha256:3ec79e517b0f1506c242a2fe80ec18329530f4bdb901999400c9f9a0a3a3d47a\", repo tag \"docker.io/grafana/grafana-image-renderer:3.9.1\", repo digest \"docker.io/grafana/grafana-image-renderer@sha256:17449ff911c086b17a1220d728bfb3283770b26890a5733ca62f8c402706cbe8\", size \"375526994\" in 156.383303ms"`,
				`time="2024-05-28T15:23:15.423016720Z" level=info msg="PullImage \"grafana/grafana-image-renderer:3.9.1\" returns image reference \"sha256:3ec79e517b0f1506c242a2fe80ec18329530f4bdb901999400c9f9a0a3a3d47a\""`,
				`time="2024-05-28T15:23:15.433080604Z" level=info msg="Pulled image \"grafana/agent:main-315599b\" with image id \"sha256:23096f814fec70010c45edac7b678b6b5d7ea2ac8fca2cb09dc47cb43a5a2742\", repo tag \"docker.io/grafana/agent:main-315599b\", repo digest \"docker.io/grafana/agent@sha256:e9f9b5d5d69c97c708c214dd355876170f7801d252b9d3b6814df608b7b9de5c\", size \"233415610\" in 6.55001031s"`,
				`time="2024-05-28T15:23:15.433157365Z" level=info msg="PullImage \"grafana/agent:main-315599b\" returns image reference \"sha256:23096f814fec70010c45edac7b678b6b5d7ea2ac8fca2cb09dc47cb43a5a2742\""`,
				`time="2024-05-28T15:23:15.434152183Z" level=info msg="PullImage \"grafana/loki-canary:k204-4901a5c\""`,
				`time="2024-05-28T15:23:15.501474101Z" level=info msg="stop pulling image docker.io/grafana/agent:v0.19.0: active requests=0, bytes read=58360096"`,
				`time="2024-05-28T15:23:15.516190326Z" level=info msg="Pulled image \"grafana/agent:v0.19.0\" with image id \"sha256:7714c10fffbd3c093311679eea5ac3d8c5e4423d77610fd9ef7151c8ac002790\", repo tag \"docker.io/grafana/agent:v0.19.0\", repo digest \"docker.io/grafana/agent@sha256:49a7e468de0353e162057fc4d09b9ceac88bf54ed7403f88b440c0ea9af134f9\", size \"58355524\" in 2.682592722s"`,
				`time="2024-05-28T15:23:15.516245422Z" level=info msg="PullImage \"grafana/agent:v0.19.0\" returns image reference \"sha256:7714c10fffbd3c093311679eea5ac3d8c5e4423d77610fd9ef7151c8ac002790\""`,
				`time="2024-05-28T15:23:15.517194886Z" level=info msg="PullImage \"grafana/promtail:2.7.1\""`,
				`time="2024-05-28T15:23:16.417051919Z" level=info msg="stop pulling image docker.io/grafana/loki-canary:k204-4901a5c: active requests=0, bytes read=14030381"`,
				`time="2024-05-28T15:23:16.425903586Z" level=info msg="Pulled image \"grafana/loki-canary:k204-4901a5c\" with image id \"sha256:258063b978d94856d3c585444b0b9edf9875ad5b54adaf64db0178d4469c7f01\", repo tag \"docker.io/grafana/loki-canary:k204-4901a5c\", repo digest \"docker.io/grafana/loki-canary@sha256:6ed545971a56472106e41855efa9dc02b29f474f032ab733fbe3e16dfe85520a\", size \"14025809\" in 991.67814ms"`,
				`time="2024-05-28T15:23:16.425970496Z" level=info msg="PullImage \"grafana/loki-canary:k204-4901a5c\" returns image reference \"sha256:258063b978d94856d3c585444b0b9edf9875ad5b54adaf64db0178d4469c7f01\""`,
				`time="2024-05-28T15:23:18.384856064Z" level=info msg="stop pulling image docker.io/grafana/promtail:2.7.1: active requests=0, bytes read=72479520"`,
				`time="2024-05-28T15:23:18.393434007Z" level=info msg="Pulled image \"grafana/promtail:2.7.1\" with image id \"sha256:d8ff59c577c5b9a07922029eeb9cae321d661a35f8e8f17d11f9904cdd26e798\", repo tag \"docker.io/grafana/promtail:2.7.1\", repo digest \"docker.io/grafana/promtail@sha256:aa77333bb912b1017d35f16bd5d7f6d191963d6cee78bc0bd0df0a56990a3e42\", size \"72473804\" in 2.876196652s"`,
				`time="2024-05-28T15:23:18.393468676Z" level=info msg="PullImage \"grafana/promtail:2.7.1\" returns image reference \"sha256:d8ff59c577c5b9a07922029eeb9cae321d661a35f8e8f17d11f9904cdd26e798\""`,
				`time="2024-05-28T15:23:20.412597678Z" level=info msg="PullImage \"us.gcr.io/kubernetes-dev/startup-script:2024-01-29-v462118-4c4b0b365\" returns image reference \"sha256:6bebdaf186280640a2111b46db718e00f08fc4d77aed7b440ecdecd805690a97\""`,
				`time="2024-05-28T15:23:21.017237046Z" level=info msg="ImageCreate event name:\"us.gcr.io/hosted-grafana/hosted-grafana-pro:11.1.0-71307\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:21.615981132Z" level=info msg="PullImage \"gke.gcr.io/calico/node:v3.26.3-gke.7@sha256:4f74413efce048b2df0d534eb7865197d4839fd7c7009676bf06ab1dfbbfc7c0\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=error msg="ExecSync for \"51f71e1ac59ec5db0aa744684bbf3eda9c379b440682dc0cbd27710c622ed895\" failed" error="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task 51f71e1ac59ec5db0aa744684bbf3eda9c379b440682dc0cbd27710c622ed895 not found: not found"`,
				`time="2024-05-28T15:23:<_>.<_>" level=error msg="PullImage \"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\" failed" error="rpc error: code = NotFound desc = failed to pull and unpack image \"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\": failed to resolve reference \"us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c\": us.gcr.io/kubernetes-dev/frontend-monitoring:ba9262c: not found"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="Forcibly stopping sandbox \"<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageCreate event name:\"docker.io/<_>/<_>:<_>.<_>.<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageCreate event name:\"docker.io/<_>/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageCreate event name:\"docker.io/grafana/<_>:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageCreate event name:\"us.gcr.io/<_>/<_>:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageCreate event name:\"us.gcr.io/<_>/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageUpdate event name:\"us.gcr.io/<_>/<_>:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="ImageUpdate event name:\"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"docker.io/bloomberg/goldpinger:v3.8.0\" returns image reference \"sha256:23cd6a4df34124b6b26cede41da0834f3200b4b7df44e05b2fdfdb466252af74\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"docker.io/bloomberg/goldpinger:v3.8.0\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"grafana/promtail:<_>\" returns image reference \"sha256:<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"grafana/promtail:<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"us.gcr.io/hosted-grafana/hg-plugins:<_>\" returns image reference \"sha256:<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="PullImage \"us.gcr.io/kubernetes-dev/<_>:<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="Pulled image \"docker.io/bloomberg/goldpinger:v3.8.0\" with image id \"sha256:23cd6a4df34124b6b26cede41da0834f3200b4b7df44e05b2fdfdb466252af74\", repo tag \"docker.io/bloomberg/goldpinger:v3.8.0\", repo digest \"docker.io/bloomberg/goldpinger@sha256:459a8522a79b91dfbb86b993509f76b1a62a51e624eb2e7f4d253dc16bbc8629\", size \"33769254\" in <_>.<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="Pulled image \"grafana/promtail:<_>\" with image id \"sha256:<_>\", repo tag \"docker.io/grafana/promtail:<_>\", repo digest \"docker.io/grafana/promtail@sha256:<_>\", size \"<_>\" in <_>.<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="Pulled image \"us.gcr.io/<_>/<_>:<_>\" with image id \"sha256:<_>\", repo tag \"us.gcr.io/<_>/<_>:<_>\", repo digest \"us.gcr.io/<_>/<_>@sha256:<_>\", size \"<_>\" in <_>.<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="RemovePodSandbox \"<_>\" returns successfully"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="RemovePodSandbox for \"<_>\""`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="stop pulling image docker.io/bloomberg/goldpinger:v3.8.0: active requests=0, bytes read=<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="stop pulling image docker.io/grafana/promtail:<_>: active requests=0, bytes read=<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="stop pulling image us.gcr.io/hosted-grafana/hg-plugins:<_>: active requests=0, bytes read=<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="stop pulling image us.gcr.io/kubernetes-dev/<_>:<_>: active requests=0, bytes read=<_>"`,
				`time="2024-05-28T15:23:<_>.<_>" level=info msg="trying next host - response was http.StatusNotFound" host=us.gcr.io`,
				`time="2024-05-28T15:23:<_>.<_>" level=warning msg="Failed to get podSandbox status for container event for sandboxID \"<_>\": an error occurred when try to find sandbox: not found. Sending the event with nil podSandboxStatus."`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=error msg="ContainerStatus for \"<_>\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"<_>\": not found"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=error msg="ExecSync for \"<_>\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 30s exceeded: context deadline exceeded"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=error msg="StopPodSandbox for \"<_>\" failed" error="failed to stop sandbox container \"<_>\" in \"SANDBOX_READY\" state: failed to kill sandbox container: context canceled: unknown"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=error msg="get state for <_>" error="context deadline exceeded: unknown"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="Container to stop \"<_>\" must be in running or unknown state, current state \"CONTAINER_EXITED\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="CreateContainer within sandbox \"<_>\" for container &ContainerMetadata{Name:<_>,Attempt:<_>,}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageCreate event name:\"sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageUpdate event name:\"docker.io/<_>/<_>:<_>.<_>.<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageUpdate event name:\"docker.io/<_>/<_>:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageUpdate event name:\"docker.io/<_>/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageUpdate event name:\"sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="ImageUpdate event name:\"us.gcr.io/hosted-grafana/<_>@sha256:<_>\" labels:{key:\"io.cri-containerd.image\" value:\"managed\"}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="PullImage \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\" returns image reference \"sha256:<_>\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="PullImage \"us.gcr.io/hosted-grafana/<_>:<_>.1.<_>\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="Pulled image \"us.gcr.io/<_>/<_>:<_>.<_>.<_>\" with image id \"sha256:<_>\", repo tag \"us.gcr.io/<_>/<_>:<_>.<_>.<_>\", repo digest \"us.gcr.io/<_>/<_>@sha256:<_>\", size \"<_>\" in <_>.<_>"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="RemoveContainer for \"<_>\" returns successfully"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="RemoveContainer for \"<_>\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:<_>,Uid:<_>,Namespace:<_>,Attempt:0,}"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StartContainer for \"<_>\" returns successfully"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StartContainer for \"<_>\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="Stop container \"<_>\" with signal terminated"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StopContainer for \"<_>\" returns successfully"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StopContainer for \"<_>\" with timeout <_> (s)"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StopPodSandbox for \"<_>\" returns successfully"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="StopPodSandbox for \"<_>\""`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="TearDown network for sandbox \"<_>\" successfully"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="cleaning up dead shim" namespace=k8s.io`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="loading plugin \"io.containerd.ttrpc.v1.<_>\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="shim disconnected" id=<_> namespace=k8s.io`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=info msg="stop pulling image us.gcr.io/hosted-grafana/<_>:<_>.1.<_>: active requests=0, bytes read=<_>"`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=warning msg="cleaning up after shim disconnected" id=<_> namespace=k8s.io`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=warning msg="cleanup warnings time=\"2024-05-28T15:<_>:<_>\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n" namespace=k8s.io`,
				`time="2024-05-28T15:<_>:<_>.<_>" level=warning msg="unknown status" status=0`,
				`ts=2024-05-28T15:23:02.158801601Z level=error caller=http_client.go:56 app=hgrun hgrun_version=0.1.457-30-g86d524e19 msg="request`,
				`ts=2024-05-28T15:<_>:<_>.<_> level=error caller=http_client.go:56 app=hgrun hgrun_version=0.1.457-30-g86d524e19 msg="request failed" error="Get \"http://127.0.0.1:3000/api/health\": dial tcp 127.0.0.1:3000: connect: connection refused" method=GET url=http://127.0.0.1:3000/api/health`,
				`unmap 0x793e72600000/12099584`,
				`var-lib-containerd-tmpmounts-containerd\<_>.mount: Deactivated successfully.`,
				`warm-up.service: Deactivated successfully.`,
				`while [ "$(pidof plugins-pause)" = "" ]; do sleep 0.5; done;`,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.inputFile, func(t *testing.T) {
			file, err := os.Open(tt.inputFile)
			require.NoError(t, err)
			defer file.Close()

			scanner := bufio.NewScanner(file)
			for scanner.Scan() {
				line := scanner.Text()
				tt.drain.Train(line, 0)
			}

			var output []string
			clusters := tt.drain.Clusters()
			for _, cluster := range clusters {
				output = append(output, cluster.String())
			}
			slices.Sort(output)

			if outputPatternsForTestUpdate {
				for _, pattern := range output {
					fmt.Printf("`%s`,\n", pattern)
				}
			}

			require.Equal(t, tt.patterns, output)
			require.Falsef(t, outputPatternsForTestUpdate, `outputPatternsForTestUpdate should only be used locally to update test patterns.`)
		})
	}
}

func TestDrain_TrainGeneratesMatchablePatterns(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		drain      *Drain
		inputLines []string
	}{
		{
			name:  "should match each line against a pattern",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				"test test test test",
				"test test test test",
				"test test test test",
				"test test test test",
			},
		},
		{
			name:  "should also match newlines",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				`test test test test
`,
				`test test test test
`,
				`test test test test
`,
				`test test test test
`,
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			for _, line := range tt.inputLines {
				tt.drain.Train(line, 0)
			}

			for _, line := range tt.inputLines {
				match := tt.drain.Match(line)
				require.NotNil(t, match, `Line should match a cluster`)
			}
		})
	}

}

func TestDrain_TrainGeneratesPatternsMatchableByLokiPatternFilter(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		drain      *Drain
		inputLines []string
	}{
		{
			name:  "should extract patterns that all lines match",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				"test 1 test test",
				"test 2 test test",
				"test 3 test test",
				"test 4 test test",
			},
		},
		{
			name:  "should extract patterns that match if line ends with newlines",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				`test 1 test test
`,
				`test 2 test test
`,
				`test 3 test test
`,
				`test 4 test test
`,
			},
		},
		{
			name:  "should extract patterns that match if line ends with empty space",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				`test 1 test test			`,
				`test 2 test test			`,
				`test 3 test test			`,
				`test 4 test test			`,
			},
		},
		{
			name:  "should extract patterns that match if line starts with empty space",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				`			test 1 test test`,
				`			test 2 test test`,
				`			test 3 test test`,
				`			test 4 test test`,
			},
		},
		{
			name:  "Scheduler patterns are matchable",
			drain: New(DefaultConfig(), nil),
			inputLines: []string{
				`ts=2024-05-30T12:50:36.648377186Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.350575929Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.335784477Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.250406732Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.248030329Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.45.239:9095`,
				`ts=2024-05-30T12:50:36.176344754Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.174730772Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.151.101:9095`,
				`ts=2024-05-30T12:50:36.076517207Z caller=scheduler_processor.go:143 level=warn msg="error contacting scheduler" err="rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: EOF\"" addr=10.0.45.239:9095`,
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			for _, line := range tt.inputLines {
				tt.drain.Train(line, 0)
			}
			require.Equal(t, 1, len(tt.drain.Clusters()))
			cluster := tt.drain.Clusters()[0]

			matcher, err := pattern.ParseLineFilter([]byte(cluster.String()))
			require.NoError(t, err)

			for _, line := range tt.inputLines {
				passes := matcher.Test([]byte(line))
				require.Truef(t, passes, "Line should match extracted pattern: \nPatt[%q] \nLine[%q]", cluster.String(), line)

			}
		})
	}

}
