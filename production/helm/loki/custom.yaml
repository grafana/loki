deploymentMode: Distributed

#useTestSchema: true
#storage:
#  bucketNames:
#    chunks: chunks
#    ruler: ruler

loki:
  structuredConfig:
    frontend:
      grpc_client_config:
        grpc_compression: snappy
    frontend_worker:
      grpc_client_config:
        grpc_compression: snappy
    ingester_client:
      grpc_client_config:
        grpc_compression: snappy
    query_scheduler:
      grpc_client_config:
        grpc_compression: snappy
    memberlist:
      cluster_label: "{{.Release.Name}}-{{.Release.Namespace}}"

  analytics:
    reporting_enabled: false
  tracing:
    enabled: false
  ui:
    enabled: false
  image:
    registry: 360915197767.dkr.ecr.us-east-2.amazonaws.com
    repository: loki
    tag: 3.5.1
  podLabels:
    eb/owner: Ares
    eb/envtype: preprod
    eb/service: loki
  serviceLabels:
    eb/owner: Ares
    eb/envtype: preprod
    eb/service: loki

  storage:
    # Loki requires a bucket for chunks and the ruler. GEL requires a third bucket for the admin API.
    bucketNames:
      chunks: chunks
      ruler: ruler
    type: s3
    s3:
      region: us-west-2
      endpoint:

  auth_enabled: true
  limits_config:
    otlp_config:
      resource_attributes:
        attributes_config:
          - action: index_label
            attributes:
              - source_name

    # When logs are ingested by Loki using an OpenTelemetry protocol (OTLP) ingestion endpoint, some of the data is
    # stored as Structured Metadata. You must set allow_structured_metadata to true within your Loki config file.
    # Otherwise, Loki will reject the log payload as malformed.
    # https://grafana.com/docs/loki/latest/send-data/otel/#loki-configuration
    allow_structured_metadata: true
    ingestion_rate_mb: 100
    ingestion_burst_size_mb: 200
    per_stream_rate_limit: 30MB
    per_stream_rate_limit_burst: 60MB
    query_timeout: 10m
    split_queries_by_interval: 15m
    max_entries_limit_per_query: 20000
    max_line_size_truncate: true
    tsdb_max_query_parallelism: 2048

    shard_streams:
      enabled: true
      logging_enabled: false
      desired_rate: 1MB

    discover_service_name:
      # This is a stop gap until we can set service_name from the OpenTelemetry collectors. Ideally we don't want to be
      # doing this extra computation as it needs to do this processing for each log line if the service_name label
      # does not exist
      #
      # The following are resource attribute labels that we can pull from to populate the service_name
      - k8s_deployment_name
      - k8s_daemonset_name
      - k8s_statefulset_name
      - k8s_cronjob_name

  distributor:
    otlp_config:
      # List of default otlp resource attributes to be picked as index labels
      default_resource_attributes_as_index_labels:
        - "service.name"
        - "deployment.environment.name"
        - "cloud.region"
        - "k8s.cluster.name"
        - "k8s.namespace.name"
        - "k8s.deployment.name"
        - "k8s.statefulset.name"
        - "k8s.daemonset.name"
        - "k8s.cronjob.name"
        - "k8s.job.name"

  schemaConfig:
    configs:
      - from: "2020-07-01"
        store: tsdb
        object_store: aws
        schema: v13
        index:
          prefix: index_
          period: 24h

  storage_config:
    tsdb_shipper:
      active_index_directory: /var/loki/index
      cache_location: /var/loki/index_cache
      cache_ttl: 24h

  pattern_ingester:
    enabled: true

  server:
    log_level: info
    grpc_server_max_recv_msg_size: 52428800
    grpc_server_max_send_msg_size: 52428800

  # config.yaml configmap
  # {{- with .Values.loki.ingester }}
  # ingester:
  #   {{- tpl (. | toYaml) $ | nindent 4 }}
  # {{- end }}
  ingester:
    autoforget_unhealthy: true
    chunk_encoding: snappy
    chunk_target_size: 2097152

  querier:
    # The maximum number of queries that can be simultaneously processed by the querier
    max_concurrent: 8

  frontend:
    log_queries_longer_than: 5s

patternIngester:
  replicas: 3
  maxUnavailable: 1
  resources:
    requests:
      cpu: 500m
      memory: 1024Mi
    limits:
      memory: 2048Mi
ingester:
  replicas: 3
  maxUnavailable: 1
  resources:
    requests:
      cpu: 2
      memory: 10Gi
    limits:
      memory: 20Gi
  persistence:
    enabled: true
    claims:
      - name: data
        size: 150Gi
        volumeAttributesClassName: ebs-csi-o11y-loki-ingesters
  updateStrategy:
    type: OnDelete
  serviceLabels:
    eb/owner: Ares
    eb/envtype: dev
    eb/service: loki
#    zoneAwareReplication:
#      zoneA:
#        nodeSelector:
#          eb/group: platform_observability
#      zoneB:
#        nodeSelector:
#          eb/group: platform_observability
#      zoneC:
#        nodeSelector:
#          eb/group: platform_observability
#    tolerations:
#      - effect: NoSchedule
#        key: dedicated
#        operator: Equal
#        value: loki

querier:
  # overriding the affinity due to it spinning up a new node for every querier
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: argogithub16312issue
          topologyKey: kubernetes.io/hostname
  maxSurge: 25%
  keda:
    enabled: true
    prometheusEndpoint: http://mimir-gateway.mimir.svc.cluster.local/prometheus
    minReplicas: 10
    maxReplicas: 450
    threshold: "300"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 30
        policies:
          - type: Percent
            value: 50
            periodSeconds: 30
      scaleUp:
        policies:
          - periodSeconds: 15
            type: Pods
            value: 4
          - periodSeconds: 15
            type: Percent
            value: 150
    fallback:
      cpu:
        utilization: 50
  autoscaling:
    enabled: true
    minReplicas: 10
    maxReplicas: 450
    # Optimizing for the user experience. Setting this to a lower number to enable queriers to scale up quicker. At a
    # threshold of 60, we would see a slow ramp up time leaving queries to be queued for extended periods of time. This
    # will most likely change as we start to introduce consistent read traffic via alerting queries and human searches
    targetCPUUtilizationPercentage: 30
  maxUnavailable: 50%
  resources:
    requests:
      cpu: 2
      memory: 3Gi
    limits:
      memory: 4Gi
  nodeSelector:
    eb/group: platform_observability
  tolerations:
    - key: dedicated
      operator: Equal
      value: loki
      effect: NoSchedule

queryFrontend:
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
  maxUnavailable: 1
  resources:
    requests:
      cpu: 1000m
      memory: 3Gi
    limits:
      memory: 4Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

queryScheduler:
  replicas: 3
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      memory:
        1Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

distributor:
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
  podDisruptionBudget:
    maxUnavailable: 1
  resources:
    requests:
      cpu: 1
      memory: 4Gi
    limits:
      memory:
        8Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

compactor:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      memory:
        4Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

indexGateway:
  replicas: 3
  maxUnavailable: 1
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      memory:
        1Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

gateway:
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
  image:
    registry: 360915197767.dkr.ecr.us-east-2.amazonaws.com
    repository: nginx
    tag: 1.28-alpine
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      memory:
        2Gi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule
# General Memcached settings
memcached:
  image:
    repository: 360915197767.dkr.ecr.us-east-2.amazonaws.com/grafana/memcached
    tag: 1.6.38-alpine
memcachedExporter:
  enabled: true
  image:
    repository: 360915197767.dkr.ecr.us-east-2.amazonaws.com/grafana/memcached-exporter
    tag: v0.15.2
# Settings specific to ChunksCache
chunksCache:
  enabled: true
  replicas: 12
  # Pod memory is calulated as allocatedMemory * 1.2 in the template
  allocatedMemory: 8192
  # NOTE: batch_size can be set to 2x the number of memcached servers you have. So if you have 3 servers, 6 should work. Keep parallelism as low as possible, but increase this value if you are not maxing out network usage on memcached.
  batchSize: 24
  parallelism: 2
  # setting to 60s due to using extStore https://github.com/memcached/memcached/wiki/ConfiguringLokiExtstore
  timeout: 120s
  extraExtendedOptions: "ext_threads=8,ext_max_sleep=10000,slab_automove_freeratio=0.10,ext_recache_rate=0"
  maxUnavailable: 3
  persistence:
    enabled: true
    storageSize: 375Gi
    storageClass: ebs-csi-o11y-loki-chunks-cache
    ebsVolumeType: gp3
    ebsVolumeIops: 3000
    ebsVolumeThroughput: 125
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

# Settings specific to ResultsCache
resultsCache:
  enabled: true
  replicas: 3
  # Pod memory is calulated as allocatedMemory * 1.2 in the template
  allocatedMemory: 1024
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

rollout_operator:
  enabled: true
  image:
    repository: grafana/rollout-operator
    pullPolicy: IfNotPresent
    # -- Overrides the image tag whose default is the chart appVersion.
    tag: ""
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

ruler:
  enabled: true
  replicas: 3
  maxUnavailable: 1
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule

#######################################################################################
# Loki Canary component - https://grafana.com/docs/loki/latest/operations/loki-canary/
#######################################################################################
test:
  enabled: false
lokiCanary:
  enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 256Mi
#    nodeSelector:
#      eb/group: platform_observability
#    tolerations:
#      - key: dedicated
#        operator: Equal
#        value: loki
#        effect: NoSchedule
# Zero out replica counts of other deployment modes
# This is just dumb and really should be done by the helm chart if we are running with Distributed deployment mode
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0

singleBinary:
  replicas: 0
